{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV with shape: (232074, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cb4d04ccef4087a5325a0db2d9d89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/232074 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 185659, Test set size: 46415\n",
      "Final TRAIN size: 37131\n",
      "Final EVAL size: 46415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /Users/ehsan/.llama/checkpoints/Llama3.2-1B-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d1653ad041416d817bc0b23dd23ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58afc0fd7cb4452d91430736d2d3833d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46415 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9620d9f4444366a135a8a0ccffe720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18566 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1094, 'grad_norm': 106.77273559570312, 'learning_rate': 9.994613810190672e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6751, 'grad_norm': 71.1974105834961, 'learning_rate': 9.989227620381343e-05, 'epoch': 0.0}\n",
      "{'loss': 2.0439, 'grad_norm': 54.92241287231445, 'learning_rate': 9.983841430572014e-05, 'epoch': 0.0}\n",
      "{'loss': 1.3188, 'grad_norm': 111.18098449707031, 'learning_rate': 9.978455240762685e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0407, 'grad_norm': 46.394065856933594, 'learning_rate': 9.973069050953357e-05, 'epoch': 0.01}\n",
      "{'loss': 1.0606, 'grad_norm': 26.005048751831055, 'learning_rate': 9.967682861144027e-05, 'epoch': 0.01}\n",
      "{'loss': 0.8373, 'grad_norm': 70.67825317382812, 'learning_rate': 9.962296671334699e-05, 'epoch': 0.01}\n",
      "{'loss': 0.6686, 'grad_norm': 84.9291763305664, 'learning_rate': 9.95691048152537e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7721, 'grad_norm': 5.769374370574951, 'learning_rate': 9.95152429171604e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4556, 'grad_norm': 141.23388671875, 'learning_rate': 9.946138101906712e-05, 'epoch': 0.01}\n",
      "{'loss': 0.9562, 'grad_norm': 101.68986511230469, 'learning_rate': 9.940751912097382e-05, 'epoch': 0.01}\n",
      "{'loss': 0.6007, 'grad_norm': 9.405245780944824, 'learning_rate': 9.935365722288054e-05, 'epoch': 0.01}\n",
      "{'loss': 0.5525, 'grad_norm': 5.65691614151001, 'learning_rate': 9.929979532478724e-05, 'epoch': 0.01}\n",
      "{'loss': 0.5524, 'grad_norm': 35.00889205932617, 'learning_rate': 9.924593342669396e-05, 'epoch': 0.02}\n",
      "{'loss': 1.0451, 'grad_norm': 184.3754425048828, 'learning_rate': 9.919207152860068e-05, 'epoch': 0.02}\n",
      "{'loss': 0.7976, 'grad_norm': 83.83727264404297, 'learning_rate': 9.913820963050738e-05, 'epoch': 0.02}\n",
      "{'loss': 0.5063, 'grad_norm': 0.09978991746902466, 'learning_rate': 9.90843477324141e-05, 'epoch': 0.02}\n",
      "{'loss': 0.5165, 'grad_norm': 77.7939453125, 'learning_rate': 9.90304858343208e-05, 'epoch': 0.02}\n",
      "{'loss': 0.2105, 'grad_norm': 87.55155944824219, 'learning_rate': 9.897662393622752e-05, 'epoch': 0.02}\n",
      "{'loss': 0.7403, 'grad_norm': 0.06609459966421127, 'learning_rate': 9.892276203813423e-05, 'epoch': 0.02}\n",
      "{'loss': 0.8029, 'grad_norm': 7.420684814453125, 'learning_rate': 9.886890014004094e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3387, 'grad_norm': 42.894386291503906, 'learning_rate': 9.881503824194765e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4929, 'grad_norm': 0.0003145149676129222, 'learning_rate': 9.876117634385437e-05, 'epoch': 0.02}\n",
      "{'loss': 0.8559, 'grad_norm': 0.0008984169107861817, 'learning_rate': 9.870731444576108e-05, 'epoch': 0.03}\n",
      "{'loss': 0.7755, 'grad_norm': 0.018825888633728027, 'learning_rate': 9.865345254766778e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3561, 'grad_norm': 57.30915451049805, 'learning_rate': 9.859959064957449e-05, 'epoch': 0.03}\n",
      "{'loss': 0.8661, 'grad_norm': 74.12673950195312, 'learning_rate': 9.85457287514812e-05, 'epoch': 0.03}\n",
      "{'loss': 0.567, 'grad_norm': 0.00016964389942586422, 'learning_rate': 9.849186685338792e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0613, 'grad_norm': 4.856104715145193e-05, 'learning_rate': 9.843800495529463e-05, 'epoch': 0.03}\n",
      "{'loss': 0.4074, 'grad_norm': 113.75533294677734, 'learning_rate': 9.838414305720134e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0503, 'grad_norm': 0.01754719391465187, 'learning_rate': 9.833028115910806e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2418, 'grad_norm': 7.779753650538623e-05, 'learning_rate': 9.827641926101476e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0084, 'grad_norm': 0.0005739403422921896, 'learning_rate': 9.822255736292148e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1814, 'grad_norm': 0.0013508083065971732, 'learning_rate': 9.816869546482818e-05, 'epoch': 0.04}\n",
      "{'loss': 0.4957, 'grad_norm': 0.5911813974380493, 'learning_rate': 9.81148335667349e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2886, 'grad_norm': 0.0020276124123483896, 'learning_rate': 9.80609716686416e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2068, 'grad_norm': 0.06525381654500961, 'learning_rate': 9.800710977054832e-05, 'epoch': 0.04}\n",
      "{'loss': 0.3014, 'grad_norm': 0.0031457506120204926, 'learning_rate': 9.795324787245504e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0329, 'grad_norm': 0.10855811089277267, 'learning_rate': 9.789938597436174e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0005, 'grad_norm': 0.9092071652412415, 'learning_rate': 9.784552407626846e-05, 'epoch': 0.04}\n",
      "{'loss': 0.5273, 'grad_norm': 68.99183654785156, 'learning_rate': 9.779166217817515e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0094, 'grad_norm': 7.029948028502986e-05, 'learning_rate': 9.773780028008187e-05, 'epoch': 0.05}\n",
      "{'loss': 0.5796, 'grad_norm': 0.07592565566301346, 'learning_rate': 9.768393838198859e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1526, 'grad_norm': 0.06292301416397095, 'learning_rate': 9.763007648389529e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2021, 'grad_norm': 0.45673906803131104, 'learning_rate': 9.757621458580201e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0104, 'grad_norm': 0.00013730466889683157, 'learning_rate': 9.752235268770871e-05, 'epoch': 0.05}\n",
      "{'loss': 0.6825, 'grad_norm': 0.5987980961799622, 'learning_rate': 9.746849078961543e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0, 'grad_norm': 3.841956640826538e-06, 'learning_rate': 9.741462889152214e-05, 'epoch': 0.05}\n",
      "{'loss': 0.5098, 'grad_norm': 8.341960347024724e-05, 'learning_rate': 9.736076699342885e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1606, 'grad_norm': 0.002790655940771103, 'learning_rate': 9.730690509533556e-05, 'epoch': 0.05}\n",
      "{'loss': 0.3718, 'grad_norm': 1.3976625204086304, 'learning_rate': 9.725304319724228e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0013, 'grad_norm': 0.11671080440282822, 'learning_rate': 9.7199181299149e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0013, 'grad_norm': 0.17267338931560516, 'learning_rate': 9.71453194010557e-05, 'epoch': 0.06}\n",
      "{'loss': 0.1917, 'grad_norm': 8.59901376770722e-08, 'learning_rate': 9.709145750296242e-05, 'epoch': 0.06}\n",
      "{'loss': 0.3207, 'grad_norm': 5.985362076899037e-05, 'learning_rate': 9.703759560486912e-05, 'epoch': 0.06}\n",
      "{'loss': 0.1601, 'grad_norm': 8.334501035278663e-05, 'learning_rate': 9.698373370677584e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0031, 'grad_norm': 0.06347070634365082, 'learning_rate': 9.692987180868254e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0039, 'grad_norm': 2.7400294584367657e-06, 'learning_rate': 9.687600991058925e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0, 'grad_norm': 3.181762849635561e-06, 'learning_rate': 9.682214801249597e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0001, 'grad_norm': 0.011267969384789467, 'learning_rate': 9.676828611440267e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0512, 'grad_norm': 0.33108624815940857, 'learning_rate': 9.671442421630939e-05, 'epoch': 0.07}\n",
      "{'loss': 0.4937, 'grad_norm': 4.165901401620431e-08, 'learning_rate': 9.666056231821609e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': 9.145821877609706e-08, 'learning_rate': 9.660670042012281e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0002, 'grad_norm': 0.023099785670638084, 'learning_rate': 9.655283852202951e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0022, 'grad_norm': 1.0020837493129875e-09, 'learning_rate': 9.649897662393623e-05, 'epoch': 0.07}\n",
      "{'loss': 1.4005, 'grad_norm': 104.02527618408203, 'learning_rate': 9.644511472584295e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': 3.494703923934139e-05, 'learning_rate': 9.639125282774965e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': 8.771142347541172e-06, 'learning_rate': 9.633739092965637e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': 2.519547024348867e-07, 'learning_rate': 9.628352903156308e-05, 'epoch': 0.07}\n",
      "{'loss': 0.1609, 'grad_norm': 8.623629241810704e-07, 'learning_rate': 9.62296671334698e-05, 'epoch': 0.08}\n",
      "{'loss': 0.193, 'grad_norm': 5.40347002697672e-07, 'learning_rate': 9.61758052353765e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0323, 'grad_norm': 41.363895416259766, 'learning_rate': 9.612194333728322e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0241, 'grad_norm': 13.046343803405762, 'learning_rate': 9.606808143918992e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2927, 'grad_norm': 2.0668845176696777, 'learning_rate': 9.601421954109663e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0036, 'grad_norm': 0.003025394631549716, 'learning_rate': 9.596035764300334e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2552, 'grad_norm': 4.728836131562275e-07, 'learning_rate': 9.590649574491005e-05, 'epoch': 0.08}\n",
      "{'loss': 1.1609, 'grad_norm': 0.034066565334796906, 'learning_rate': 9.585263384681677e-05, 'epoch': 0.08}\n",
      "{'loss': 0.1384, 'grad_norm': 154.7288055419922, 'learning_rate': 9.579877194872347e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': 0.00016216548101510853, 'learning_rate': 9.574491005063019e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0017, 'grad_norm': 2.7273178100585938, 'learning_rate': 9.56910481525369e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0004, 'grad_norm': 1.0910990567936096e-05, 'learning_rate': 9.563718625444361e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0001, 'grad_norm': 3.6537608139042277e-06, 'learning_rate': 9.558332435635033e-05, 'epoch': 0.09}\n",
      "{'loss': 0.7157, 'grad_norm': 5.295674121441607e-09, 'learning_rate': 9.552946245825703e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0034, 'grad_norm': 4.853614154853858e-05, 'learning_rate': 9.547560056016375e-05, 'epoch': 0.09}\n",
      "{'loss': 0.3025, 'grad_norm': 5.192176831769757e-05, 'learning_rate': 9.542173866207045e-05, 'epoch': 0.09}\n",
      "{'loss': 0.102, 'grad_norm': 5.351812839508057, 'learning_rate': 9.536787676397717e-05, 'epoch': 0.09}\n",
      "{'loss': 0.223, 'grad_norm': 0.0013187512522563338, 'learning_rate': 9.531401486588388e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5443, 'grad_norm': 123.92289733886719, 'learning_rate': 9.52601529677906e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0222, 'grad_norm': 0.007623319514095783, 'learning_rate': 9.52062910696973e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5262, 'grad_norm': 1.6180217699002242e-06, 'learning_rate': 9.5152429171604e-05, 'epoch': 0.1}\n",
      "{'loss': 0.1804, 'grad_norm': 0.017546990886330605, 'learning_rate': 9.509856727351072e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0059, 'grad_norm': 11.412373542785645, 'learning_rate': 9.504470537541743e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0011, 'grad_norm': 0.0003377241955604404, 'learning_rate': 9.499084347732414e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0514, 'grad_norm': 0.07576561719179153, 'learning_rate': 9.493698157923086e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0, 'grad_norm': 0.0015195967862382531, 'learning_rate': 9.488311968113757e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0, 'grad_norm': 2.7583214432524983e-06, 'learning_rate': 9.482925778304428e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0051, 'grad_norm': 5.46539695278625e-07, 'learning_rate': 9.477539588495099e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0002, 'grad_norm': 7.200517870842305e-08, 'learning_rate': 9.47215339868577e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0, 'grad_norm': 1.6830178140025964e-08, 'learning_rate': 9.466767208876441e-05, 'epoch': 0.11}\n",
      "{'loss': 0.169, 'grad_norm': 5.418933506007306e-05, 'learning_rate': 9.461381019067113e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010031360579887405, 'learning_rate': 9.455994829257783e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0862, 'grad_norm': 2.00279305317963e-06, 'learning_rate': 9.450608639448455e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0, 'grad_norm': 0.000192191859241575, 'learning_rate': 9.445222449639127e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0455, 'grad_norm': 24.947832107543945, 'learning_rate': 9.439836259829797e-05, 'epoch': 0.11}\n",
      "{'loss': 0.1802, 'grad_norm': 9.437116204935592e-06, 'learning_rate': 9.434450070020468e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0008, 'grad_norm': 0.016027547419071198, 'learning_rate': 9.429063880211138e-05, 'epoch': 0.11}\n",
      "{'loss': 0.7943, 'grad_norm': 4.9489397468960306e-08, 'learning_rate': 9.42367769040181e-05, 'epoch': 0.12}\n",
      "{'loss': 0.1351, 'grad_norm': 0.0006910086376592517, 'learning_rate': 9.418291500592482e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5176, 'grad_norm': 1.64304333338805e-06, 'learning_rate': 9.412905310783152e-05, 'epoch': 0.12}\n",
      "{'loss': 0.1444, 'grad_norm': 0.008006727322936058, 'learning_rate': 9.407519120973824e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5174, 'grad_norm': 1.079883098602295, 'learning_rate': 9.402132931164494e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0005, 'grad_norm': 0.022183265537023544, 'learning_rate': 9.396746741355166e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0013, 'grad_norm': 2.719156952935009e-07, 'learning_rate': 9.391360551545836e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0, 'grad_norm': 1.114617020903097e-07, 'learning_rate': 9.385974361736508e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0002, 'grad_norm': 0.011501691304147243, 'learning_rate': 9.380588171927179e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0, 'grad_norm': 8.336834071087651e-06, 'learning_rate': 9.37520198211785e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0, 'grad_norm': 1.2116189509470132e-06, 'learning_rate': 9.369815792308522e-05, 'epoch': 0.13}\n",
      "{'loss': 0.9887, 'grad_norm': 86.45773315429688, 'learning_rate': 9.364429602499193e-05, 'epoch': 0.13}\n",
      "{'loss': 0.2418, 'grad_norm': 0.0001101180532714352, 'learning_rate': 9.359043412689864e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00010375345300417393, 'learning_rate': 9.353657222880535e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0013, 'grad_norm': 0.00885624811053276, 'learning_rate': 9.348271033071205e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0923, 'grad_norm': 0.0006439367425628006, 'learning_rate': 9.342884843261877e-05, 'epoch': 0.13}\n",
      "{'loss': 0.2351, 'grad_norm': 0.11665495485067368, 'learning_rate': 9.337498653452548e-05, 'epoch': 0.13}\n",
      "{'loss': 0.7782, 'grad_norm': 2.2710726625518873e-05, 'learning_rate': 9.33211246364322e-05, 'epoch': 0.13}\n",
      "{'loss': 0.1593, 'grad_norm': 153.9987030029297, 'learning_rate': 9.32672627383389e-05, 'epoch': 0.13}\n",
      "{'loss': 0.8283, 'grad_norm': 0.00035860136267729104, 'learning_rate': 9.321340084024562e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001392831327393651, 'learning_rate': 9.315953894215232e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0, 'grad_norm': 0.03953938186168671, 'learning_rate': 9.310567704405904e-05, 'epoch': 0.14}\n",
      "{'loss': 0.7383, 'grad_norm': 117.20914459228516, 'learning_rate': 9.305181514596574e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0493, 'grad_norm': 2.1092761016916484e-05, 'learning_rate': 9.299795324787246e-05, 'epoch': 0.14}\n",
      "{'loss': 0.3375, 'grad_norm': 100.60293579101562, 'learning_rate': 9.294409134977918e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0183, 'grad_norm': 3.989648291735648e-07, 'learning_rate': 9.289022945168588e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0025, 'grad_norm': 0.31600189208984375, 'learning_rate': 9.28363675535926e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0001, 'grad_norm': 0.010494167916476727, 'learning_rate': 9.27825056554993e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0132, 'grad_norm': 0.004684575367718935, 'learning_rate': 9.272864375740602e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0005, 'grad_norm': 6.990895862202251e-09, 'learning_rate': 9.267478185931273e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0126, 'grad_norm': 0.05079027637839317, 'learning_rate': 9.262091996121943e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0007, 'grad_norm': 0.006832628510892391, 'learning_rate': 9.256705806312615e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0027, 'grad_norm': 0.26835405826568604, 'learning_rate': 9.251319616503285e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0004, 'grad_norm': 5.390236765379086e-05, 'learning_rate': 9.245933426693957e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003200547944288701, 'learning_rate': 9.240547236884628e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0001, 'grad_norm': 0.1305362433195114, 'learning_rate': 9.235161047075299e-05, 'epoch': 0.15}\n",
      "{'loss': 0.1054, 'grad_norm': 4.2810553713135135e-11, 'learning_rate': 9.22977485726597e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0, 'grad_norm': 7.876038665699525e-08, 'learning_rate': 9.224388667456642e-05, 'epoch': 0.16}\n",
      "{'loss': 0.9547, 'grad_norm': 0.00013620604295283556, 'learning_rate': 9.219002477647313e-05, 'epoch': 0.16}\n",
      "{'loss': 0.1973, 'grad_norm': 111.05953979492188, 'learning_rate': 9.213616287837984e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0716, 'grad_norm': 4.352387747985631e-07, 'learning_rate': 9.208230098028656e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0, 'grad_norm': 4.424926373758353e-05, 'learning_rate': 9.202843908219326e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0442, 'grad_norm': 0.0030676417518407106, 'learning_rate': 9.197457718409998e-05, 'epoch': 0.16}\n",
      "{'loss': 0.001, 'grad_norm': 3.0498881340026855, 'learning_rate': 9.192071528600668e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0029, 'grad_norm': 5.669687652698485e-07, 'learning_rate': 9.18668533879134e-05, 'epoch': 0.16}\n",
      "{'loss': 0.1533, 'grad_norm': 4.458146577235311e-05, 'learning_rate': 9.18129914898201e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0001, 'grad_norm': 1.5350792637036648e-07, 'learning_rate': 9.175912959172682e-05, 'epoch': 0.16}\n",
      "{'loss': 0.2077, 'grad_norm': 7.058463961584494e-05, 'learning_rate': 9.170526769363353e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0, 'grad_norm': 0.000722022436093539, 'learning_rate': 9.165140579554023e-05, 'epoch': 0.17}\n",
      "{'loss': 0.6907, 'grad_norm': 1.3218151906357889e-08, 'learning_rate': 9.159754389744695e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0, 'grad_norm': 1.1817959602922201e-05, 'learning_rate': 9.154368199935365e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0095, 'grad_norm': 1.8883441953221336e-05, 'learning_rate': 9.148982010126037e-05, 'epoch': 0.17}\n",
      "{'loss': 0.1274, 'grad_norm': 0.00027995993150398135, 'learning_rate': 9.143595820316709e-05, 'epoch': 0.17}\n",
      "{'loss': 0.1314, 'grad_norm': 192.1996307373047, 'learning_rate': 9.138209630507379e-05, 'epoch': 0.17}\n",
      "{'loss': 0.2141, 'grad_norm': 0.0021275014150887728, 'learning_rate': 9.132823440698051e-05, 'epoch': 0.17}\n",
      "{'loss': 0.2, 'grad_norm': 0.0004259738198015839, 'learning_rate': 9.127437250888721e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0304, 'grad_norm': 5.2336410590214655e-05, 'learning_rate': 9.122051061079393e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0, 'grad_norm': 2.660799935938485e-08, 'learning_rate': 9.116664871270064e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0, 'grad_norm': 6.452582113070093e-08, 'learning_rate': 9.111278681460735e-05, 'epoch': 0.18}\n",
      "{'loss': 0.2194, 'grad_norm': 6.847913755336776e-05, 'learning_rate': 9.105892491651406e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0119, 'grad_norm': 1.5439178469023318e-06, 'learning_rate': 9.100506301842078e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0328, 'grad_norm': 41.28425979614258, 'learning_rate': 9.09512011203275e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0, 'grad_norm': 3.0145645979473557e-09, 'learning_rate': 9.08973392222342e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0, 'grad_norm': 3.3007338060997427e-05, 'learning_rate': 9.08434773241409e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0018, 'grad_norm': 0.0002149517968064174, 'learning_rate': 9.078961542604761e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0, 'grad_norm': 1.9435146896285005e-06, 'learning_rate': 9.073575352795433e-05, 'epoch': 0.19}\n",
      "{'loss': 0.389, 'grad_norm': 80.2544174194336, 'learning_rate': 9.068189162986104e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0016, 'grad_norm': 9.764411515789106e-06, 'learning_rate': 9.062802973176775e-05, 'epoch': 0.19}\n",
      "{'loss': 0.002, 'grad_norm': 0.0007840949692763388, 'learning_rate': 9.057416783367447e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0, 'grad_norm': 5.951719140284695e-05, 'learning_rate': 9.052030593558117e-05, 'epoch': 0.19}\n",
      "{'loss': 0.049, 'grad_norm': 0.014644492417573929, 'learning_rate': 9.046644403748789e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6537, 'grad_norm': 7.245438098907471, 'learning_rate': 9.041258213939459e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00019560263899620622, 'learning_rate': 9.035872024130131e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0, 'grad_norm': 7.819497827021848e-10, 'learning_rate': 9.030485834320801e-05, 'epoch': 0.19}\n",
      "{'loss': 0.2276, 'grad_norm': 7.395942134280631e-07, 'learning_rate': 9.025099644511473e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0003, 'grad_norm': 0.0006100909668020904, 'learning_rate': 9.019713454702145e-05, 'epoch': 0.2}\n",
      "{'loss': 0.1734, 'grad_norm': 1.5853735249038436e-06, 'learning_rate': 9.014327264892815e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0044, 'grad_norm': 4.61455845757186e-10, 'learning_rate': 9.008941075083487e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0001, 'grad_norm': 1.0419103091408033e-05, 'learning_rate': 9.003554885274158e-05, 'epoch': 0.2}\n",
      "{'loss': 1.8143, 'grad_norm': 101.32569122314453, 'learning_rate': 8.998168695464828e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0001, 'grad_norm': 1.830240034905728e-05, 'learning_rate': 8.9927825056555e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0223, 'grad_norm': 0.08154170960187912, 'learning_rate': 8.98739631584617e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0009, 'grad_norm': 0.15604987740516663, 'learning_rate': 8.982010126036842e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0086, 'grad_norm': 0.009136734530329704, 'learning_rate': 8.976623936227513e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0, 'grad_norm': 0.014103375375270844, 'learning_rate': 8.971237746418184e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3562, 'grad_norm': 0.00023242106544785202, 'learning_rate': 8.965851556608855e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0002, 'grad_norm': 5.161058691527387e-09, 'learning_rate': 8.960465366799527e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0141, 'grad_norm': 0.00014362479851115495, 'learning_rate': 8.955079176990197e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0, 'grad_norm': 7.579333214380313e-07, 'learning_rate': 8.949692987180869e-05, 'epoch': 0.21}\n",
      "{'loss': 0.015, 'grad_norm': 1.2099532796128187e-05, 'learning_rate': 8.94430679737154e-05, 'epoch': 0.21}\n",
      "{'loss': 0.715, 'grad_norm': 2.278058309457265e-05, 'learning_rate': 8.938920607562211e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0039, 'grad_norm': 0.027722598984837532, 'learning_rate': 8.933534417752883e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0821, 'grad_norm': 1.4536662718001025e-07, 'learning_rate': 8.928148227943553e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0255, 'grad_norm': 2.5922653890120273e-07, 'learning_rate': 8.922762038134225e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0, 'grad_norm': 3.0326193609653274e-06, 'learning_rate': 8.917375848324895e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0011, 'grad_norm': 0.015012589283287525, 'learning_rate': 8.911989658515566e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0001, 'grad_norm': 0.005469741765409708, 'learning_rate': 8.906603468706238e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0, 'grad_norm': 1.4090409949929494e-09, 'learning_rate': 8.901217278896908e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4808, 'grad_norm': 5.0346731228501085e-08, 'learning_rate': 8.89583108908758e-05, 'epoch': 0.22}\n",
      "{'loss': 0.2085, 'grad_norm': 2.3241641429194715e-06, 'learning_rate': 8.89044489927825e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0157, 'grad_norm': 1.7776461902485607e-07, 'learning_rate': 8.885058709468922e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4593, 'grad_norm': 111.66422271728516, 'learning_rate': 8.879672519659593e-05, 'epoch': 0.22}\n",
      "{'loss': 0.1403, 'grad_norm': 0.0008193685789592564, 'learning_rate': 8.874286329850264e-05, 'epoch': 0.23}\n",
      "{'loss': 0.1137, 'grad_norm': 1.6851729469635757e-06, 'learning_rate': 8.868900140040936e-05, 'epoch': 0.23}\n",
      "{'loss': 0.1211, 'grad_norm': 7.515155164838916e-10, 'learning_rate': 8.863513950231607e-05, 'epoch': 0.23}\n",
      "{'loss': 0.7396, 'grad_norm': 1.161734417109983e-05, 'learning_rate': 8.858127760422278e-05, 'epoch': 0.23}\n",
      "{'loss': 0.5097, 'grad_norm': 9.725941652050096e-08, 'learning_rate': 8.852741570612949e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0917, 'grad_norm': 7.753505997243337e-06, 'learning_rate': 8.84735538080362e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0, 'grad_norm': 6.455716174968984e-07, 'learning_rate': 8.841969190994291e-05, 'epoch': 0.23}\n",
      "{'loss': 1.2473, 'grad_norm': 0.000204093026695773, 'learning_rate': 8.836583001184963e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0047, 'grad_norm': 4.920616149902344, 'learning_rate': 8.831196811375633e-05, 'epoch': 0.23}\n",
      "{'loss': 0.066, 'grad_norm': 0.003265334526076913, 'learning_rate': 8.825810621566304e-05, 'epoch': 0.23}\n",
      "{'loss': 0.5651, 'grad_norm': 6.526031484099803e-07, 'learning_rate': 8.820424431756975e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5973, 'grad_norm': 0.009919913485646248, 'learning_rate': 8.815038241947646e-05, 'epoch': 0.24}\n",
      "{'loss': 0.6953, 'grad_norm': 0.0059976582415401936, 'learning_rate': 8.809652052138318e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0072, 'grad_norm': 0.06892275810241699, 'learning_rate': 8.804265862328988e-05, 'epoch': 0.24}\n",
      "{'loss': 1.0257, 'grad_norm': 3.0065348148345947, 'learning_rate': 8.79887967251966e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0009, 'grad_norm': 0.00044499049545265734, 'learning_rate': 8.793493482710332e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008011836907826364, 'learning_rate': 8.788107292901002e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0304, 'grad_norm': 1.6923566548143754e-08, 'learning_rate': 8.782721103091674e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0003, 'grad_norm': 4.91578930450487e-07, 'learning_rate': 8.777334913282344e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3999, 'grad_norm': 0.0003322166739962995, 'learning_rate': 8.771948723473016e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0001, 'grad_norm': 1.20793663427321e-06, 'learning_rate': 8.766562533663686e-05, 'epoch': 0.25}\n",
      "{'loss': 0.098, 'grad_norm': 9.933472028933465e-06, 'learning_rate': 8.761176343854358e-05, 'epoch': 0.25}\n",
      "{'loss': 0.5273, 'grad_norm': 83.73333740234375, 'learning_rate': 8.755790154045029e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1666, 'grad_norm': 9.823357686400414e-06, 'learning_rate': 8.7504039642357e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0005, 'grad_norm': 1.8143163060813094e-06, 'learning_rate': 8.745017774426372e-05, 'epoch': 0.25}\n",
      "{'loss': 0.2568, 'grad_norm': 8.557645742257591e-06, 'learning_rate': 8.739631584617041e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002343665255466476, 'learning_rate': 8.734245394807713e-05, 'epoch': 0.25}\n",
      "{'loss': 0.2922, 'grad_norm': 0.000712852634023875, 'learning_rate': 8.728859204998384e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0819, 'grad_norm': 3.4037106900336456e-13, 'learning_rate': 8.723473015189055e-05, 'epoch': 0.26}\n",
      "{'loss': 0.9255, 'grad_norm': 1.291383147239685, 'learning_rate': 8.718086825379727e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0946, 'grad_norm': 2.9998802801856073e-06, 'learning_rate': 8.712700635570398e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0988, 'grad_norm': 0.027535397559404373, 'learning_rate': 8.70731444576107e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0, 'grad_norm': 0.002565992996096611, 'learning_rate': 8.70192825595174e-05, 'epoch': 0.26}\n",
      "{'loss': 0.1003, 'grad_norm': 5.701509664390869e-08, 'learning_rate': 8.696542066142412e-05, 'epoch': 0.26}\n",
      "{'loss': 0.1055, 'grad_norm': 2.9819841074640863e-05, 'learning_rate': 8.691155876333082e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0289, 'grad_norm': 0.0002066218585241586, 'learning_rate': 8.685769686523754e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0463, 'grad_norm': 7.656074103579158e-07, 'learning_rate': 8.680383496714424e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3384, 'grad_norm': 1.4626223787672643e-07, 'learning_rate': 8.674997306905096e-05, 'epoch': 0.27}\n",
      "{'loss': 0.8055, 'grad_norm': 0.04067659378051758, 'learning_rate': 8.669611117095768e-05, 'epoch': 0.27}\n",
      "{'loss': 0.017, 'grad_norm': 25.241708755493164, 'learning_rate': 8.664224927286438e-05, 'epoch': 0.27}\n",
      "{'loss': 0.8322, 'grad_norm': 81.76362609863281, 'learning_rate': 8.65883873747711e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0, 'grad_norm': 0.039585553109645844, 'learning_rate': 8.653452547667779e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0001, 'grad_norm': 1.1182367032347429e-08, 'learning_rate': 8.648066357858451e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0185, 'grad_norm': 4.007688403362408e-05, 'learning_rate': 8.642680168049123e-05, 'epoch': 0.27}\n",
      "{'loss': 0.2841, 'grad_norm': 2.2392497101009212e-07, 'learning_rate': 8.637293978239793e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0, 'grad_norm': 7.518215170421172e-07, 'learning_rate': 8.631907788430465e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3964, 'grad_norm': 3.5505800042301416e-05, 'learning_rate': 8.626521598621135e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0001, 'grad_norm': 7.152851321734488e-05, 'learning_rate': 8.621135408811807e-05, 'epoch': 0.28}\n",
      "{'loss': 0.278, 'grad_norm': 6.677711644442752e-05, 'learning_rate': 8.615749219002478e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3634, 'grad_norm': 1.7468097212258726e-05, 'learning_rate': 8.610363029193149e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0022, 'grad_norm': 0.34922730922698975, 'learning_rate': 8.60497683938382e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0003, 'grad_norm': 2.435734131722711e-06, 'learning_rate': 8.599590649574492e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0498, 'grad_norm': 4.7303876876831055, 'learning_rate': 8.594204459765163e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0002, 'grad_norm': 0.29412102699279785, 'learning_rate': 8.588818269955834e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0, 'grad_norm': 1.2611639249371365e-05, 'learning_rate': 8.583432080146506e-05, 'epoch': 0.28}\n",
      "{'loss': 0.207, 'grad_norm': 2.7336263030974806e-08, 'learning_rate': 8.578045890337176e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0, 'grad_norm': 2.5751733119250275e-05, 'learning_rate': 8.572659700527848e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0002, 'grad_norm': 0.21634431183338165, 'learning_rate': 8.567273510718518e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3345, 'grad_norm': 4.717709316537366e-07, 'learning_rate': 8.561887320909189e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0005, 'grad_norm': 3.068632213398814e-05, 'learning_rate': 8.55650113109986e-05, 'epoch': 0.29}\n",
      "{'loss': 0.1701, 'grad_norm': 0.0007881151395849884, 'learning_rate': 8.551114941290531e-05, 'epoch': 0.29}\n",
      "{'loss': 0.7961, 'grad_norm': 104.91268920898438, 'learning_rate': 8.545728751481203e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0002, 'grad_norm': 4.5214317623276656e-08, 'learning_rate': 8.540342561671873e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0117, 'grad_norm': 3.251708236007289e-09, 'learning_rate': 8.534956371862545e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0003, 'grad_norm': 0.0015676342882215977, 'learning_rate': 8.529570182053215e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0014, 'grad_norm': 2.971588134765625, 'learning_rate': 8.524183992243887e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0, 'grad_norm': 3.6945632331253364e-08, 'learning_rate': 8.518797802434559e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0, 'grad_norm': 8.262277084725156e-09, 'learning_rate': 8.513411612625229e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0, 'grad_norm': 4.097016476078963e-11, 'learning_rate': 8.508025422815901e-05, 'epoch': 0.3}\n",
      "{'loss': 0.5227, 'grad_norm': 4.423420740806705e-09, 'learning_rate': 8.502639233006571e-05, 'epoch': 0.3}\n",
      "{'loss': 0.4877, 'grad_norm': 2.1049987708465778e-07, 'learning_rate': 8.497253043197243e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0045, 'grad_norm': 0.017298951745033264, 'learning_rate': 8.491866853387914e-05, 'epoch': 0.3}\n",
      "{'loss': 0.7961, 'grad_norm': 112.74297332763672, 'learning_rate': 8.486480663578585e-05, 'epoch': 0.3}\n",
      "{'loss': 0.487, 'grad_norm': 3.9279046177398413e-05, 'learning_rate': 8.481094473769256e-05, 'epoch': 0.3}\n",
      "{'loss': 0.1792, 'grad_norm': 1.3937670928498846e-07, 'learning_rate': 8.475708283959926e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0053, 'grad_norm': 2.80330228805542, 'learning_rate': 8.470322094150598e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0004, 'grad_norm': 1.022453943733126e-06, 'learning_rate': 8.464935904341269e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0012, 'grad_norm': 8.758674994169269e-06, 'learning_rate': 8.45954971453194e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0, 'grad_norm': 3.269962292051787e-07, 'learning_rate': 8.454163524722611e-05, 'epoch': 0.31}\n",
      "{'loss': 0.097, 'grad_norm': 3.9231247228599386e-07, 'learning_rate': 8.448777334913283e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0, 'grad_norm': 0.008051778189837933, 'learning_rate': 8.443391145103954e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0067, 'grad_norm': 2.072397137453663e-06, 'learning_rate': 8.438004955294625e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0, 'grad_norm': 1.705682706187872e-07, 'learning_rate': 8.432618765485297e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0, 'grad_norm': 8.962908395915292e-06, 'learning_rate': 8.427232575675967e-05, 'epoch': 0.31}\n",
      "{'loss': 0.2651, 'grad_norm': 5.842747119455538e-11, 'learning_rate': 8.421846385866639e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2187, 'grad_norm': 1.9545884910598943e-08, 'learning_rate': 8.416460196057309e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3526, 'grad_norm': 91.20047760009766, 'learning_rate': 8.411074006247981e-05, 'epoch': 0.32}\n",
      "{'loss': 0.9505, 'grad_norm': 0.11993977427482605, 'learning_rate': 8.405687816438651e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010592367470962927, 'learning_rate': 8.400301626629323e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0084, 'grad_norm': 0.0012716291239485145, 'learning_rate': 8.394915436819995e-05, 'epoch': 0.32}\n",
      "{'loss': 0.5965, 'grad_norm': 1.9407754734857008e-05, 'learning_rate': 8.389529247010664e-05, 'epoch': 0.32}\n",
      "{'loss': 0.035, 'grad_norm': 8.51513671875, 'learning_rate': 8.384143057201336e-05, 'epoch': 0.32}\n",
      "{'loss': 0.005, 'grad_norm': 3.270582055847626e-06, 'learning_rate': 8.378756867392006e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2997, 'grad_norm': 7.248994648989537e-08, 'learning_rate': 8.373370677582678e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0569, 'grad_norm': 0.0015600024489685893, 'learning_rate': 8.36798448777335e-05, 'epoch': 0.33}\n",
      "{'loss': 0.086, 'grad_norm': 99.68794250488281, 'learning_rate': 8.36259829796402e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0114, 'grad_norm': 1.2577953384607099e-05, 'learning_rate': 8.357212108154692e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0004, 'grad_norm': 1.0505818863748573e-06, 'learning_rate': 8.351825918345363e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0, 'grad_norm': 6.881764047506067e-10, 'learning_rate': 8.346439728536034e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3457, 'grad_norm': 7.966085831867531e-06, 'learning_rate': 8.341053538726705e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003780589031521231, 'learning_rate': 8.335667348917377e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0022, 'grad_norm': 6.342597043840215e-05, 'learning_rate': 8.330281159108047e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0, 'grad_norm': 5.890211929315825e-11, 'learning_rate': 8.324894969298719e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0, 'grad_norm': 8.59777937733952e-09, 'learning_rate': 8.31950877948939e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0054, 'grad_norm': 0.0011715699220076203, 'learning_rate': 8.314122589680061e-05, 'epoch': 0.34}\n",
      "{'loss': 0.2535, 'grad_norm': 1.1030197555328414e-07, 'learning_rate': 8.308736399870733e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0007, 'grad_norm': 1.183803915977478, 'learning_rate': 8.303350210061402e-05, 'epoch': 0.34}\n",
      "{'loss': 1.809, 'grad_norm': 6.882922232165356e-09, 'learning_rate': 8.297964020252074e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0, 'grad_norm': 0.00020155675883870572, 'learning_rate': 8.292577830442745e-05, 'epoch': 0.34}\n",
      "{'loss': 0.2879, 'grad_norm': 0.02724248170852661, 'learning_rate': 8.287191640633416e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0, 'grad_norm': 2.1102866298861045e-07, 'learning_rate': 8.281805450824088e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0, 'grad_norm': 0.002891057636588812, 'learning_rate': 8.276419261014758e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0003, 'grad_norm': 2.78438219538657e-05, 'learning_rate': 8.27103307120543e-05, 'epoch': 0.35}\n",
      "{'loss': 0.4071, 'grad_norm': 1.1674053894239478e-05, 'learning_rate': 8.2656468813961e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0015, 'grad_norm': 0.06725358963012695, 'learning_rate': 8.260260691586772e-05, 'epoch': 0.35}\n",
      "{'loss': 0.1634, 'grad_norm': 7.711040780122858e-06, 'learning_rate': 8.254874501777443e-05, 'epoch': 0.35}\n",
      "{'loss': 0.1752, 'grad_norm': 2.0596983176801587e-06, 'learning_rate': 8.249488311968114e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0015, 'grad_norm': 1.8324029445648193, 'learning_rate': 8.244102122158786e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0, 'grad_norm': 4.887971840616956e-07, 'learning_rate': 8.238715932349457e-05, 'epoch': 0.35}\n",
      "{'loss': 0.2733, 'grad_norm': 8.531546882295515e-06, 'learning_rate': 8.233329742540128e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0, 'grad_norm': 0.002133088419213891, 'learning_rate': 8.227943552730799e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0006, 'grad_norm': 3.1845147532294504e-06, 'learning_rate': 8.22255736292147e-05, 'epoch': 0.36}\n",
      "{'loss': 0.203, 'grad_norm': 0.0008316466119140387, 'learning_rate': 8.217171173112141e-05, 'epoch': 0.36}\n",
      "{'loss': 0.5926, 'grad_norm': 2.475541123203584e-07, 'learning_rate': 8.211784983302811e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0749, 'grad_norm': 4.5315269403545244e-07, 'learning_rate': 8.206398793493483e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0, 'grad_norm': 9.674707689555362e-05, 'learning_rate': 8.201012603684154e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0426, 'grad_norm': 2.8407632726157317e-06, 'learning_rate': 8.195626413874825e-05, 'epoch': 0.36}\n",
      "{'loss': 0.7675, 'grad_norm': 0.0005398649373091757, 'learning_rate': 8.190240224065496e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0004, 'grad_norm': 8.851566235534847e-06, 'learning_rate': 8.184854034256168e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0001, 'grad_norm': 7.877762982388958e-06, 'learning_rate': 8.179467844446838e-05, 'epoch': 0.36}\n",
      "{'loss': 0.2771, 'grad_norm': 1.056761266227113e-05, 'learning_rate': 8.17408165463751e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0075, 'grad_norm': 0.06502934545278549, 'learning_rate': 8.168695464828182e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0063, 'grad_norm': 5.175510153776486e-08, 'learning_rate': 8.163309275018852e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0001, 'grad_norm': 2.835741724993568e-05, 'learning_rate': 8.157923085209524e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0001, 'grad_norm': 7.223263054356721e-08, 'learning_rate': 8.152536895400194e-05, 'epoch': 0.37}\n",
      "{'loss': 0.7979, 'grad_norm': 6.120093894423917e-05, 'learning_rate': 8.147150705590866e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0001, 'grad_norm': 9.468973075854592e-06, 'learning_rate': 8.141764515781536e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0, 'grad_norm': 5.8216549803091766e-08, 'learning_rate': 8.136378325972208e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0404, 'grad_norm': 0.0022875850554555655, 'learning_rate': 8.130992136162879e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3848, 'grad_norm': 90.41204833984375, 'learning_rate': 8.125605946353549e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0, 'grad_norm': 0.0017356332391500473, 'learning_rate': 8.120219756544221e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0687, 'grad_norm': 1.799616370590229e-06, 'learning_rate': 8.114833566734891e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3796, 'grad_norm': 5.596235723714926e-07, 'learning_rate': 8.109447376925563e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0, 'grad_norm': 8.53335495776264e-06, 'learning_rate': 8.104061187116234e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0182, 'grad_norm': 2.5348035705974326e-06, 'learning_rate': 8.098674997306905e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0926, 'grad_norm': 1.372230940432928e-06, 'learning_rate': 8.093288807497577e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0, 'grad_norm': 1.968662218132522e-05, 'learning_rate': 8.087902617688248e-05, 'epoch': 0.38}\n",
      "{'loss': 0.2205, 'grad_norm': 7.279307283170056e-06, 'learning_rate': 8.08251642787892e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0008, 'grad_norm': 0.02921222150325775, 'learning_rate': 8.07713023806959e-05, 'epoch': 0.38}\n",
      "{'loss': 1.0002, 'grad_norm': 0.011465041898190975, 'learning_rate': 8.071744048260262e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0002, 'grad_norm': 7.473267515933912e-08, 'learning_rate': 8.066357858450932e-05, 'epoch': 0.39}\n",
      "{'loss': 0.026, 'grad_norm': 1.7023810983118892e-07, 'learning_rate': 8.060971668641604e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0008, 'grad_norm': 1.4002911541410867e-07, 'learning_rate': 8.055585478832274e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0004, 'grad_norm': 8.419950972893275e-06, 'learning_rate': 8.050199289022946e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0032, 'grad_norm': 1.6976244410216168e-08, 'learning_rate': 8.044813099213616e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3349, 'grad_norm': 90.29253387451172, 'learning_rate': 8.039426909404287e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0221, 'grad_norm': 3.1641133091397933e-07, 'learning_rate': 8.034040719594959e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0, 'grad_norm': 0.02618345431983471, 'learning_rate': 8.028654529785629e-05, 'epoch': 0.39}\n",
      "{'loss': 0.2563, 'grad_norm': 0.0024045598693192005, 'learning_rate': 8.023268339976301e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0102, 'grad_norm': 4.406398801393152e-09, 'learning_rate': 8.017882150166973e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7348, 'grad_norm': 2.3396974313527608e-07, 'learning_rate': 8.012495960357643e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0, 'grad_norm': 0.005509339738637209, 'learning_rate': 8.007109770548315e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0076, 'grad_norm': 3.1833149449767006e-08, 'learning_rate': 8.001723580738985e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0, 'grad_norm': 1.4058419139928446e-07, 'learning_rate': 7.996337390929657e-05, 'epoch': 0.4}\n",
      "{'loss': 0.1955, 'grad_norm': 101.80628204345703, 'learning_rate': 7.990951201120328e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0038, 'grad_norm': 2.474123675710871e-06, 'learning_rate': 7.985565011310999e-05, 'epoch': 0.4}\n",
      "{'loss': 0.1078, 'grad_norm': 1.8828460213171638e-07, 'learning_rate': 7.98017882150167e-05, 'epoch': 0.4}\n",
      "{'loss': 1.0784, 'grad_norm': 8.356610487680882e-05, 'learning_rate': 7.974792631692342e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0014, 'grad_norm': 0.0005492546479217708, 'learning_rate': 7.969406441883013e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0017, 'grad_norm': 2.9626770015056536e-07, 'learning_rate': 7.964020252073684e-05, 'epoch': 0.41}\n",
      "{'loss': 0.1322, 'grad_norm': 1.00819488579873e-05, 'learning_rate': 7.958634062264354e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3503, 'grad_norm': 5.544545729208039e-07, 'learning_rate': 7.953247872455025e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0004, 'grad_norm': 5.407441494753584e-05, 'learning_rate': 7.947861682645696e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0007, 'grad_norm': 0.00022525995154865086, 'learning_rate': 7.942475492836368e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0421, 'grad_norm': 0.0007800657767802477, 'learning_rate': 7.937089303027039e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0313, 'grad_norm': 9.51429456108599e-07, 'learning_rate': 7.93170311321771e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0, 'grad_norm': 0.004457470495253801, 'learning_rate': 7.926316923408381e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0, 'grad_norm': 7.967150850163307e-06, 'learning_rate': 7.920930733599053e-05, 'epoch': 0.42}\n",
      "{'loss': 0.1144, 'grad_norm': 0.005218185018748045, 'learning_rate': 7.915544543789723e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0003, 'grad_norm': 8.065834663284477e-06, 'learning_rate': 7.910158353980395e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0, 'grad_norm': 9.643487146604457e-07, 'learning_rate': 7.904772164171065e-05, 'epoch': 0.42}\n",
      "{'loss': 0.4947, 'grad_norm': 0.09182295203208923, 'learning_rate': 7.899385974361737e-05, 'epoch': 0.42}\n",
      "{'loss': 0.2621, 'grad_norm': 1.662026988924481e-05, 'learning_rate': 7.893999784552409e-05, 'epoch': 0.42}\n",
      "{'loss': 1.1562, 'grad_norm': 6.65362236418332e-08, 'learning_rate': 7.888613594743079e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0, 'grad_norm': 4.187071738215309e-07, 'learning_rate': 7.883227404933751e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0247, 'grad_norm': 9.813115866563749e-07, 'learning_rate': 7.877841215124421e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0001, 'grad_norm': 1.1569674825295806e-05, 'learning_rate': 7.872455025315092e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0, 'grad_norm': 2.0633308395190397e-06, 'learning_rate': 7.867068835505764e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0, 'grad_norm': 5.270851488603512e-07, 'learning_rate': 7.861682645696434e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0, 'grad_norm': 1.0941073014691938e-05, 'learning_rate': 7.856296455887106e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0001, 'grad_norm': 9.822602464737429e-08, 'learning_rate': 7.850910266077776e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00019563973182812333, 'learning_rate': 7.845524076268448e-05, 'epoch': 0.43}\n",
      "{'loss': 0.5283, 'grad_norm': 86.94995880126953, 'learning_rate': 7.840137886459119e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0129, 'grad_norm': 0.0031086495146155357, 'learning_rate': 7.83475169664979e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0004, 'grad_norm': 0.354284405708313, 'learning_rate': 7.829365506840461e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0057, 'grad_norm': 2.980442914690684e-08, 'learning_rate': 7.823979317031133e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0356, 'grad_norm': 1.1977754184044898e-06, 'learning_rate': 7.818593127221804e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0002, 'grad_norm': 1.7461787138017826e-05, 'learning_rate': 7.813206937412475e-05, 'epoch': 0.44}\n",
      "{'loss': 2.0624, 'grad_norm': 9.694309119367972e-05, 'learning_rate': 7.807820747603147e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010826997458934784, 'learning_rate': 7.802434557793817e-05, 'epoch': 0.44}\n",
      "{'loss': 0.228, 'grad_norm': 3.770705234273919e-06, 'learning_rate': 7.797048367984489e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0, 'grad_norm': 0.00024959605070762336, 'learning_rate': 7.791662178175159e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0238, 'grad_norm': 0.0003355411463417113, 'learning_rate': 7.78627598836583e-05, 'epoch': 0.44}\n",
      "{'loss': 0.1939, 'grad_norm': 0.24965745210647583, 'learning_rate': 7.780889798556501e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0089, 'grad_norm': 8.782202712609433e-06, 'learning_rate': 7.775503608747172e-05, 'epoch': 0.44}\n",
      "{'loss': 0.006, 'grad_norm': 0.025164345279335976, 'learning_rate': 7.770117418937844e-05, 'epoch': 0.45}\n",
      "{'loss': 0.1797, 'grad_norm': 0.0009447934571653605, 'learning_rate': 7.764731229128514e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0, 'grad_norm': 9.267770337828551e-07, 'learning_rate': 7.759345039319186e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0, 'grad_norm': 4.693208666139981e-07, 'learning_rate': 7.753958849509856e-05, 'epoch': 0.45}\n",
      "{'loss': 0.4994, 'grad_norm': 0.25668513774871826, 'learning_rate': 7.748572659700528e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0016, 'grad_norm': 0.00036653660936281085, 'learning_rate': 7.7431864698912e-05, 'epoch': 0.45}\n",
      "{'loss': 0.6974, 'grad_norm': 0.04998066648840904, 'learning_rate': 7.73780028008187e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2195, 'grad_norm': 0.0001137674626079388, 'learning_rate': 7.732414090272542e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0067, 'grad_norm': 0.0004995522322133183, 'learning_rate': 7.727027900463213e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0002, 'grad_norm': 0.07976575940847397, 'learning_rate': 7.721641710653884e-05, 'epoch': 0.46}\n",
      "{'loss': 1.0841, 'grad_norm': 5.406765239968081e-07, 'learning_rate': 7.716255520844555e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3537, 'grad_norm': 5.336249273568683e-07, 'learning_rate': 7.710869331035227e-05, 'epoch': 0.46}\n",
      "{'loss': 0.4181, 'grad_norm': 2.2063826691010036e-05, 'learning_rate': 7.705483141225897e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0762, 'grad_norm': 3.9134985030386815e-08, 'learning_rate': 7.700096951416569e-05, 'epoch': 0.46}\n",
      "{'loss': 0.1444, 'grad_norm': 1.8732325770542957e-06, 'learning_rate': 7.694710761607239e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0582, 'grad_norm': 56.10414123535156, 'learning_rate': 7.68932457179791e-05, 'epoch': 0.46}\n",
      "{'loss': 0.6092, 'grad_norm': 2.9775192737579346, 'learning_rate': 7.683938381988581e-05, 'epoch': 0.46}\n",
      "{'loss': 0.098, 'grad_norm': 0.0017099836841225624, 'learning_rate': 7.678552192179252e-05, 'epoch': 0.46}\n",
      "{'loss': 0.1178, 'grad_norm': 8.907465598895214e-06, 'learning_rate': 7.673166002369924e-05, 'epoch': 0.47}\n",
      "{'loss': 0.6243, 'grad_norm': 6.0512484196806327e-05, 'learning_rate': 7.667779812560595e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0028, 'grad_norm': 1.086555471374595e-06, 'learning_rate': 7.662393622751266e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0015, 'grad_norm': 0.06669501215219498, 'learning_rate': 7.657007432941938e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0107, 'grad_norm': 15.603304862976074, 'learning_rate': 7.651621243132608e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0004, 'grad_norm': 1.0212312190560624e-05, 'learning_rate': 7.64623505332328e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3219, 'grad_norm': 0.00039394196937792003, 'learning_rate': 7.64084886351395e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3341, 'grad_norm': 9.362446462546359e-08, 'learning_rate': 7.635462673704622e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0707, 'grad_norm': 1.227102518081665, 'learning_rate': 7.630076483895293e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0002, 'grad_norm': 0.00014056969666853547, 'learning_rate': 7.624690294085964e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0161, 'grad_norm': 21.015729904174805, 'learning_rate': 7.619304104276636e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0079, 'grad_norm': 4.448408503776591e-07, 'learning_rate': 7.613917914467307e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0017, 'grad_norm': 0.00012603848881553859, 'learning_rate': 7.608531724657977e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0892, 'grad_norm': 1.8928218992186885e-08, 'learning_rate': 7.603145534848647e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004106143314857036, 'learning_rate': 7.597759345039319e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0003, 'grad_norm': 3.379842610229389e-07, 'learning_rate': 7.592373155229991e-05, 'epoch': 0.48}\n",
      "{'loss': 0.1585, 'grad_norm': 0.05926641821861267, 'learning_rate': 7.586986965420661e-05, 'epoch': 0.48}\n",
      "{'loss': 0.002, 'grad_norm': 9.724122946863645e-07, 'learning_rate': 7.581600775611333e-05, 'epoch': 0.48}\n",
      "{'loss': 0.1303, 'grad_norm': 1.4009631286171498e-06, 'learning_rate': 7.576214585802004e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0, 'grad_norm': 4.946236344949284e-07, 'learning_rate': 7.570828395992675e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0003, 'grad_norm': 0.22972120344638824, 'learning_rate': 7.565442206183346e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0041, 'grad_norm': 7.510878674565902e-08, 'learning_rate': 7.560056016374018e-05, 'epoch': 0.49}\n",
      "{'loss': 0.1312, 'grad_norm': 1.6136296835611574e-05, 'learning_rate': 7.554669826564688e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0955, 'grad_norm': 4.521383762359619, 'learning_rate': 7.54928363675536e-05, 'epoch': 0.49}\n",
      "{'loss': 0.4009, 'grad_norm': 0.0011419474612921476, 'learning_rate': 7.543897446946032e-05, 'epoch': 0.49}\n",
      "{'loss': 0.023, 'grad_norm': 1.0479615397684938e-08, 'learning_rate': 7.538511257136702e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0005, 'grad_norm': 3.118109361821553e-07, 'learning_rate': 7.533125067327374e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0146, 'grad_norm': 0.45157259702682495, 'learning_rate': 7.527738877518044e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0051, 'grad_norm': 9.355569090985227e-06, 'learning_rate': 7.522352687708715e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5492, 'grad_norm': 0.004761075135320425, 'learning_rate': 7.516966497899386e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0009, 'grad_norm': 1.239591360092163, 'learning_rate': 7.511580308090057e-05, 'epoch': 0.5}\n",
      "{'loss': 0.422, 'grad_norm': 4.0752267523203045e-05, 'learning_rate': 7.506194118280729e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003358545363880694, 'learning_rate': 7.500807928471399e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6372, 'grad_norm': 1.4126420033733211e-09, 'learning_rate': 7.495421738662071e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0, 'grad_norm': 0.024739068001508713, 'learning_rate': 7.490035548852741e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0113, 'grad_norm': 4.0187936974689364e-05, 'learning_rate': 7.484649359043413e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5987, 'grad_norm': 1.151473938421077e-08, 'learning_rate': 7.479263169234084e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2558, 'grad_norm': 0.00011901716061402112, 'learning_rate': 7.473876979424755e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0002002264081966132, 'learning_rate': 7.468490789615427e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0268, 'grad_norm': 2.195134048491032e-12, 'learning_rate': 7.463104599806098e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0007, 'grad_norm': 7.602585583299515e-07, 'learning_rate': 7.45771840999677e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0003, 'grad_norm': 0.5733658671379089, 'learning_rate': 7.45233222018744e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0, 'grad_norm': 2.1185192053962965e-06, 'learning_rate': 7.446946030378112e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0, 'grad_norm': 2.773763618790781e-08, 'learning_rate': 7.441559840568782e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0, 'grad_norm': 4.0524231081917605e-08, 'learning_rate': 7.436173650759452e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0, 'grad_norm': 8.004013579920866e-06, 'learning_rate': 7.430787460950124e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3729, 'grad_norm': 0.12905971705913544, 'learning_rate': 7.425401271140795e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0, 'grad_norm': 0.004058892838656902, 'learning_rate': 7.420015081331466e-05, 'epoch': 0.52}\n",
      "{'loss': 0.001, 'grad_norm': 0.000443129101768136, 'learning_rate': 7.414628891522137e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5084, 'grad_norm': 5.845277309417725, 'learning_rate': 7.409242701712809e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0023, 'grad_norm': 0.0001122093090089038, 'learning_rate': 7.403856511903479e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9922, 'grad_norm': 0.000344431318808347, 'learning_rate': 7.398470322094151e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0, 'grad_norm': 0.01385351549834013, 'learning_rate': 7.393084132284823e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0008, 'grad_norm': 0.00014395742618944496, 'learning_rate': 7.387697942475493e-05, 'epoch': 0.52}\n",
      "{'loss': 0.2705, 'grad_norm': 3.323215787531808e-05, 'learning_rate': 7.382311752666165e-05, 'epoch': 0.52}\n",
      "{'loss': 0.225, 'grad_norm': 6.545042907646348e-08, 'learning_rate': 7.376925562856835e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005743918009102345, 'learning_rate': 7.371539373047507e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0, 'grad_norm': 2.3373586373054422e-06, 'learning_rate': 7.366153183238178e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3423, 'grad_norm': 7.568263754365034e-06, 'learning_rate': 7.360766993428849e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1033, 'grad_norm': 0.00023812537256162614, 'learning_rate': 7.355380803619521e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0001, 'grad_norm': 1.5657865333196241e-06, 'learning_rate': 7.34999461381019e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0, 'grad_norm': 0.0019410992972552776, 'learning_rate': 7.344608424000862e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00010670240590116009, 'learning_rate': 7.339222234191532e-05, 'epoch': 0.53}\n",
      "{'loss': 0.171, 'grad_norm': 92.34872436523438, 'learning_rate': 7.333836044382204e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0, 'grad_norm': 2.5641429601819254e-06, 'learning_rate': 7.328449854572875e-05, 'epoch': 0.53}\n",
      "{'loss': 0.004, 'grad_norm': 1.1804859241237864e-06, 'learning_rate': 7.323063664763546e-05, 'epoch': 0.54}\n",
      "{'loss': 0.115, 'grad_norm': 7.977660970936995e-06, 'learning_rate': 7.317677474954218e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0, 'grad_norm': 1.1710019407473737e-06, 'learning_rate': 7.312291285144889e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0002, 'grad_norm': 1.1758478422052576e-06, 'learning_rate': 7.30690509533556e-05, 'epoch': 0.54}\n",
      "{'loss': 0.1102, 'grad_norm': 0.00898817740380764, 'learning_rate': 7.301518905526231e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0, 'grad_norm': 8.327519026352093e-06, 'learning_rate': 7.296132715716903e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0, 'grad_norm': 0.0012732455506920815, 'learning_rate': 7.290746525907573e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0021, 'grad_norm': 2.96266171062598e-05, 'learning_rate': 7.285360336098245e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0001, 'grad_norm': 5.492068157764152e-05, 'learning_rate': 7.279974146288917e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0003, 'grad_norm': 9.698262193325036e-10, 'learning_rate': 7.274587956479587e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7679, 'grad_norm': 5.035932417740696e-07, 'learning_rate': 7.269201766670259e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0068, 'grad_norm': 3.5981727819489606e-08, 'learning_rate': 7.263815576860928e-05, 'epoch': 0.55}\n",
      "{'loss': 0.001, 'grad_norm': 2.8080438596589374e-07, 'learning_rate': 7.2584293870516e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0026, 'grad_norm': 0.00018214364536106586, 'learning_rate': 7.25304319724227e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0005, 'grad_norm': 7.0565638452535495e-06, 'learning_rate': 7.247657007432942e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0655, 'grad_norm': 1.9630053138541825e-09, 'learning_rate': 7.242270817623614e-05, 'epoch': 0.55}\n",
      "{'loss': 1.4152, 'grad_norm': 21.245227813720703, 'learning_rate': 7.236884627814284e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3665, 'grad_norm': 0.0005370550788938999, 'learning_rate': 7.231498438004956e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0001, 'grad_norm': 1.1747738426493015e-05, 'learning_rate': 7.226112248195626e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9106, 'grad_norm': 0.0011632349342107773, 'learning_rate': 7.220726058386298e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0069, 'grad_norm': 9.385705925524235e-05, 'learning_rate': 7.215339868576969e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0004, 'grad_norm': 3.476859274087474e-05, 'learning_rate': 7.20995367876764e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0888, 'grad_norm': 7.711381840636022e-06, 'learning_rate': 7.204567488958311e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7267, 'grad_norm': 107.49884033203125, 'learning_rate': 7.199181299148983e-05, 'epoch': 0.56}\n",
      "{'loss': 0.019, 'grad_norm': 0.001781604252755642, 'learning_rate': 7.193795109339654e-05, 'epoch': 0.56}\n",
      "{'loss': 0.1822, 'grad_norm': 0.00011271159746684134, 'learning_rate': 7.188408919530325e-05, 'epoch': 0.56}\n",
      "{'loss': 0.1101, 'grad_norm': 0.0016936216270551085, 'learning_rate': 7.183022729720997e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0164, 'grad_norm': 1.231256828759797e-05, 'learning_rate': 7.177636539911666e-05, 'epoch': 0.56}\n",
      "{'loss': 0.1122, 'grad_norm': 6.388218025676906e-05, 'learning_rate': 7.172250350102337e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3482, 'grad_norm': 6.97747793765302e-07, 'learning_rate': 7.166864160293009e-05, 'epoch': 0.57}\n",
      "{'loss': 0.01, 'grad_norm': 0.30776315927505493, 'learning_rate': 7.16147797048368e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0048, 'grad_norm': 7.509510760428384e-05, 'learning_rate': 7.156091780674351e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0, 'grad_norm': 0.00016035720182117075, 'learning_rate': 7.150705590865022e-05, 'epoch': 0.57}\n",
      "{'loss': 0.8603, 'grad_norm': 8.224486009567045e-06, 'learning_rate': 7.145319401055694e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0003174680459778756, 'learning_rate': 7.139933211246364e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2981, 'grad_norm': 2.4774459461696097e-07, 'learning_rate': 7.134547021437036e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0228, 'grad_norm': 1.977470856218133e-05, 'learning_rate': 7.129160831627706e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0007, 'grad_norm': 1.1078171730041504, 'learning_rate': 7.123774641818378e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0025, 'grad_norm': 4.5615234375, 'learning_rate': 7.11838845200905e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0002, 'grad_norm': 2.8591692171175964e-05, 'learning_rate': 7.11300226219972e-05, 'epoch': 0.58}\n",
      "{'loss': 0.1378, 'grad_norm': 2.7199266696698032e-06, 'learning_rate': 7.107616072390392e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0011, 'grad_norm': 3.2952601031865925e-05, 'learning_rate': 7.102229882581063e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0, 'grad_norm': 1.0186590770899784e-06, 'learning_rate': 7.096843692771734e-05, 'epoch': 0.58}\n",
      "{'loss': 0.4955, 'grad_norm': 1.5560048041152186e-06, 'learning_rate': 7.091457502962405e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0011, 'grad_norm': 8.234837878262624e-05, 'learning_rate': 7.086071313153075e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0003, 'grad_norm': 1.786991177255004e-08, 'learning_rate': 7.080685123343747e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0404, 'grad_norm': 0.00011553493095561862, 'learning_rate': 7.075298933534417e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0001, 'grad_norm': 2.094166302413214e-05, 'learning_rate': 7.069912743725089e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5032, 'grad_norm': 1.3186854630475864e-06, 'learning_rate': 7.06452655391576e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0027, 'grad_norm': 7.442046353389742e-06, 'learning_rate': 7.059140364106431e-05, 'epoch': 0.59}\n",
      "{'loss': 0.1956, 'grad_norm': 1.288449880121334e-06, 'learning_rate': 7.053754174297102e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6783, 'grad_norm': 6.724766965504614e-09, 'learning_rate': 7.048367984487774e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0, 'grad_norm': 8.91255240276223e-07, 'learning_rate': 7.042981794678445e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0, 'grad_norm': 1.5337375316448743e-06, 'learning_rate': 7.037595604869116e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0006, 'grad_norm': 1.160352098850126e-06, 'learning_rate': 7.032209415059788e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2281, 'grad_norm': 1.2847080732569793e-08, 'learning_rate': 7.026823225250458e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0, 'grad_norm': 0.0021721478551626205, 'learning_rate': 7.02143703544113e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0, 'grad_norm': 0.013700243085622787, 'learning_rate': 7.0160508456318e-05, 'epoch': 0.6}\n",
      "{'loss': 0.1363, 'grad_norm': 1.3261396247798984e-07, 'learning_rate': 7.010664655822472e-05, 'epoch': 0.6}\n",
      "{'loss': 0.2837, 'grad_norm': 8.917801096686162e-06, 'learning_rate': 7.005278466013144e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6001, 'grad_norm': 0.0033440024126321077, 'learning_rate': 6.999892276203813e-05, 'epoch': 0.6}\n",
      "{'loss': 0.001, 'grad_norm': 1.2466251064324751e-05, 'learning_rate': 6.994506086394485e-05, 'epoch': 0.6}\n",
      "{'loss': 0.4062, 'grad_norm': 7.602230880365823e-07, 'learning_rate': 6.989119896585155e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0003, 'grad_norm': 8.296981832245365e-06, 'learning_rate': 6.983733706775827e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0, 'grad_norm': 1.9474297005217522e-05, 'learning_rate': 6.978347516966497e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0008, 'grad_norm': 3.685345291160047e-06, 'learning_rate': 6.972961327157169e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0, 'grad_norm': 0.004738117568194866, 'learning_rate': 6.967575137347841e-05, 'epoch': 0.61}\n",
      "{'loss': 0.003, 'grad_norm': 0.00019332415831740946, 'learning_rate': 6.962188947538511e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0253, 'grad_norm': 28.218183517456055, 'learning_rate': 6.956802757729183e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0008661722531542182, 'learning_rate': 6.951416567919854e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7408, 'grad_norm': 0.0030239159241318703, 'learning_rate': 6.946030378110525e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0, 'grad_norm': 4.1091949242400005e-05, 'learning_rate': 6.940644188301196e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0441, 'grad_norm': 1.9754654856374287e-10, 'learning_rate': 6.935257998491868e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0001, 'grad_norm': 0.20660525560379028, 'learning_rate': 6.92987180868254e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0151, 'grad_norm': 8.921971357267466e-07, 'learning_rate': 6.92448561887321e-05, 'epoch': 0.62}\n",
      "{'loss': 0.8012, 'grad_norm': 107.11544799804688, 'learning_rate': 6.919099429063882e-05, 'epoch': 0.62}\n",
      "{'loss': 0.1039, 'grad_norm': 2.9623302921777395e-08, 'learning_rate': 6.91371323925455e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0, 'grad_norm': 9.582320359413643e-08, 'learning_rate': 6.908327049445222e-05, 'epoch': 0.62}\n",
      "{'loss': 0.8324, 'grad_norm': 0.018979335203766823, 'learning_rate': 6.902940859635893e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0803, 'grad_norm': 0.18171994388103485, 'learning_rate': 6.897554669826565e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0059, 'grad_norm': 15.515717506408691, 'learning_rate': 6.892168480017236e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0041, 'grad_norm': 4.237004031892866e-05, 'learning_rate': 6.886782290207907e-05, 'epoch': 0.62}\n",
      "{'loss': 0.1884, 'grad_norm': 2.6786710805026814e-06, 'learning_rate': 6.881396100398579e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2531, 'grad_norm': 87.0274429321289, 'learning_rate': 6.876009910589249e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0208, 'grad_norm': 3.1924455470289104e-06, 'learning_rate': 6.870623720779921e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0001, 'grad_norm': 1.7731540680188118e-08, 'learning_rate': 6.865237530970591e-05, 'epoch': 0.63}\n",
      "{'loss': 0.4028, 'grad_norm': 5.422247113529011e-07, 'learning_rate': 6.859851341161263e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0001, 'grad_norm': 2.2924521658751473e-07, 'learning_rate': 6.854465151351935e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0027, 'grad_norm': 2.0325316540947824e-08, 'learning_rate': 6.849078961542605e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0, 'grad_norm': 0.0015091797104105353, 'learning_rate': 6.843692771733277e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0, 'grad_norm': 5.166591154903699e-09, 'learning_rate': 6.838306581923948e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0004, 'grad_norm': 2.3680602225795155e-06, 'learning_rate': 6.83292039211462e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2342, 'grad_norm': 4.5775174761786275e-09, 'learning_rate': 6.827534202305288e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0106, 'grad_norm': 7.971597142386599e-07, 'learning_rate': 6.82214801249596e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0828, 'grad_norm': 0.0005500574479810894, 'learning_rate': 6.816761822686632e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0263, 'grad_norm': 8.150171204590606e-09, 'learning_rate': 6.811375632877302e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011178765998920426, 'learning_rate': 6.805989443067974e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0, 'grad_norm': 4.5837666995396376e-09, 'learning_rate': 6.800603253258645e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0003, 'grad_norm': 1.952713319042232e-06, 'learning_rate': 6.795217063449316e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0, 'grad_norm': 7.638809620402753e-05, 'learning_rate': 6.789830873639987e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0, 'grad_norm': 3.2223826565314084e-05, 'learning_rate': 6.784444683830659e-05, 'epoch': 0.64}\n",
      "{'loss': 0.1568, 'grad_norm': 5.557607801165432e-05, 'learning_rate': 6.779058494021329e-05, 'epoch': 0.64}\n",
      "{'loss': 0.4304, 'grad_norm': 8.813307204036391e-07, 'learning_rate': 6.773672304212001e-05, 'epoch': 0.65}\n",
      "{'loss': 0.019, 'grad_norm': 2.6718096179934037e-08, 'learning_rate': 6.768286114402673e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0002, 'grad_norm': 0.000589714152738452, 'learning_rate': 6.762899924593343e-05, 'epoch': 0.65}\n",
      "{'loss': 0.1431, 'grad_norm': 2.2243169806035468e-10, 'learning_rate': 6.757513734784015e-05, 'epoch': 0.65}\n",
      "{'loss': 0.041, 'grad_norm': 0.10028476268053055, 'learning_rate': 6.752127544974685e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0, 'grad_norm': 9.893123653625935e-09, 'learning_rate': 6.746741355165357e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0001, 'grad_norm': 8.677844743942842e-05, 'learning_rate': 6.741355165356028e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0017, 'grad_norm': 3.483871635623359e-09, 'learning_rate': 6.735968975546698e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0, 'grad_norm': 7.0685031161588086e-09, 'learning_rate': 6.73058278573737e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0, 'grad_norm': 1.2348574784937227e-07, 'learning_rate': 6.72519659592804e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0028, 'grad_norm': 1.0360108717577532e-06, 'learning_rate': 6.719810406118712e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0572, 'grad_norm': 5.228147248459436e-09, 'learning_rate': 6.714424216309382e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0005, 'grad_norm': 2.695992946624756, 'learning_rate': 6.709038026500054e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0, 'grad_norm': 2.0437916248283727e-08, 'learning_rate': 6.703651836690725e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0, 'grad_norm': 2.267361765007081e-07, 'learning_rate': 6.698265646881396e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0, 'grad_norm': 2.0487873897412356e-10, 'learning_rate': 6.692879457072068e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0, 'grad_norm': 3.6210954235116333e-09, 'learning_rate': 6.687493267262739e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0, 'grad_norm': 1.2089750311972125e-09, 'learning_rate': 6.68210707745341e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0, 'grad_norm': 2.2906213416717947e-05, 'learning_rate': 6.676720887644081e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0961, 'grad_norm': 2.5055622021596946e-09, 'learning_rate': 6.671334697834753e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0006, 'grad_norm': 5.737366581826109e-09, 'learning_rate': 6.665948508025423e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0, 'grad_norm': 6.9947923009294755e-09, 'learning_rate': 6.660562318216095e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3718, 'grad_norm': 85.07713317871094, 'learning_rate': 6.655176128406765e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3776, 'grad_norm': 7.95471564174477e-08, 'learning_rate': 6.649789938597436e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0, 'grad_norm': 8.086966118980854e-08, 'learning_rate': 6.644403748788107e-05, 'epoch': 0.67}\n",
      "{'loss': 0.1496, 'grad_norm': 0.001050902414135635, 'learning_rate': 6.639017558978778e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0, 'grad_norm': 4.921864160678524e-07, 'learning_rate': 6.63363136916945e-05, 'epoch': 0.67}\n",
      "{'loss': 0.586, 'grad_norm': 3.1507996212098988e-09, 'learning_rate': 6.62824517936012e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0, 'grad_norm': 2.9185268431319855e-05, 'learning_rate': 6.622858989550792e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0, 'grad_norm': 5.604543957815622e-07, 'learning_rate': 6.617472799741464e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0, 'grad_norm': 2.832948484865483e-05, 'learning_rate': 6.612086609932134e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0001, 'grad_norm': 0.1492224633693695, 'learning_rate': 6.606700420122806e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0, 'grad_norm': 6.450216574194201e-07, 'learning_rate': 6.601314230313476e-05, 'epoch': 0.68}\n",
      "{'loss': 0.573, 'grad_norm': 1.5037643663617928e-07, 'learning_rate': 6.595928040504148e-05, 'epoch': 0.68}\n",
      "{'loss': 0.099, 'grad_norm': 3.67144430413191e-08, 'learning_rate': 6.590541850694819e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0, 'grad_norm': 4.684284431277774e-05, 'learning_rate': 6.58515566088549e-05, 'epoch': 0.68}\n",
      "{'loss': 0.349, 'grad_norm': 4.306402683258057, 'learning_rate': 6.579769471076162e-05, 'epoch': 0.68}\n",
      "{'loss': 0.2382, 'grad_norm': 0.0009350174805149436, 'learning_rate': 6.574383281266833e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0008, 'grad_norm': 4.985234909327119e-07, 'learning_rate': 6.568997091457503e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0003, 'grad_norm': 7.322655619645957e-06, 'learning_rate': 6.563610901648173e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0, 'grad_norm': 1.7034159100148827e-05, 'learning_rate': 6.558224711838845e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7202, 'grad_norm': 0.00025249607278965414, 'learning_rate': 6.552838522029516e-05, 'epoch': 0.69}\n",
      "{'loss': 0.1847, 'grad_norm': 0.0001623534335521981, 'learning_rate': 6.547452332220187e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0, 'grad_norm': 0.0032066195271909237, 'learning_rate': 6.542066142410859e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0015, 'grad_norm': 1.8087213902617805e-05, 'learning_rate': 6.53667995260153e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0014, 'grad_norm': 0.004394622519612312, 'learning_rate': 6.531293762792201e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0786, 'grad_norm': 0.00038560241227969527, 'learning_rate': 6.525907572982872e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0316, 'grad_norm': 1.6053156286943704e-05, 'learning_rate': 6.520521383173544e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3411, 'grad_norm': 8.882229280970932e-07, 'learning_rate': 6.515135193364214e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0032, 'grad_norm': 7.741947047179565e-05, 'learning_rate': 6.509749003554886e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0123, 'grad_norm': 2.873775031275727e-07, 'learning_rate': 6.504362813745558e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0005, 'grad_norm': 1.9396242350921966e-05, 'learning_rate': 6.498976623936228e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0003, 'grad_norm': 0.6751964688301086, 'learning_rate': 6.4935904341269e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001081590962712653, 'learning_rate': 6.48820424431757e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0977, 'grad_norm': 3.587757601053454e-05, 'learning_rate': 6.482818054508241e-05, 'epoch': 0.7}\n",
      "{'loss': 0.1908, 'grad_norm': 1.0544318485017357e-07, 'learning_rate': 6.477431864698911e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0004, 'grad_norm': 5.417390639195219e-05, 'learning_rate': 6.472045674889583e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0, 'grad_norm': 3.6857431950920727e-07, 'learning_rate': 6.466659485080255e-05, 'epoch': 0.71}\n",
      "{'loss': 0.404, 'grad_norm': 9.470200893701985e-06, 'learning_rate': 6.461273295270925e-05, 'epoch': 0.71}\n",
      "{'loss': 0.1792, 'grad_norm': 0.20096401870250702, 'learning_rate': 6.455887105461597e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0077, 'grad_norm': 6.96211051940918, 'learning_rate': 6.450500915652267e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0002, 'grad_norm': 0.006498303264379501, 'learning_rate': 6.445114725842939e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0001, 'grad_norm': 9.024441169458441e-06, 'learning_rate': 6.43972853603361e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0, 'grad_norm': 1.1961687960138079e-05, 'learning_rate': 6.434342346224281e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0003, 'grad_norm': 2.177720972440511e-07, 'learning_rate': 6.428956156414953e-05, 'epoch': 0.71}\n",
      "{'loss': 1.199, 'grad_norm': 0.010831966064870358, 'learning_rate': 6.423569966605624e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0039, 'grad_norm': 8.144070875459875e-07, 'learning_rate': 6.418183776796295e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0, 'grad_norm': 1.4730637303728145e-05, 'learning_rate': 6.412797586986966e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0016, 'grad_norm': 2.487963914871216, 'learning_rate': 6.407411397177638e-05, 'epoch': 0.72}\n",
      "{'loss': 0.5109, 'grad_norm': 0.017722994089126587, 'learning_rate': 6.402025207368308e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0012, 'grad_norm': 8.341644388565328e-07, 'learning_rate': 6.396639017558978e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0049, 'grad_norm': 9.54778158757108e-08, 'learning_rate': 6.39125282774965e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011155864922329783, 'learning_rate': 6.385866637940321e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0, 'grad_norm': 0.0025269323959946632, 'learning_rate': 6.380480448130992e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0, 'grad_norm': 2.2892816531339122e-08, 'learning_rate': 6.375094258321663e-05, 'epoch': 0.72}\n",
      "{'loss': 0.8637, 'grad_norm': 5.449459422379732e-05, 'learning_rate': 6.369708068512335e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0013, 'grad_norm': 7.954005013743881e-06, 'learning_rate': 6.364321878703005e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5808, 'grad_norm': 5.776370016974397e-05, 'learning_rate': 6.358935688893677e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0312, 'grad_norm': 8.729925866646226e-06, 'learning_rate': 6.353549499084349e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0009, 'grad_norm': 9.191648132400587e-06, 'learning_rate': 6.348163309275019e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0132, 'grad_norm': 19.578861236572266, 'learning_rate': 6.342777119465691e-05, 'epoch': 0.73}\n",
      "{'loss': 0.306, 'grad_norm': 0.0003064860065933317, 'learning_rate': 6.337390929656361e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0009, 'grad_norm': 0.8574725985527039, 'learning_rate': 6.332004739847033e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0001, 'grad_norm': 2.6065450583701022e-05, 'learning_rate': 6.326618550037704e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0137, 'grad_norm': 1.9523039895830152e-07, 'learning_rate': 6.321232360228375e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0, 'grad_norm': 4.922208245261572e-05, 'learning_rate': 6.315846170419046e-05, 'epoch': 0.74}\n",
      "{'loss': 0.4899, 'grad_norm': 1.712144512566738e-05, 'learning_rate': 6.310459980609716e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0003, 'grad_norm': 1.0703809749657012e-07, 'learning_rate': 6.305073790800388e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0015, 'grad_norm': 0.0001404304348398, 'learning_rate': 6.299687600991058e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0004, 'grad_norm': 0.027921654284000397, 'learning_rate': 6.29430141118173e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0, 'grad_norm': 1.4056735153644695e-06, 'learning_rate': 6.2889152213724e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0, 'grad_norm': 0.0011225303169339895, 'learning_rate': 6.283529031563072e-05, 'epoch': 0.74}\n",
      "{'loss': 0.2355, 'grad_norm': 8.99539008969441e-06, 'learning_rate': 6.278142841753743e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0, 'grad_norm': 3.4558904626891263e-09, 'learning_rate': 6.272756651944415e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0518, 'grad_norm': 4.1965130037624476e-08, 'learning_rate': 6.267370462135086e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0005, 'grad_norm': 1.0328967571258545, 'learning_rate': 6.261984272325757e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0, 'grad_norm': 2.290717873165704e-07, 'learning_rate': 6.256598082516429e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0, 'grad_norm': 0.001181944040581584, 'learning_rate': 6.251211892707099e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0, 'grad_norm': 1.9447945760475704e-06, 'learning_rate': 6.245825702897771e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0, 'grad_norm': 0.0010035975137725472, 'learning_rate': 6.240439513088441e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0003, 'grad_norm': 1.3081085853627883e-05, 'learning_rate': 6.235053323279113e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6881, 'grad_norm': 0.0018026574980467558, 'learning_rate': 6.229667133469785e-05, 'epoch': 0.75}\n",
      "{'loss': 0.4773, 'grad_norm': 0.00344968237914145, 'learning_rate': 6.224280943660455e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0, 'grad_norm': 5.12953635123381e-09, 'learning_rate': 6.218894753851126e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0001, 'grad_norm': 7.376904250122607e-06, 'learning_rate': 6.213508564041796e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0, 'grad_norm': 0.0012976385187357664, 'learning_rate': 6.208122374232468e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0, 'grad_norm': 4.034333755953412e-07, 'learning_rate': 6.202736184423138e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0, 'grad_norm': 0.00040137689211405814, 'learning_rate': 6.19734999461381e-05, 'epoch': 0.76}\n",
      "{'loss': 0.4066, 'grad_norm': 2.989076719472905e-08, 'learning_rate': 6.191963804804482e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0001, 'grad_norm': 3.23051935993135e-05, 'learning_rate': 6.186577614995152e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0, 'grad_norm': 1.2874771755377878e-06, 'learning_rate': 6.181191425185824e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0002, 'grad_norm': 1.6337792230203263e-09, 'learning_rate': 6.175805235376495e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0245, 'grad_norm': 7.5753609962703194e-06, 'learning_rate': 6.170419045567166e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5615, 'grad_norm': 1.3782385233085392e-09, 'learning_rate': 6.165032855757837e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0, 'grad_norm': 3.4540479987299477e-07, 'learning_rate': 6.159646665948509e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0496, 'grad_norm': 57.425106048583984, 'learning_rate': 6.15426047613918e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5979, 'grad_norm': 0.0012361633125692606, 'learning_rate': 6.148874286329851e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0, 'grad_norm': 7.236939381094487e-10, 'learning_rate': 6.143488096520523e-05, 'epoch': 0.77}\n",
      "{'loss': 0.1338, 'grad_norm': 0.005941045470535755, 'learning_rate': 6.138101906711193e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0154, 'grad_norm': 2.413597485428909e-06, 'learning_rate': 6.132715716901864e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0002, 'grad_norm': 1.2400438436088734e-06, 'learning_rate': 6.127329527092534e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0002, 'grad_norm': 0.24560333788394928, 'learning_rate': 6.121943337283206e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4784, 'grad_norm': 119.30261993408203, 'learning_rate': 6.116557147473878e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0, 'grad_norm': 2.9622586605881907e-09, 'learning_rate': 6.111170957664548e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0044, 'grad_norm': 0.0001677741383900866, 'learning_rate': 6.10578476785522e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0, 'grad_norm': 1.2927161563425216e-09, 'learning_rate': 6.100398578045891e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0, 'grad_norm': 8.139386409311555e-07, 'learning_rate': 6.095012388236562e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4825, 'grad_norm': 0.0006170482956804335, 'learning_rate': 6.089626198427233e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4904, 'grad_norm': 0.003478162456303835, 'learning_rate': 6.084240008617904e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0, 'grad_norm': 0.00019642108236439526, 'learning_rate': 6.078853818808575e-05, 'epoch': 0.78}\n",
      "{'loss': 0.005, 'grad_norm': 0.0005375904147513211, 'learning_rate': 6.0734676289992464e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011059035023208708, 'learning_rate': 6.0680814391899175e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3944, 'grad_norm': 0.0001930471189552918, 'learning_rate': 6.0626952493805886e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0, 'grad_norm': 5.07002005178947e-05, 'learning_rate': 6.05730905957126e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0, 'grad_norm': 9.036888513946906e-05, 'learning_rate': 6.0519228697619315e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0928816944360733, 'learning_rate': 6.046536679952601e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0032, 'grad_norm': 0.0009126374498009682, 'learning_rate': 6.0411504901432724e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0449, 'grad_norm': 2.6564878768953193e-11, 'learning_rate': 6.0357643003339435e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003605223319027573, 'learning_rate': 6.0303781105246146e-05, 'epoch': 0.79}\n",
      "{'loss': 0.1903, 'grad_norm': 0.03399126976728439, 'learning_rate': 6.0249919207152864e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0033, 'grad_norm': 0.00011242623440921307, 'learning_rate': 6.0196057309059575e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0, 'grad_norm': 7.52708365325816e-06, 'learning_rate': 6.0142195410966286e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0, 'grad_norm': 1.5565196008537896e-05, 'learning_rate': 6.0088333512873e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5577, 'grad_norm': 0.0950622409582138, 'learning_rate': 6.003447161477971e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0, 'grad_norm': 0.007191576063632965, 'learning_rate': 5.998060971668642e-05, 'epoch': 0.8}\n",
      "{'loss': 0.2337, 'grad_norm': 9.545788088871632e-06, 'learning_rate': 5.992674781859313e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3409, 'grad_norm': 1.8588871955871582, 'learning_rate': 5.987288592049984e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0118, 'grad_norm': 0.013755647465586662, 'learning_rate': 5.981902402240655e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0004751306842081249, 'learning_rate': 5.976516212431327e-05, 'epoch': 0.8}\n",
      "{'loss': 0.043, 'grad_norm': 1.699990548331698e-07, 'learning_rate': 5.971130022621998e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0343, 'grad_norm': 9.31878716414758e-08, 'learning_rate': 5.965743832812669e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3232, 'grad_norm': 0.00013569141447078437, 'learning_rate': 5.960357643003339e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010739501158241183, 'learning_rate': 5.95497145319401e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0003, 'grad_norm': 0.02556157298386097, 'learning_rate': 5.949585263384682e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0696, 'grad_norm': 3.712368652486475e-07, 'learning_rate': 5.944199073575353e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5973, 'grad_norm': 0.030818389728665352, 'learning_rate': 5.938812883766024e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0, 'grad_norm': 0.0036794908810406923, 'learning_rate': 5.933426693956695e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0328, 'grad_norm': 0.00028630613815039396, 'learning_rate': 5.928040504147366e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0349, 'grad_norm': 3.606805965006288e-09, 'learning_rate': 5.9226543143380374e-05, 'epoch': 0.82}\n",
      "{'loss': 0.4711, 'grad_norm': 0.013868358917534351, 'learning_rate': 5.9172681245287085e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0009, 'grad_norm': 0.00016217441589105874, 'learning_rate': 5.9118819347193797e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0, 'grad_norm': 8.01691797391868e-08, 'learning_rate': 5.906495744910051e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0, 'grad_norm': 8.438627446594182e-06, 'learning_rate': 5.9011095551007226e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0243, 'grad_norm': 0.002243941882625222, 'learning_rate': 5.895723365291394e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0, 'grad_norm': 3.4265969588886946e-05, 'learning_rate': 5.890337175482065e-05, 'epoch': 0.82}\n",
      "{'loss': 0.1685, 'grad_norm': 8.3293309671717e-07, 'learning_rate': 5.884950985672736e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0365, 'grad_norm': 47.275421142578125, 'learning_rate': 5.879564795863407e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0048, 'grad_norm': 1.824342143663671e-05, 'learning_rate': 5.8741786060540774e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0016, 'grad_norm': 3.213819582015276e-05, 'learning_rate': 5.8687924162447485e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0839, 'grad_norm': 2.6832469757209765e-07, 'learning_rate': 5.8634062264354196e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0071, 'grad_norm': 9.300621968577616e-06, 'learning_rate': 5.858020036626091e-05, 'epoch': 0.83}\n",
      "{'loss': 0.4363, 'grad_norm': 83.7773666381836, 'learning_rate': 5.852633846816762e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0, 'grad_norm': 2.838903355950606e-06, 'learning_rate': 5.847247657007433e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0001, 'grad_norm': 5.5109430832089856e-05, 'learning_rate': 5.841861467198104e-05, 'epoch': 0.83}\n",
      "{'loss': 0.3176, 'grad_norm': 2.2559663026555654e-09, 'learning_rate': 5.836475277388775e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0402, 'grad_norm': 9.146509546553716e-05, 'learning_rate': 5.831089087579446e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0172, 'grad_norm': 2.019004625708476e-07, 'learning_rate': 5.825702897770118e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011890394671354443, 'learning_rate': 5.820316707960789e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7189, 'grad_norm': 2.855372940757661e-06, 'learning_rate': 5.81493051815146e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0471, 'grad_norm': 5.499658186636225e-07, 'learning_rate': 5.8095443283421314e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0, 'grad_norm': 4.63324525412645e-08, 'learning_rate': 5.8041581385328025e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7662, 'grad_norm': 2.7254371643066406, 'learning_rate': 5.7987719487234736e-05, 'epoch': 0.84}\n",
      "{'loss': 0.1768, 'grad_norm': 1.0137496246898081e-05, 'learning_rate': 5.793385758914145e-05, 'epoch': 0.84}\n",
      "{'loss': 0.2521, 'grad_norm': 5.186141152080381e-07, 'learning_rate': 5.787999569104815e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0084, 'grad_norm': 0.0007382857729680836, 'learning_rate': 5.782613379295486e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0, 'grad_norm': 2.5554400053806603e-06, 'learning_rate': 5.7772271894861574e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0, 'grad_norm': 1.887179905679659e-06, 'learning_rate': 5.7718409996768285e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0002, 'grad_norm': 0.001070090918801725, 'learning_rate': 5.7664548098674996e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0, 'grad_norm': 2.701561925277929e-07, 'learning_rate': 5.761068620058171e-05, 'epoch': 0.85}\n",
      "{'loss': 0.1164, 'grad_norm': 8.928866009227931e-06, 'learning_rate': 5.755682430248842e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0026, 'grad_norm': 0.00010470132838236168, 'learning_rate': 5.7502962404395136e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0005, 'grad_norm': 0.005917564034461975, 'learning_rate': 5.744910050630185e-05, 'epoch': 0.85}\n",
      "{'loss': 0.1345, 'grad_norm': 2.969135039165849e-06, 'learning_rate': 5.739523860820856e-05, 'epoch': 0.85}\n",
      "{'loss': 0.003, 'grad_norm': 0.00042174081318080425, 'learning_rate': 5.734137671011527e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0, 'grad_norm': 0.003727550385519862, 'learning_rate': 5.728751481202198e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0, 'grad_norm': 1.4773622751818039e-06, 'learning_rate': 5.723365291392869e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010254618246108294, 'learning_rate': 5.71797910158354e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0007, 'grad_norm': 0.6073659658432007, 'learning_rate': 5.7125929117742114e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0779, 'grad_norm': 7.703085884713801e-07, 'learning_rate': 5.707206721964883e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0002, 'grad_norm': 3.5803954233415425e-05, 'learning_rate': 5.701820532155553e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0054, 'grad_norm': 5.982462880638195e-07, 'learning_rate': 5.696434342346224e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0, 'grad_norm': 3.984386580668797e-07, 'learning_rate': 5.691048152536895e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006008146447129548, 'learning_rate': 5.685661962727566e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0, 'grad_norm': 1.2543073246717995e-08, 'learning_rate': 5.6802757729182373e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0162, 'grad_norm': 8.964653534349054e-05, 'learning_rate': 5.674889583108909e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0435, 'grad_norm': 1.0842895648011108e-07, 'learning_rate': 5.66950339329958e-05, 'epoch': 0.87}\n",
      "{'loss': 1.2413, 'grad_norm': 83.8661117553711, 'learning_rate': 5.6641172034902513e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0, 'grad_norm': 1.2519176380010322e-06, 'learning_rate': 5.6587310136809225e-05, 'epoch': 0.87}\n",
      "{'loss': 0.1398, 'grad_norm': 3.1874881756266404e-07, 'learning_rate': 5.6533448238715936e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0, 'grad_norm': 2.8550372732638607e-08, 'learning_rate': 5.647958634062265e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0, 'grad_norm': 3.4530822290435026e-07, 'learning_rate': 5.642572444252936e-05, 'epoch': 0.87}\n",
      "{'loss': 0.4185, 'grad_norm': 2.764609052974265e-05, 'learning_rate': 5.637186254443607e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0, 'grad_norm': 1.2326539433615835e-07, 'learning_rate': 5.631800064634278e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0001, 'grad_norm': 1.0063407898996957e-05, 'learning_rate': 5.62641387482495e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0, 'grad_norm': 6.92852700012736e-05, 'learning_rate': 5.621027685015621e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0027, 'grad_norm': 0.001192567404359579, 'learning_rate': 5.6156414952062907e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0001, 'grad_norm': 0.004564918112009764, 'learning_rate': 5.610255305396962e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0, 'grad_norm': 2.3132743081077933e-06, 'learning_rate': 5.604869115587633e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0007, 'grad_norm': 7.952336744665445e-08, 'learning_rate': 5.5994829257783047e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0002, 'grad_norm': 7.966843185158723e-08, 'learning_rate': 5.594096735968976e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0009, 'grad_norm': 8.56429833220318e-05, 'learning_rate': 5.588710546159647e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0, 'grad_norm': 2.7407022571424022e-06, 'learning_rate': 5.583324356350318e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0, 'grad_norm': 8.57188060763292e-05, 'learning_rate': 5.577938166540989e-05, 'epoch': 0.88}\n",
      "{'loss': 0.7839, 'grad_norm': 1.5412917946378002e-06, 'learning_rate': 5.57255197673166e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0, 'grad_norm': 1.2021601833112072e-05, 'learning_rate': 5.567165786922331e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0002238776651211083, 'learning_rate': 5.5617795971130024e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0, 'grad_norm': 1.710710239422042e-05, 'learning_rate': 5.5563934073036735e-05, 'epoch': 0.89}\n",
      "{'loss': 0.4061, 'grad_norm': 109.81492614746094, 'learning_rate': 5.551007217494345e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0, 'grad_norm': 2.274006334346268e-07, 'learning_rate': 5.5456210276850164e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0217, 'grad_norm': 0.0022861992474645376, 'learning_rate': 5.5402348378756875e-05, 'epoch': 0.89}\n",
      "{'loss': 0.8841, 'grad_norm': 0.10964801162481308, 'learning_rate': 5.5348486480663586e-05, 'epoch': 0.89}\n",
      "{'loss': 0.2735, 'grad_norm': 0.005324758589267731, 'learning_rate': 5.5294624582570284e-05, 'epoch': 0.89}\n",
      "{'loss': 0.1051, 'grad_norm': 0.0001062901719706133, 'learning_rate': 5.5240762684477e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0054, 'grad_norm': 7.47178864912712e-06, 'learning_rate': 5.518690078638371e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3901, 'grad_norm': 5.416915769274055e-07, 'learning_rate': 5.5133038888290424e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0, 'grad_norm': 8.111170245683752e-06, 'learning_rate': 5.5079176990197135e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0, 'grad_norm': 8.992319635581225e-05, 'learning_rate': 5.5025315092103846e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6562, 'grad_norm': 1.1835935765702743e-05, 'learning_rate': 5.497145319401056e-05, 'epoch': 0.9}\n",
      "{'loss': 0.1252, 'grad_norm': 0.00011610372894210741, 'learning_rate': 5.491759129591727e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0, 'grad_norm': 7.047948020044714e-05, 'learning_rate': 5.486372939782398e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0125, 'grad_norm': 4.0294125938089564e-05, 'learning_rate': 5.480986749973069e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0, 'grad_norm': 0.01208382286131382, 'learning_rate': 5.475600560163741e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0006, 'grad_norm': 0.000567738781683147, 'learning_rate': 5.470214370354412e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6868, 'grad_norm': 0.0335959866642952, 'learning_rate': 5.464828180545083e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0326, 'grad_norm': 0.00021993897098582238, 'learning_rate': 5.459441990735754e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0013, 'grad_norm': 0.018362481147050858, 'learning_rate': 5.454055800926425e-05, 'epoch': 0.91}\n",
      "{'loss': 0.441, 'grad_norm': 84.96780395507812, 'learning_rate': 5.4486696111170964e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0003, 'grad_norm': 0.0009778864914551377, 'learning_rate': 5.4432834213077675e-05, 'epoch': 0.91}\n",
      "{'loss': 0.1752, 'grad_norm': 79.009033203125, 'learning_rate': 5.437897231498438e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0, 'grad_norm': 8.896857616491616e-05, 'learning_rate': 5.432511041689109e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0, 'grad_norm': 6.496626883745193e-05, 'learning_rate': 5.42712485187978e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0006216389592736959, 'learning_rate': 5.421738662070451e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00017845217371359468, 'learning_rate': 5.4163524722611224e-05, 'epoch': 0.92}\n",
      "{'loss': 0.1832, 'grad_norm': 81.69802856445312, 'learning_rate': 5.4109662824517935e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0727, 'grad_norm': 8.717275619506836, 'learning_rate': 5.4055800926424646e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004710982320830226, 'learning_rate': 5.4001939028331364e-05, 'epoch': 0.92}\n",
      "{'loss': 0.268, 'grad_norm': 1.3007195320824394e-06, 'learning_rate': 5.3948077130238075e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0, 'grad_norm': 0.00047405928489752114, 'learning_rate': 5.3894215232144786e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0734, 'grad_norm': 8.744045771891251e-05, 'learning_rate': 5.38403533340515e-05, 'epoch': 0.92}\n",
      "{'loss': 0.2807, 'grad_norm': 9.293788934883196e-06, 'learning_rate': 5.378649143595821e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0, 'grad_norm': 9.892319940263405e-05, 'learning_rate': 5.373262953786492e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0429, 'grad_norm': 0.00031448621302843094, 'learning_rate': 5.367876763977163e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0049, 'grad_norm': 4.754653855343349e-05, 'learning_rate': 5.362490574167834e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0001, 'grad_norm': 7.387187361018732e-05, 'learning_rate': 5.357104384358506e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0001, 'grad_norm': 0.006321036256849766, 'learning_rate': 5.351718194549176e-05, 'epoch': 0.93}\n",
      "{'loss': 0.3835, 'grad_norm': 0.00021843270224053413, 'learning_rate': 5.346332004739847e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0005, 'grad_norm': 0.00010912493598880246, 'learning_rate': 5.340945814930518e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0, 'grad_norm': 2.246678741357755e-05, 'learning_rate': 5.335559625121189e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0002, 'grad_norm': 1.0352284363079889e-07, 'learning_rate': 5.33017343531186e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0368, 'grad_norm': 0.004053741693496704, 'learning_rate': 5.324787245502532e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0, 'grad_norm': 4.074553544342052e-06, 'learning_rate': 5.319401055693203e-05, 'epoch': 0.94}\n",
      "{'loss': 0.5998, 'grad_norm': 3.529804234858602e-05, 'learning_rate': 5.314014865883874e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0051, 'grad_norm': 2.0000345130055663e-11, 'learning_rate': 5.308628676074545e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0, 'grad_norm': 0.004111912101507187, 'learning_rate': 5.303242486265216e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0512, 'grad_norm': 132.6732940673828, 'learning_rate': 5.2978562964558874e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0457, 'grad_norm': 0.0005037255468778312, 'learning_rate': 5.2924701066465585e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0112, 'grad_norm': 0.00926861260086298, 'learning_rate': 5.2870839168372297e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0022, 'grad_norm': 0.014296374283730984, 'learning_rate': 5.2816977270279014e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0379, 'grad_norm': 0.0005696019507013261, 'learning_rate': 5.2763115372185726e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0, 'grad_norm': 7.304187874979107e-06, 'learning_rate': 5.2709253474092437e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3356, 'grad_norm': 2.564379656178062e-06, 'learning_rate': 5.2655391575999134e-05, 'epoch': 0.95}\n",
      "{'loss': 0.0149, 'grad_norm': 6.564774057693512e-09, 'learning_rate': 5.2601529677905845e-05, 'epoch': 0.95}\n",
      "{'loss': 0.0, 'grad_norm': 3.157395258313045e-05, 'learning_rate': 5.2547667779812556e-05, 'epoch': 0.95}\n",
      "{'loss': 0.001, 'grad_norm': 1.7158930631921976e-07, 'learning_rate': 5.2493805881719274e-05, 'epoch': 0.95}\n",
      "{'loss': 0.0187, 'grad_norm': 7.328540232265368e-05, 'learning_rate': 5.2439943983625985e-05, 'epoch': 0.95}\n",
      "{'loss': 0.0008, 'grad_norm': 0.0009019867284223437, 'learning_rate': 5.2386082085532696e-05, 'epoch': 0.95}\n",
      "{'loss': 0.0, 'grad_norm': 0.0705021470785141, 'learning_rate': 5.233222018743941e-05, 'epoch': 0.95}\n",
      "{'loss': 0.5892, 'grad_norm': 7.945658921926224e-07, 'learning_rate': 5.227835828934612e-05, 'epoch': 0.95}\n",
      "{'loss': 0.028, 'grad_norm': 1.8579329434942338e-06, 'learning_rate': 5.222449639125283e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0, 'grad_norm': 3.1528350518783554e-05, 'learning_rate': 5.217063449315954e-05, 'epoch': 0.96}\n",
      "{'loss': 0.4081, 'grad_norm': 0.00016662054986227304, 'learning_rate': 5.211677259506625e-05, 'epoch': 0.96}\n",
      "{'loss': 0.3186, 'grad_norm': 1.3951255368738202e-06, 'learning_rate': 5.206291069697297e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0, 'grad_norm': 0.000304448971292004, 'learning_rate': 5.200904879887968e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0, 'grad_norm': 0.00023471711028832942, 'learning_rate': 5.195518690078639e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0831, 'grad_norm': 1.6991589291137643e-05, 'learning_rate': 5.19013250026931e-05, 'epoch': 0.96}\n",
      "{'loss': 0.4521, 'grad_norm': 1.5745192285976373e-05, 'learning_rate': 5.1847463104599814e-05, 'epoch': 0.96}\n",
      "{'loss': 0.2807, 'grad_norm': 83.3349380493164, 'learning_rate': 5.179360120650651e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001568726438563317, 'learning_rate': 5.173973930841323e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0004, 'grad_norm': 3.2657528663548874e-06, 'learning_rate': 5.168587741031994e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00029362906934693456, 'learning_rate': 5.163201551222665e-05, 'epoch': 0.97}\n",
      "{'loss': 0.067, 'grad_norm': 1.8055316963749846e-10, 'learning_rate': 5.157815361413336e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011275731230853125, 'learning_rate': 5.1524291716040074e-05, 'epoch': 0.97}\n",
      "{'loss': 0.4112, 'grad_norm': 1.954624713107478e-05, 'learning_rate': 5.1470429817946785e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0, 'grad_norm': 0.0029086077120155096, 'learning_rate': 5.1416567919853496e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0, 'grad_norm': 0.003320788498967886, 'learning_rate': 5.136270602176021e-05, 'epoch': 0.97}\n",
      "{'loss': 0.6448, 'grad_norm': 9.486136150371749e-06, 'learning_rate': 5.130884412366692e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0002, 'grad_norm': 0.015980368480086327, 'learning_rate': 5.1254982225573636e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0212, 'grad_norm': 25.366775512695312, 'learning_rate': 5.120112032748035e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0042, 'grad_norm': 0.00020019551448058337, 'learning_rate': 5.114725842938706e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0002, 'grad_norm': 0.3771275579929352, 'learning_rate': 5.109339653129377e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0, 'grad_norm': 1.230048241041004e-07, 'learning_rate': 5.103953463320048e-05, 'epoch': 0.98}\n",
      "{'loss': 0.5524, 'grad_norm': 2.676593794603832e-06, 'learning_rate': 5.098567273510719e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0, 'grad_norm': 5.8476281992625445e-05, 'learning_rate': 5.0931810837013896e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0018, 'grad_norm': 4.743138197227381e-05, 'learning_rate': 5.087794893892061e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0001, 'grad_norm': 2.0649866200983524e-05, 'learning_rate': 5.082408704082732e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0021, 'grad_norm': 0.2217833697795868, 'learning_rate': 5.077022514273403e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0, 'grad_norm': 6.522499461425468e-05, 'learning_rate': 5.071636324464074e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0, 'grad_norm': 0.016635211184620857, 'learning_rate': 5.066250134654745e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0, 'grad_norm': 1.7819775166572072e-06, 'learning_rate': 5.060863944845416e-05, 'epoch': 0.99}\n",
      "{'loss': 0.1621, 'grad_norm': 97.19945526123047, 'learning_rate': 5.0554777550360873e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0159, 'grad_norm': 4.604368314176099e-06, 'learning_rate': 5.050091565226759e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0419, 'grad_norm': 2.50962639558594e-10, 'learning_rate': 5.04470537541743e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0, 'grad_norm': 3.895568079315126e-05, 'learning_rate': 5.0393191856081013e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0013884622603654861, 'learning_rate': 5.0339329957987725e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0, 'grad_norm': 2.7260057322564535e-05, 'learning_rate': 5.0285468059894436e-05, 'epoch': 0.99}\n",
      "{'loss': 0.1014, 'grad_norm': 6.832656072219834e-05, 'learning_rate': 5.023160616180115e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00064709666185081, 'learning_rate': 5.017774426370786e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0004982012324035168, 'learning_rate': 5.012388236561457e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0003, 'grad_norm': 5.020565936320054e-07, 'learning_rate': 5.007002046752127e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0, 'grad_norm': 2.1474924949416163e-08, 'learning_rate': 5.0016158569427984e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c6173b4129445d983e09383c698b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06478647142648697, 'eval_accuracy': 0.9896369708068512, 'eval_runtime': 3197.0799, 'eval_samples_per_second': 14.518, 'eval_steps_per_second': 7.259, 'epoch': 1.0}\n",
      "{'loss': 0.7928, 'grad_norm': 0.0004550080338958651, 'learning_rate': 4.99622966713347e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0755, 'grad_norm': 1.7167056398648128e-07, 'learning_rate': 4.990843477324141e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0, 'grad_norm': 1.0739310027929605e-06, 'learning_rate': 4.985457287514812e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0, 'grad_norm': 1.7877606296679005e-05, 'learning_rate': 4.980071097705483e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0, 'grad_norm': 2.887624532377231e-06, 'learning_rate': 4.9746849078961547e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0, 'grad_norm': 1.5866982721490785e-05, 'learning_rate': 4.969298718086826e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0004, 'grad_norm': 7.744497487749413e-09, 'learning_rate': 4.963912528277497e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0, 'grad_norm': 4.528665158431977e-06, 'learning_rate': 4.958526338468168e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0011, 'grad_norm': 2.391437419646536e-06, 'learning_rate': 4.953140148658839e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001812338741729036, 'learning_rate': 4.94775395884951e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0, 'grad_norm': 0.00034570496063679457, 'learning_rate': 4.9423677690401806e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0001, 'grad_norm': 0.07775672525167465, 'learning_rate': 4.9369815792308524e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0, 'grad_norm': 0.00024844007566571236, 'learning_rate': 4.9315953894215235e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003684905532281846, 'learning_rate': 4.9262091996121946e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0001, 'grad_norm': 2.0077343378943624e-06, 'learning_rate': 4.920823009802866e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0, 'grad_norm': 4.761723104707016e-08, 'learning_rate': 4.915436819993537e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0, 'grad_norm': 0.002524326089769602, 'learning_rate': 4.910050630184208e-05, 'epoch': 1.02}\n",
      "{'loss': 0.1578, 'grad_norm': 0.00017324613872915506, 'learning_rate': 4.904664440374879e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0, 'grad_norm': 1.9593492197600426e-06, 'learning_rate': 4.89927825056555e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0, 'grad_norm': 7.141061786342107e-08, 'learning_rate': 4.893892060756221e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0001, 'grad_norm': 8.119258382066619e-06, 'learning_rate': 4.8885058709468924e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0, 'grad_norm': 0.0017269065137952566, 'learning_rate': 4.8831196811375635e-05, 'epoch': 1.02}\n",
      "{'loss': 0.1091, 'grad_norm': 2.4594200009886436e-09, 'learning_rate': 4.8777334913282346e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0001, 'grad_norm': 3.451251302521996e-07, 'learning_rate': 4.872347301518906e-05, 'epoch': 1.03}\n",
      "{'loss': 0.1972, 'grad_norm': 1.3545923138735816e-06, 'learning_rate': 4.866961111709577e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0, 'grad_norm': 9.237050107913092e-06, 'learning_rate': 4.861574921900248e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0, 'grad_norm': 0.0007183666457422078, 'learning_rate': 4.856188732090919e-05, 'epoch': 1.03}\n",
      "{'loss': 0.559, 'grad_norm': 7.58071649897829e-08, 'learning_rate': 4.85080254228159e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0247, 'grad_norm': 0.0009935799753293395, 'learning_rate': 4.845416352472261e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0, 'grad_norm': 1.9573126337490976e-05, 'learning_rate': 4.8400301626629324e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0, 'grad_norm': 8.192518180294428e-06, 'learning_rate': 4.8346439728536035e-05, 'epoch': 1.03}\n",
      "{'loss': 0.139, 'grad_norm': 0.0011489278404042125, 'learning_rate': 4.8292577830442746e-05, 'epoch': 1.03}\n",
      "{'loss': 0.2803, 'grad_norm': 5.370985309127718e-05, 'learning_rate': 4.823871593234946e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0002, 'grad_norm': 7.197891682153568e-05, 'learning_rate': 4.8184854034256175e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0, 'grad_norm': 1.9459022951195948e-05, 'learning_rate': 4.813099213616288e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0, 'grad_norm': 1.132068405240716e-06, 'learning_rate': 4.807713023806959e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0002, 'grad_norm': 7.001969788689166e-05, 'learning_rate': 4.80232683399763e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0, 'grad_norm': 1.949861689354293e-05, 'learning_rate': 4.796940644188301e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0, 'grad_norm': 1.0367701179347932e-05, 'learning_rate': 4.7915544543789724e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0, 'grad_norm': 5.304007686390833e-07, 'learning_rate': 4.7861682645696435e-05, 'epoch': 1.04}\n",
      "{'loss': 0.4078, 'grad_norm': 4.96527245559264e-05, 'learning_rate': 4.780782074760315e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0063, 'grad_norm': 0.00015647475083824247, 'learning_rate': 4.7753958849509864e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0024, 'grad_norm': 0.0005440826062113047, 'learning_rate': 4.7700096951416575e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0029, 'grad_norm': 0.0002445652207825333, 'learning_rate': 4.764623505332328e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0029, 'grad_norm': 0.004349670838564634, 'learning_rate': 4.759237315522999e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0027, 'grad_norm': 1.7943657439900562e-05, 'learning_rate': 4.75385112571367e-05, 'epoch': 1.05}\n",
      "{'loss': 0.738, 'grad_norm': 0.00028730739722959697, 'learning_rate': 4.748464935904341e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0, 'grad_norm': 7.140267371141817e-06, 'learning_rate': 4.743078746095013e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0002961586869787425, 'learning_rate': 4.737692556285684e-05, 'epoch': 1.05}\n",
      "{'loss': 0.4027, 'grad_norm': 2.144461359421257e-05, 'learning_rate': 4.732306366476355e-05, 'epoch': 1.05}\n",
      "{'loss': 0.1818, 'grad_norm': 1.2298417573219922e-07, 'learning_rate': 4.7269201766670263e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0, 'grad_norm': 6.611275011891848e-07, 'learning_rate': 4.721533986857697e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0, 'grad_norm': 0.004480457864701748, 'learning_rate': 4.716147797048368e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0, 'grad_norm': 0.005385443568229675, 'learning_rate': 4.710761607239039e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0001, 'grad_norm': 7.851266673242208e-06, 'learning_rate': 4.70537541742971e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0015, 'grad_norm': 7.38166028302345e-10, 'learning_rate': 4.699989227620382e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0, 'grad_norm': 5.468388189910911e-05, 'learning_rate': 4.694603037811053e-05, 'epoch': 1.06}\n",
      "{'loss': 0.2179, 'grad_norm': 0.024371346458792686, 'learning_rate': 4.689216848001724e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0, 'grad_norm': 1.2087263030480244e-06, 'learning_rate': 4.683830658192395e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0185, 'grad_norm': 6.060215673642233e-05, 'learning_rate': 4.6784444683830656e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0002239764726255089, 'learning_rate': 4.673058278573737e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0005, 'grad_norm': 9.8853715826408e-06, 'learning_rate': 4.667672088764408e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0027, 'grad_norm': 2.881074124161387e-06, 'learning_rate': 4.6622858989550797e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0039, 'grad_norm': 0.21483905613422394, 'learning_rate': 4.656899709145751e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0, 'grad_norm': 0.019405962899327278, 'learning_rate': 4.651513519336422e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0001, 'grad_norm': 3.4916338336188346e-05, 'learning_rate': 4.646127329527093e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0001, 'grad_norm': 0.043914951384067535, 'learning_rate': 4.640741139717764e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011607031774474308, 'learning_rate': 4.6353549499084345e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0003, 'grad_norm': 0.4264186918735504, 'learning_rate': 4.6299687600991056e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0001, 'grad_norm': 9.999233043345157e-06, 'learning_rate': 4.6245825702897774e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0, 'grad_norm': 0.0023284670896828175, 'learning_rate': 4.6191963804804485e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0, 'grad_norm': 1.1066826566263899e-08, 'learning_rate': 4.6138101906711196e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0, 'grad_norm': 7.874859875300899e-09, 'learning_rate': 4.608424000861791e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0, 'grad_norm': 0.00025278623797930777, 'learning_rate': 4.603037811052462e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0001, 'grad_norm': 1.1542767424543854e-05, 'learning_rate': 4.597651621243133e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0001, 'grad_norm': 1.5590447901558946e-06, 'learning_rate': 4.5922654314338034e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0027, 'grad_norm': 2.890079667849932e-05, 'learning_rate': 4.586879241624475e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0001, 'grad_norm': 5.414979682427656e-07, 'learning_rate': 4.581493051815146e-05, 'epoch': 1.08}\n",
      "{'loss': 0.2163, 'grad_norm': 83.95650482177734, 'learning_rate': 4.5761068620058174e-05, 'epoch': 1.08}\n",
      "{'loss': 0.001, 'grad_norm': 0.00011255981371505186, 'learning_rate': 4.5707206721964885e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0277, 'grad_norm': 1.770209678397805e-06, 'learning_rate': 4.5653344823871596e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0018, 'grad_norm': 0.07147902250289917, 'learning_rate': 4.559948292577831e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0, 'grad_norm': 0.00025981187354773283, 'learning_rate': 4.554562102768502e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0, 'grad_norm': 2.8681629373750184e-06, 'learning_rate': 4.549175912959173e-05, 'epoch': 1.09}\n",
      "{'loss': 0.1118, 'grad_norm': 2.126214440067997e-06, 'learning_rate': 4.543789723149844e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0, 'grad_norm': 8.09561413461779e-07, 'learning_rate': 4.538403533340515e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0, 'grad_norm': 1.8947130229207687e-05, 'learning_rate': 4.533017343531186e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0026, 'grad_norm': 0.11704310029745102, 'learning_rate': 4.5276311537218574e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0015, 'grad_norm': 0.0003147036477457732, 'learning_rate': 4.5222449639125285e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00015904662723187357, 'learning_rate': 4.5168587741031996e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0006, 'grad_norm': 2.4052969820331782e-05, 'learning_rate': 4.511472584293871e-05, 'epoch': 1.1}\n",
      "{'loss': 0.1498, 'grad_norm': 1.9045158978769905e-06, 'learning_rate': 4.506086394484542e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0, 'grad_norm': 7.3112701102218125e-06, 'learning_rate': 4.500700204675213e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0, 'grad_norm': 2.7902220267606026e-07, 'learning_rate': 4.495314014865884e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0, 'grad_norm': 0.00014266847574617714, 'learning_rate': 4.489927825056555e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0023, 'grad_norm': 3.679982546600513e-05, 'learning_rate': 4.484541635247226e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0, 'grad_norm': 4.9456237320555374e-05, 'learning_rate': 4.4791554454378974e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0012, 'grad_norm': 1.5949007092785905e-07, 'learning_rate': 4.4737692556285685e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0002, 'grad_norm': 3.2897062851589e-07, 'learning_rate': 4.46838306581924e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0, 'grad_norm': 1.8076478227158077e-05, 'learning_rate': 4.462996876009911e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0, 'grad_norm': 7.354517947533168e-06, 'learning_rate': 4.457610686200582e-05, 'epoch': 1.11}\n",
      "{'loss': 0.1566, 'grad_norm': 0.18833856284618378, 'learning_rate': 4.452224496391253e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0, 'grad_norm': 0.00110440026037395, 'learning_rate': 4.446838306581924e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0, 'grad_norm': 1.9474954626730323e-07, 'learning_rate': 4.441452116772595e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0, 'grad_norm': 0.0016263234429061413, 'learning_rate': 4.436065926963266e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0, 'grad_norm': 0.007820263504981995, 'learning_rate': 4.430679737153938e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0, 'grad_norm': 6.998388562351465e-05, 'learning_rate': 4.425293547344609e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0007, 'grad_norm': 5.092785431770608e-05, 'learning_rate': 4.4199073575352796e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0, 'grad_norm': 0.01836712472140789, 'learning_rate': 4.414521167725951e-05, 'epoch': 1.12}\n",
      "{'loss': 0.106, 'grad_norm': 1.8023116581389331e-06, 'learning_rate': 4.409134977916622e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0, 'grad_norm': 0.0014451075112447143, 'learning_rate': 4.403748788107293e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0294, 'grad_norm': 0.00017030166054610163, 'learning_rate': 4.398362598297964e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0, 'grad_norm': 1.7063892300939187e-05, 'learning_rate': 4.392976408488636e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0033, 'grad_norm': 9.04977150639752e-06, 'learning_rate': 4.387590218679307e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0084, 'grad_norm': 0.0009059662115760148, 'learning_rate': 4.382204028869978e-05, 'epoch': 1.12}\n",
      "{'loss': 0.5137, 'grad_norm': 2.911639739977545e-06, 'learning_rate': 4.3768178390606484e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0, 'grad_norm': 0.00019709205662366003, 'learning_rate': 4.3714316492513195e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010342151654185727, 'learning_rate': 4.3660454594419906e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0, 'grad_norm': 0.002679479541257024, 'learning_rate': 4.360659269632662e-05, 'epoch': 1.13}\n",
      "{'loss': 0.6398, 'grad_norm': 3.755499847102328e-06, 'learning_rate': 4.3552730798233335e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0004049601557198912, 'learning_rate': 4.3498868900140047e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0, 'grad_norm': 9.368909559270833e-06, 'learning_rate': 4.344500700204676e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0023, 'grad_norm': 7.259713172912598, 'learning_rate': 4.339114510395347e-05, 'epoch': 1.13}\n",
      "{'loss': 0.2213, 'grad_norm': 5.572305235546082e-05, 'learning_rate': 4.333728320586017e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0, 'grad_norm': 2.7904137823497877e-05, 'learning_rate': 4.3283421307766884e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00016505613166373223, 'learning_rate': 4.3229559409673595e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0, 'grad_norm': 0.0007897749310359359, 'learning_rate': 4.317569751158031e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0, 'grad_norm': 2.6073550429828174e-07, 'learning_rate': 4.3121835613487024e-05, 'epoch': 1.14}\n",
      "{'loss': 0.2597, 'grad_norm': 5.571587462327443e-05, 'learning_rate': 4.3067973715393735e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0001, 'grad_norm': 6.648321868851781e-05, 'learning_rate': 4.3014111817300446e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0001, 'grad_norm': 0.009968925267457962, 'learning_rate': 4.296024991920716e-05, 'epoch': 1.14}\n",
      "{'loss': 0.5361, 'grad_norm': 120.27906036376953, 'learning_rate': 4.290638802111386e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0, 'grad_norm': 0.00024307245621457696, 'learning_rate': 4.285252612302057e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0005, 'grad_norm': 3.8984646380413324e-05, 'learning_rate': 4.279866422492729e-05, 'epoch': 1.14}\n",
      "{'loss': 0.2357, 'grad_norm': 1.730911753838882e-05, 'learning_rate': 4.2744802326834e-05, 'epoch': 1.15}\n",
      "{'loss': 0.1641, 'grad_norm': 72.67227935791016, 'learning_rate': 4.269094042874071e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0, 'grad_norm': 0.001011303742416203, 'learning_rate': 4.2637078530647424e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0001, 'grad_norm': 0.002441562246531248, 'learning_rate': 4.2583216632554135e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0012910434743389487, 'learning_rate': 4.2529354734460846e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0126, 'grad_norm': 0.03901105746626854, 'learning_rate': 4.247549283636755e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0002, 'grad_norm': 0.04883742332458496, 'learning_rate': 4.242163093827426e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0015382175333797932, 'learning_rate': 4.236776904018098e-05, 'epoch': 1.15}\n",
      "{'loss': 0.001, 'grad_norm': 2.279680529682082e-06, 'learning_rate': 4.231390714208769e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0, 'grad_norm': 1.1044778148061596e-05, 'learning_rate': 4.22600452439944e-05, 'epoch': 1.15}\n",
      "{'loss': 0.1745, 'grad_norm': 6.598620529985055e-07, 'learning_rate': 4.220618334590111e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0, 'grad_norm': 2.8011747417622246e-05, 'learning_rate': 4.2152321447807824e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0, 'grad_norm': 2.042318374151364e-05, 'learning_rate': 4.2098459549714535e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013162977120373398, 'learning_rate': 4.204459765162124e-05, 'epoch': 1.16}\n",
      "{'loss': 0.2691, 'grad_norm': 1.921985858643893e-05, 'learning_rate': 4.199073575352796e-05, 'epoch': 1.16}\n",
      "{'loss': 0.4238, 'grad_norm': 5.3180992836132646e-05, 'learning_rate': 4.193687385543467e-05, 'epoch': 1.16}\n",
      "{'loss': 0.002, 'grad_norm': 1.903200609376654e-05, 'learning_rate': 4.188301195734138e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0007, 'grad_norm': 0.00014327181270346045, 'learning_rate': 4.182915005924809e-05, 'epoch': 1.16}\n",
      "{'loss': 0.2417, 'grad_norm': 0.0027068057097494602, 'learning_rate': 4.17752881611548e-05, 'epoch': 1.16}\n",
      "{'loss': 0.3706, 'grad_norm': 8.452340262010694e-05, 'learning_rate': 4.172142626306151e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0, 'grad_norm': 1.0228957762592472e-05, 'learning_rate': 4.1667564364968224e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0, 'grad_norm': 3.490865196908999e-07, 'learning_rate': 4.1613702466874935e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0, 'grad_norm': 1.1853759929181251e-07, 'learning_rate': 4.1559840568781646e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0, 'grad_norm': 1.5111711491044844e-06, 'learning_rate': 4.150597867068836e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0, 'grad_norm': 1.8842185454559512e-05, 'learning_rate': 4.145211677259507e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0, 'grad_norm': 1.7697918508474686e-07, 'learning_rate': 4.139825487450178e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0003, 'grad_norm': 2.0572169887600467e-05, 'learning_rate': 4.134439297640849e-05, 'epoch': 1.17}\n",
      "{'loss': 0.1456, 'grad_norm': 0.00010613944323267788, 'learning_rate': 4.12905310783152e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0001, 'grad_norm': 1.1472842231796676e-08, 'learning_rate': 4.123666918022191e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0007936402107588947, 'learning_rate': 4.1182807282128623e-05, 'epoch': 1.18}\n",
      "{'loss': 0.1933, 'grad_norm': 87.88262939453125, 'learning_rate': 4.1128945384035334e-05, 'epoch': 1.18}\n",
      "{'loss': 0.3876, 'grad_norm': 0.01242329552769661, 'learning_rate': 4.1075083485942046e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0, 'grad_norm': 0.000606129236984998, 'learning_rate': 4.102122158784876e-05, 'epoch': 1.18}\n",
      "{'loss': 0.1743, 'grad_norm': 1.6224350929405773e-07, 'learning_rate': 4.096735968975547e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0006, 'grad_norm': 0.00011526797607075423, 'learning_rate': 4.091349779166218e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0003, 'grad_norm': 8.723975042812526e-06, 'learning_rate': 4.085963589356889e-05, 'epoch': 1.18}\n",
      "{'loss': 0.1843, 'grad_norm': 0.07131744921207428, 'learning_rate': 4.080577399547561e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0004, 'grad_norm': 0.19054384529590607, 'learning_rate': 4.075191209738231e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0, 'grad_norm': 9.688523277873173e-06, 'learning_rate': 4.069805019928902e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006661470397375524, 'learning_rate': 4.0644188301195734e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0001, 'grad_norm': 3.4349116617704567e-07, 'learning_rate': 4.0590326403102445e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0, 'grad_norm': 3.187714128216612e-06, 'learning_rate': 4.0536464505009156e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0003, 'grad_norm': 0.5383205413818359, 'learning_rate': 4.048260260691587e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0, 'grad_norm': 5.67738368317805e-07, 'learning_rate': 4.0428740708822585e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0, 'grad_norm': 1.0164079867536202e-05, 'learning_rate': 4.0374878810729297e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0011, 'grad_norm': 1.8684296607971191, 'learning_rate': 4.0321016912636e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0, 'grad_norm': 3.1502993806498125e-05, 'learning_rate': 4.026715501454271e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0, 'grad_norm': 0.00025814917171373963, 'learning_rate': 4.021329311644942e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0052847531624138355, 'learning_rate': 4.0159431218356134e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0, 'grad_norm': 9.405281389263109e-07, 'learning_rate': 4.0105569320262845e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0, 'grad_norm': 3.885111436829902e-05, 'learning_rate': 4.005170742216956e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0, 'grad_norm': 3.136599957542785e-07, 'learning_rate': 3.9997845524076274e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0, 'grad_norm': 2.639548711158568e-06, 'learning_rate': 3.9943983625982985e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0, 'grad_norm': 3.25205000706319e-08, 'learning_rate': 3.9890121727889696e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0, 'grad_norm': 3.6661111835201154e-07, 'learning_rate': 3.98362598297964e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2703, 'grad_norm': 1.7236188796232454e-05, 'learning_rate': 3.978239793170311e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0, 'grad_norm': 1.00159786597942e-06, 'learning_rate': 3.972853603360982e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0031, 'grad_norm': 5.812421798706055, 'learning_rate': 3.967467413551654e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0315, 'grad_norm': 44.18695831298828, 'learning_rate': 3.962081223742325e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0001, 'grad_norm': 9.381695417687297e-05, 'learning_rate': 3.956695033932996e-05, 'epoch': 1.21}\n",
      "{'loss': 0.502, 'grad_norm': 0.0025263489224016666, 'learning_rate': 3.9513088441236674e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0, 'grad_norm': 8.411555896259415e-10, 'learning_rate': 3.9459226543143385e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0006, 'grad_norm': 9.820270861382596e-06, 'learning_rate': 3.940536464505009e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0914, 'grad_norm': 1.5786333084106445, 'learning_rate': 3.93515027469568e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0005, 'grad_norm': 0.057891469448804855, 'learning_rate': 3.929764084886352e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0, 'grad_norm': 0.001014964422211051, 'learning_rate': 3.924377895077023e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.00022576356423087418, 'learning_rate': 3.918991705267694e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 2.7493513243825873e-06, 'learning_rate': 3.913605515458365e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.00015571263793390244, 'learning_rate': 3.908219325649036e-05, 'epoch': 1.22}\n",
      "{'loss': 0.2509, 'grad_norm': 0.0012740545207634568, 'learning_rate': 3.9028331358397074e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 1.999772621275042e-06, 'learning_rate': 3.897446946030378e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002019032690441236, 'learning_rate': 3.8920607562210496e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 1.3375546359384316e-07, 'learning_rate': 3.886674566411721e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 2.978705879286281e-06, 'learning_rate': 3.881288376602392e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.00837669800966978, 'learning_rate': 3.875902186793063e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.00017790892161428928, 'learning_rate': 3.870515996983734e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0866, 'grad_norm': 7.915799869806506e-06, 'learning_rate': 3.865129807174405e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0202, 'grad_norm': 3.7490415252250386e-06, 'learning_rate': 3.859743617365076e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1964, 'grad_norm': 1.062773776538961e-06, 'learning_rate': 3.8543574275557474e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004362821055110544, 'learning_rate': 3.8489712377464185e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0, 'grad_norm': 0.011172257363796234, 'learning_rate': 3.8435850479370896e-05, 'epoch': 1.23}\n",
      "{'loss': 0.7861, 'grad_norm': 4.7148023440968245e-05, 'learning_rate': 3.838198858127761e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1172, 'grad_norm': 3.7517715099966154e-05, 'learning_rate': 3.832812668318432e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0002, 'grad_norm': 0.04445947706699371, 'learning_rate': 3.827426478509103e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0002, 'grad_norm': 4.8593563406029716e-05, 'learning_rate': 3.822040288699774e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0054, 'grad_norm': 0.0030551098752766848, 'learning_rate': 3.816654098890445e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0, 'grad_norm': 8.2109878007941e-08, 'learning_rate': 3.811267909081116e-05, 'epoch': 1.24}\n",
      "{'loss': 0.1924, 'grad_norm': 0.0006423163576982915, 'learning_rate': 3.8058817192717873e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0, 'grad_norm': 0.000375914853066206, 'learning_rate': 3.8004955294624584e-05, 'epoch': 1.24}\n",
      "{'loss': 0.003, 'grad_norm': 2.8569609185069567e-06, 'learning_rate': 3.7951093396531296e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0378, 'grad_norm': 8.32683508633636e-05, 'learning_rate': 3.789723149843801e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0, 'grad_norm': 7.917576112959068e-06, 'learning_rate': 3.784336960034472e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0, 'grad_norm': 6.025513954455164e-08, 'learning_rate': 3.778950770225143e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0002, 'grad_norm': 1.4270165778207389e-11, 'learning_rate': 3.773564580415814e-05, 'epoch': 1.25}\n",
      "{'loss': 0.068, 'grad_norm': 1.4663172578366357e-06, 'learning_rate': 3.768178390606485e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0263, 'grad_norm': 1.172177377384287e-07, 'learning_rate': 3.762792200797156e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0006, 'grad_norm': 7.172368350438774e-05, 'learning_rate': 3.757406010987827e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0, 'grad_norm': 1.0853678134026268e-07, 'learning_rate': 3.7520198211784984e-05, 'epoch': 1.25}\n",
      "{'loss': 0.4434, 'grad_norm': 3.742611198731538e-08, 'learning_rate': 3.7466336313691695e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0, 'grad_norm': 1.6889845255718683e-06, 'learning_rate': 3.7412474415598406e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0211, 'grad_norm': 1.7886195564642549e-06, 'learning_rate': 3.735861251750512e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0, 'grad_norm': 1.2134036069255671e-06, 'learning_rate': 3.7304750619411835e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0, 'grad_norm': 2.303402197867399e-07, 'learning_rate': 3.725088872131854e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0, 'grad_norm': 3.214633181869431e-08, 'learning_rate': 3.719702682322525e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0001, 'grad_norm': 9.989950200406383e-08, 'learning_rate': 3.714316492513196e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0, 'grad_norm': 3.088196535827592e-05, 'learning_rate': 3.708930302703867e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0, 'grad_norm': 4.947631748741799e-10, 'learning_rate': 3.7035441128945384e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0607, 'grad_norm': 3.306377948320005e-06, 'learning_rate': 3.6981579230852095e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0081, 'grad_norm': 9.255346412828658e-06, 'learning_rate': 3.692771733275881e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0, 'grad_norm': 0.00016399731975980103, 'learning_rate': 3.6873855434665524e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0, 'grad_norm': 8.950981282396242e-06, 'learning_rate': 3.681999353657223e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0, 'grad_norm': 6.935774621297242e-08, 'learning_rate': 3.676613163847894e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0, 'grad_norm': 1.0646616829035338e-05, 'learning_rate': 3.671226974038565e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0, 'grad_norm': 1.1749062167609736e-07, 'learning_rate': 3.665840784229236e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0, 'grad_norm': 0.004899661056697369, 'learning_rate': 3.660454594419907e-05, 'epoch': 1.27}\n",
      "{'loss': 0.236, 'grad_norm': 7.510020805057138e-05, 'learning_rate': 3.655068404610579e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0022, 'grad_norm': 1.725111701489368e-06, 'learning_rate': 3.64968221480125e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0002, 'grad_norm': 1.0201047189184465e-06, 'learning_rate': 3.644296024991921e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0, 'grad_norm': 7.3546798375900835e-06, 'learning_rate': 3.638909835182592e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0001, 'grad_norm': 0.1430741399526596, 'learning_rate': 3.633523645373263e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0001, 'grad_norm': 1.8382407915851218e-06, 'learning_rate': 3.628137455563934e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0, 'grad_norm': 0.00029063428519293666, 'learning_rate': 3.622751265754605e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001965713017852977, 'learning_rate': 3.617365075945277e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0009, 'grad_norm': 2.8541511710500345e-05, 'learning_rate': 3.611978886135948e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008619239088147879, 'learning_rate': 3.606592696326619e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0, 'grad_norm': 2.5034830741788028e-06, 'learning_rate': 3.60120650651729e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0, 'grad_norm': 1.3373364993185533e-07, 'learning_rate': 3.5958203167079606e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0, 'grad_norm': 2.038254223180047e-07, 'learning_rate': 3.590434126898632e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0, 'grad_norm': 1.219939207430798e-07, 'learning_rate': 3.585047937089303e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0, 'grad_norm': 6.211018330759543e-07, 'learning_rate': 3.5796617472799746e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0, 'grad_norm': 3.6189581464896037e-07, 'learning_rate': 3.574275557470646e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0, 'grad_norm': 4.1800191752372484e-08, 'learning_rate': 3.568889367661317e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0, 'grad_norm': 3.1534941626887303e-06, 'learning_rate': 3.563503177851988e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0206, 'grad_norm': 4.6307068259920925e-05, 'learning_rate': 3.558116988042659e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0, 'grad_norm': 1.7687106037556077e-06, 'learning_rate': 3.5527307982333295e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0002, 'grad_norm': 1.7297453496212256e-06, 'learning_rate': 3.5473446084240006e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0235, 'grad_norm': 2.3681100174144376e-06, 'learning_rate': 3.5419584186146724e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0, 'grad_norm': 9.159178262052592e-06, 'learning_rate': 3.5365722288053435e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0, 'grad_norm': 0.00020178876002319157, 'learning_rate': 3.5311860389960146e-05, 'epoch': 1.29}\n",
      "{'loss': 0.4162, 'grad_norm': 9.030784120511726e-09, 'learning_rate': 3.525799849186686e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0, 'grad_norm': 3.206766052699095e-08, 'learning_rate': 3.520413659377357e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0, 'grad_norm': 0.0014524541329592466, 'learning_rate': 3.515027469568028e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0, 'grad_norm': 3.034961333896885e-13, 'learning_rate': 3.509641279758698e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0, 'grad_norm': 1.9789482053056417e-08, 'learning_rate': 3.50425508994937e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0, 'grad_norm': 4.863235858465487e-07, 'learning_rate': 3.498868900140041e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0, 'grad_norm': 2.0502791358012473e-06, 'learning_rate': 3.493482710330712e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0, 'grad_norm': 1.8693061065278016e-05, 'learning_rate': 3.4880965205213834e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0, 'grad_norm': 5.400375258091117e-08, 'learning_rate': 3.4827103307120546e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0428, 'grad_norm': 2.0347945550724944e-08, 'learning_rate': 3.477324140902726e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0006506895297206938, 'learning_rate': 3.471937951093397e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0, 'grad_norm': 2.474381801675918e-07, 'learning_rate': 3.466551761284068e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0581, 'grad_norm': 1.0924359372666004e-07, 'learning_rate': 3.461165571474739e-05, 'epoch': 1.31}\n",
      "{'loss': 0.2534, 'grad_norm': 1.2608194002727835e-11, 'learning_rate': 3.45577938166541e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0, 'grad_norm': 2.4970034928628593e-07, 'learning_rate': 3.450393191856081e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0, 'grad_norm': 9.617374985282368e-08, 'learning_rate': 3.445007002046752e-05, 'epoch': 1.31}\n",
      "{'loss': 0.1024, 'grad_norm': 0.00012691192387137562, 'learning_rate': 3.4396208122374234e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0033897480461746454, 'learning_rate': 3.4342346224280945e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0007, 'grad_norm': 0.011333495378494263, 'learning_rate': 3.4288484326187656e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0, 'grad_norm': 1.300238182011526e-06, 'learning_rate': 3.423462242809437e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0, 'grad_norm': 1.0358244928454496e-08, 'learning_rate': 3.418076053000108e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0, 'grad_norm': 4.973336444891174e-07, 'learning_rate': 3.412689863190779e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0001, 'grad_norm': 1.1517297025420703e-05, 'learning_rate': 3.40730367338145e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0, 'grad_norm': 9.610062079445925e-06, 'learning_rate': 3.401917483572121e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0752, 'grad_norm': 3.010041282891507e-08, 'learning_rate': 3.396531293762792e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0, 'grad_norm': 1.7904519533651353e-10, 'learning_rate': 3.3911451039534634e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0, 'grad_norm': 9.813425627669403e-09, 'learning_rate': 3.3857589141441345e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0, 'grad_norm': 6.872759428233621e-08, 'learning_rate': 3.3803727243348056e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0001, 'grad_norm': 3.5787911656370852e-06, 'learning_rate': 3.374986534525477e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0, 'grad_norm': 4.1123200844594976e-08, 'learning_rate': 3.369600344716148e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0, 'grad_norm': 0.00046960203326307237, 'learning_rate': 3.364214154906819e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0, 'grad_norm': 2.4099699658108875e-07, 'learning_rate': 3.35882796509749e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0, 'grad_norm': 9.362781128174902e-08, 'learning_rate': 3.353441775288161e-05, 'epoch': 1.33}\n",
      "{'loss': 0.5035, 'grad_norm': 90.3434066772461, 'learning_rate': 3.348055585478832e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0, 'grad_norm': 7.531691517215222e-05, 'learning_rate': 3.342669395669504e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0001, 'grad_norm': 0.01875893585383892, 'learning_rate': 3.3372832058601745e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0, 'grad_norm': 5.172076384951652e-07, 'learning_rate': 3.3318970160508456e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0003, 'grad_norm': 0.0005656167631968856, 'learning_rate': 3.326510826241517e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005659582675434649, 'learning_rate': 3.321124636432188e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0, 'grad_norm': 6.408404829016945e-07, 'learning_rate': 3.315738446622859e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0, 'grad_norm': 1.1535144949448295e-05, 'learning_rate': 3.31035225681353e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0, 'grad_norm': 1.7115789887611754e-05, 'learning_rate': 3.304966067004202e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0, 'grad_norm': 2.481120873198961e-06, 'learning_rate': 3.299579877194873e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0177, 'grad_norm': 21.949377059936523, 'learning_rate': 3.2941936873855434e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0, 'grad_norm': 2.8901296900585294e-05, 'learning_rate': 3.2888074975762145e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0, 'grad_norm': 2.755398327281e-07, 'learning_rate': 3.2834213077668856e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0, 'grad_norm': 4.7518376959487796e-05, 'learning_rate': 3.278035117957557e-05, 'epoch': 1.34}\n",
      "{'loss': 0.1258, 'grad_norm': 3.4484762068132113e-07, 'learning_rate': 3.272648928148228e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0001, 'grad_norm': 4.441283962819398e-08, 'learning_rate': 3.2672627383388996e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0149, 'grad_norm': 8.736657508556789e-10, 'learning_rate': 3.261876548529571e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0, 'grad_norm': 4.0086796104787936e-08, 'learning_rate': 3.256490358720242e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0, 'grad_norm': 1.115566732323714e-07, 'learning_rate': 3.251104168910913e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0, 'grad_norm': 0.028079606592655182, 'learning_rate': 3.2457179791015834e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0, 'grad_norm': 6.464489317112299e-13, 'learning_rate': 3.2403317892922545e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001038349510054104, 'learning_rate': 3.2349455994829256e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0, 'grad_norm': 1.6835982261098614e-11, 'learning_rate': 3.2295594096735974e-05, 'epoch': 1.35}\n",
      "{'loss': 0.1456, 'grad_norm': 5.844187356274233e-08, 'learning_rate': 3.2241732198642685e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0, 'grad_norm': 2.140386641258374e-05, 'learning_rate': 3.2187870300549396e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0, 'grad_norm': 3.3473476435119665e-08, 'learning_rate': 3.213400840245611e-05, 'epoch': 1.36}\n",
      "{'loss': 0.7046, 'grad_norm': 7.511494914069772e-06, 'learning_rate': 3.208014650436282e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0038, 'grad_norm': 1.3286356193020765e-07, 'learning_rate': 3.202628460626952e-05, 'epoch': 1.36}\n",
      "{'loss': 0.006, 'grad_norm': 5.4640782764181495e-05, 'learning_rate': 3.197242270817623e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0004, 'grad_norm': 1.9256840460002422e-06, 'learning_rate': 3.191856081008295e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0004, 'grad_norm': 1.1204585916857468e-06, 'learning_rate': 3.186469891198966e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0023769191466271877, 'learning_rate': 3.181083701389637e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0001, 'grad_norm': 8.055286889430135e-05, 'learning_rate': 3.1756975115803084e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0, 'grad_norm': 7.3059709393419325e-06, 'learning_rate': 3.1703113217709796e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0, 'grad_norm': 3.1041759939398617e-05, 'learning_rate': 3.164925131961651e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0, 'grad_norm': 8.053927558648866e-06, 'learning_rate': 3.159538942152321e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0, 'grad_norm': 2.6879131382884225e-06, 'learning_rate': 3.154152752342993e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0, 'grad_norm': 5.036427523918974e-07, 'learning_rate': 3.148766562533664e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0, 'grad_norm': 0.0030865012668073177, 'learning_rate': 3.143380372724335e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0, 'grad_norm': 2.8110691800975474e-06, 'learning_rate': 3.137994182915006e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1889, 'grad_norm': 2.498026674402354e-07, 'learning_rate': 3.132607993105677e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1868, 'grad_norm': 0.0001354493579128757, 'learning_rate': 3.1272218032963484e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0239, 'grad_norm': 0.7641974091529846, 'learning_rate': 3.1218356134870195e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0, 'grad_norm': 4.047488255309872e-05, 'learning_rate': 3.1164494236776906e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0, 'grad_norm': 7.617504161316901e-05, 'learning_rate': 3.111063233868362e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0, 'grad_norm': 1.1628319043666124e-05, 'learning_rate': 3.105677044059033e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0383, 'grad_norm': 1.7074039533326868e-06, 'learning_rate': 3.100290854249704e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0, 'grad_norm': 1.275413126222702e-07, 'learning_rate': 3.094904664440375e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0003, 'grad_norm': 7.5896959970123135e-06, 'learning_rate': 3.089518474631046e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0, 'grad_norm': 0.005796532146632671, 'learning_rate': 3.084132284821717e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0009, 'grad_norm': 1.7452221356961672e-07, 'learning_rate': 3.0787460950123884e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0576, 'grad_norm': 2.362645387649536, 'learning_rate': 3.0733599052030595e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0001235851232195273, 'learning_rate': 3.0679737153937306e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0002, 'grad_norm': 2.5522435862512793e-06, 'learning_rate': 3.062587525584402e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0105, 'grad_norm': 0.0012815914815291762, 'learning_rate': 3.057201335775073e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0006, 'grad_norm': 4.399514182296116e-06, 'learning_rate': 3.051815145965744e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0, 'grad_norm': 1.3083157890392272e-09, 'learning_rate': 3.046428956156415e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0, 'grad_norm': 4.504272510530427e-08, 'learning_rate': 3.0410427663470865e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0, 'grad_norm': 1.2393952601996716e-06, 'learning_rate': 3.0356565765377576e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0, 'grad_norm': 3.4879099608531305e-11, 'learning_rate': 3.0302703867284284e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0, 'grad_norm': 2.8198048312333412e-05, 'learning_rate': 3.0248841969190995e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'grad_norm': 2.6542561926135022e-08, 'learning_rate': 3.0194980071097706e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'grad_norm': 2.0297888113418594e-05, 'learning_rate': 3.0141118173004417e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'grad_norm': 1.936602345153915e-08, 'learning_rate': 3.0087256274911128e-05, 'epoch': 1.4}\n",
      "{'loss': 0.3099, 'grad_norm': 4.1633235525750933e-08, 'learning_rate': 3.0033394376817843e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0009, 'grad_norm': 2.0631089210510254, 'learning_rate': 2.9979532478724554e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0003, 'grad_norm': 1.5008180245454361e-12, 'learning_rate': 2.9925670580631265e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'grad_norm': 3.1154440138791983e-10, 'learning_rate': 2.9871808682537973e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'grad_norm': 1.7142860997410025e-06, 'learning_rate': 2.9817946784444684e-05, 'epoch': 1.4}\n",
      "{'loss': 0.6151, 'grad_norm': 8.25493202682992e-07, 'learning_rate': 2.9764084886351395e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'grad_norm': 3.48907763791928e-10, 'learning_rate': 2.9710222988258106e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0, 'grad_norm': 5.513584255822934e-05, 'learning_rate': 2.965636109016482e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0002, 'grad_norm': 7.839792459662931e-08, 'learning_rate': 2.960249919207153e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0, 'grad_norm': 1.0667693572941062e-07, 'learning_rate': 2.9548637293978243e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0, 'grad_norm': 4.928351615696158e-10, 'learning_rate': 2.9494775395884954e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0008, 'grad_norm': 1.150533307736623e-06, 'learning_rate': 2.944091349779166e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0, 'grad_norm': 5.144335091245011e-07, 'learning_rate': 2.9387051599698372e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0, 'grad_norm': 1.0638549611030612e-05, 'learning_rate': 2.9333189701605084e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0, 'grad_norm': 6.767320428480161e-07, 'learning_rate': 2.9279327803511798e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0011, 'grad_norm': 0.0002849502780009061, 'learning_rate': 2.922546590541851e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0001, 'grad_norm': 2.5156172256401987e-08, 'learning_rate': 2.917160400732522e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0072, 'grad_norm': 2.3603276133599138e-07, 'learning_rate': 2.911774210923193e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0, 'grad_norm': 1.6760519638836513e-09, 'learning_rate': 2.9063880211138646e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0, 'grad_norm': 1.6307268424498034e-06, 'learning_rate': 2.901001831304535e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0, 'grad_norm': 1.3293132816727393e-08, 'learning_rate': 2.895615641495206e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0, 'grad_norm': 2.3782670268701622e-06, 'learning_rate': 2.8902294516858776e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011752072896342725, 'learning_rate': 2.8848432618765487e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0005, 'grad_norm': 1.4413685089226247e-12, 'learning_rate': 2.8794570720672198e-05, 'epoch': 1.42}\n",
      "{'loss': 0.3543, 'grad_norm': 1.8777087461785413e-05, 'learning_rate': 2.874070882257891e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0, 'grad_norm': 1.867094326368779e-11, 'learning_rate': 2.8686846924485623e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0, 'grad_norm': 2.6078963433207036e-09, 'learning_rate': 2.8632985026392334e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0, 'grad_norm': 2.8278993369212913e-08, 'learning_rate': 2.857912312829904e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0, 'grad_norm': 1.0736657118926018e-09, 'learning_rate': 2.8525261230205753e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0, 'grad_norm': 1.2239430361660197e-05, 'learning_rate': 2.8471399332112464e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0, 'grad_norm': 6.648309636148042e-08, 'learning_rate': 2.8417537434019175e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0, 'grad_norm': 1.2437708392099012e-06, 'learning_rate': 2.8363675535925887e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0006, 'grad_norm': 1.9504107058310183e-06, 'learning_rate': 2.83098136378326e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0, 'grad_norm': 3.389747143955901e-05, 'learning_rate': 2.8255951739739312e-05, 'epoch': 1.43}\n",
      "{'loss': 0.668, 'grad_norm': 2.5869528741395698e-09, 'learning_rate': 2.8202089841646023e-05, 'epoch': 1.44}\n",
      "{'loss': 0.1969, 'grad_norm': 1.7701369870337658e-05, 'learning_rate': 2.814822794355273e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0, 'grad_norm': 7.759048799016455e-07, 'learning_rate': 2.8094366045459442e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0, 'grad_norm': 1.4908037826444342e-07, 'learning_rate': 2.8040504147366153e-05, 'epoch': 1.44}\n",
      "{'loss': 0.4292, 'grad_norm': 0.2481183260679245, 'learning_rate': 2.7986642249272864e-05, 'epoch': 1.44}\n",
      "{'loss': 0.002, 'grad_norm': 0.001472587464377284, 'learning_rate': 2.793278035117958e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0, 'grad_norm': 2.568315949247335e-06, 'learning_rate': 2.787891845308629e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0021, 'grad_norm': 9.064006434300609e-08, 'learning_rate': 2.7825056554993e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0, 'grad_norm': 6.50832676640789e-09, 'learning_rate': 2.7771194656899712e-05, 'epoch': 1.44}\n",
      "{'loss': 0.1175, 'grad_norm': 3.7014760145837045e-09, 'learning_rate': 2.771733275880642e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0, 'grad_norm': 3.8957863580435514e-05, 'learning_rate': 2.766347086071313e-05, 'epoch': 1.45}\n",
      "{'loss': 0.3291, 'grad_norm': 7.923781026875076e-07, 'learning_rate': 2.7609608962619842e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0, 'grad_norm': 2.8914953986713954e-07, 'learning_rate': 2.7555747064526556e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0124, 'grad_norm': 2.221484436404353e-07, 'learning_rate': 2.7501885166433267e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006142006022855639, 'learning_rate': 2.744802326833998e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0006, 'grad_norm': 5.161288072486059e-07, 'learning_rate': 2.739416137024669e-05, 'epoch': 1.45}\n",
      "{'loss': 0.672, 'grad_norm': 1.4743663179217492e-08, 'learning_rate': 2.73402994721534e-05, 'epoch': 1.45}\n",
      "{'loss': 0.5116, 'grad_norm': 2.0342871209777513e-07, 'learning_rate': 2.728643757406011e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0, 'grad_norm': 2.140463220712263e-05, 'learning_rate': 2.723257567596682e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0, 'grad_norm': 6.203848170116544e-05, 'learning_rate': 2.7178713777873534e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0, 'grad_norm': 2.2574667468688858e-08, 'learning_rate': 2.7124851879780245e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0029, 'grad_norm': 1.9603601694107056, 'learning_rate': 2.7070989981686956e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0, 'grad_norm': 8.604170709247683e-10, 'learning_rate': 2.7017128083593667e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0, 'grad_norm': 1.6934022823988926e-06, 'learning_rate': 2.6963266185500378e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0, 'grad_norm': 2.9791347060381668e-06, 'learning_rate': 2.6909404287407093e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0001, 'grad_norm': 1.1389706742193084e-06, 'learning_rate': 2.6855542389313797e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0, 'grad_norm': 1.0651413049345138e-06, 'learning_rate': 2.680168049122051e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0014, 'grad_norm': 1.0844724229741587e-08, 'learning_rate': 2.6747818593127223e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0, 'grad_norm': 7.940895011415705e-05, 'learning_rate': 2.6693956695033934e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0, 'grad_norm': 1.0849511454580352e-05, 'learning_rate': 2.6640094796940645e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0001, 'grad_norm': 5.993061336084793e-08, 'learning_rate': 2.6586232898847356e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0, 'grad_norm': 1.3978294077787723e-08, 'learning_rate': 2.653237100075407e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0, 'grad_norm': 3.162447415050451e-09, 'learning_rate': 2.647850910266078e-05, 'epoch': 1.47}\n",
      "{'loss': 0.4998, 'grad_norm': 8.537150506526814e-07, 'learning_rate': 2.642464720456749e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0, 'grad_norm': 1.0996202945534606e-05, 'learning_rate': 2.63707853064742e-05, 'epoch': 1.47}\n",
      "{'loss': 0.7503, 'grad_norm': 1.801924156552559e-07, 'learning_rate': 2.631692340838091e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0052, 'grad_norm': 7.684888259973377e-06, 'learning_rate': 2.6263061510287622e-05, 'epoch': 1.47}\n",
      "{'loss': 0.084, 'grad_norm': 7.721392307757924e-07, 'learning_rate': 2.6209199612194334e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0, 'grad_norm': 0.00015409273328259587, 'learning_rate': 2.6155337714101048e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0, 'grad_norm': 0.005612081848084927, 'learning_rate': 2.610147581600776e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0, 'grad_norm': 3.503515983993566e-07, 'learning_rate': 2.604761391791447e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0, 'grad_norm': 1.1276441682639415e-06, 'learning_rate': 2.5993752019821178e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0003, 'grad_norm': 3.930952152586542e-05, 'learning_rate': 2.593989012172789e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0, 'grad_norm': 3.94399449987759e-07, 'learning_rate': 2.58860282236346e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0, 'grad_norm': 8.061701350925432e-07, 'learning_rate': 2.583216632554131e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0, 'grad_norm': 4.6270955955662885e-09, 'learning_rate': 2.5778304427448026e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0, 'grad_norm': 1.3594175563014232e-08, 'learning_rate': 2.5724442529354737e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0, 'grad_norm': 9.17279230350232e-09, 'learning_rate': 2.5670580631261448e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0, 'grad_norm': 4.4951372046853066e-07, 'learning_rate': 2.561671873316816e-05, 'epoch': 1.49}\n",
      "{'loss': 0.1099, 'grad_norm': 88.7966537475586, 'learning_rate': 2.5562856835074867e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0, 'grad_norm': 3.726613431354053e-05, 'learning_rate': 2.5508994936981578e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0, 'grad_norm': 9.150711775873788e-06, 'learning_rate': 2.545513303888829e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2566, 'grad_norm': 84.68161010742188, 'learning_rate': 2.5401271140795003e-05, 'epoch': 1.49}\n",
      "{'loss': 0.107, 'grad_norm': 9.629923624743242e-06, 'learning_rate': 2.5347409242701714e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0031, 'grad_norm': 7.772254321025684e-05, 'learning_rate': 2.5293547344608425e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0239, 'grad_norm': 1.4882085679346346e-06, 'learning_rate': 2.5239685446515137e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0001, 'grad_norm': 0.001959968823939562, 'learning_rate': 2.518582354842185e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0011, 'grad_norm': 7.285044745231062e-08, 'learning_rate': 2.5131961650328562e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0001, 'grad_norm': 4.184800275197631e-07, 'learning_rate': 2.5078099752235266e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0, 'grad_norm': 3.6509006804408273e-06, 'learning_rate': 2.502423785414198e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0025, 'grad_norm': 2.8833392207161523e-07, 'learning_rate': 2.4970375956048692e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0007, 'grad_norm': 3.0708945359947393e-06, 'learning_rate': 2.4916514057955403e-05, 'epoch': 1.5}\n",
      "{'loss': 0.1824, 'grad_norm': 2.709450200200081e-06, 'learning_rate': 2.4862652159862114e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0, 'grad_norm': 1.079091134670307e-06, 'learning_rate': 2.480879026176883e-05, 'epoch': 1.5}\n",
      "{'loss': 0.1839, 'grad_norm': 5.669391356377673e-08, 'learning_rate': 2.4754928363675536e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0, 'grad_norm': 1.0834892805178242e-07, 'learning_rate': 2.4701066465582247e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0, 'grad_norm': 1.5966526916599832e-05, 'learning_rate': 2.464720456748896e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0, 'grad_norm': 1.9005523427040316e-05, 'learning_rate': 2.4593342669395673e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0, 'grad_norm': 1.6269093521259492e-06, 'learning_rate': 2.453948077130238e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0, 'grad_norm': 0.017658455297350883, 'learning_rate': 2.4485618873209092e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0073, 'grad_norm': 2.4868722903192975e-06, 'learning_rate': 2.4431756975115806e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0, 'grad_norm': 1.2361889289991268e-08, 'learning_rate': 2.4377895077022517e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0, 'grad_norm': 7.718973904502491e-08, 'learning_rate': 2.4324033178929225e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0002, 'grad_norm': 8.624820111435838e-06, 'learning_rate': 2.4270171280835936e-05, 'epoch': 1.51}\n",
      "{'loss': 0.3707, 'grad_norm': 2.0684498558409814e-10, 'learning_rate': 2.421630938274265e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0, 'grad_norm': 6.507361871577189e-10, 'learning_rate': 2.4162447484649362e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0, 'grad_norm': 3.0326404157676734e-05, 'learning_rate': 2.410858558655607e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0284, 'grad_norm': 1.0302509281245875e-06, 'learning_rate': 2.4054723688462784e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0, 'grad_norm': 4.822994128517166e-07, 'learning_rate': 2.4000861790369495e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0, 'grad_norm': 9.998095265473239e-06, 'learning_rate': 2.3946999892276206e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0004, 'grad_norm': 8.465689660397402e-08, 'learning_rate': 2.3893137994182914e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0304, 'grad_norm': 4.5601076692491915e-08, 'learning_rate': 2.3839276096089628e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0, 'grad_norm': 4.0474878915119916e-05, 'learning_rate': 2.378541419799634e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0033, 'grad_norm': 1.1370397260179743e-05, 'learning_rate': 2.373155229990305e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0, 'grad_norm': 1.0382134860265069e-05, 'learning_rate': 2.367769040180976e-05, 'epoch': 1.53}\n",
      "{'loss': 0.31, 'grad_norm': 3.858673572540283, 'learning_rate': 2.3623828503716473e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0, 'grad_norm': 9.12311804768251e-07, 'learning_rate': 2.3569966605623184e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0005, 'grad_norm': 3.2315321618625603e-07, 'learning_rate': 2.3516104707529895e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0, 'grad_norm': 3.8868392948643304e-08, 'learning_rate': 2.3462242809436606e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0, 'grad_norm': 3.396508532205189e-07, 'learning_rate': 2.3408380911343317e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0001192890340462327, 'learning_rate': 2.3354519013250028e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008791260188445449, 'learning_rate': 2.330065711515674e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0, 'grad_norm': 7.251780516526196e-06, 'learning_rate': 2.324679521706345e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 9.173645594273694e-06, 'learning_rate': 2.319293331897016e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 1.9578459387048497e-07, 'learning_rate': 2.3139071420876872e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 2.2178782899118232e-07, 'learning_rate': 2.3085209522783583e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 1.0205618394820704e-07, 'learning_rate': 2.3031347624690295e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011289962276350707, 'learning_rate': 2.2977485726597006e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 8.86925988652365e-07, 'learning_rate': 2.2923623828503717e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 1.9073853763984516e-05, 'learning_rate': 2.286976193041043e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 1.7640289229348127e-10, 'learning_rate': 2.281590003231714e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0224, 'grad_norm': 2.9080621288812836e-07, 'learning_rate': 2.276203813422385e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011227556387893856, 'learning_rate': 2.270817623613056e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0, 'grad_norm': 4.572769967126078e-07, 'learning_rate': 2.2654314338037276e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0065, 'grad_norm': 10.966789245605469, 'learning_rate': 2.2600452439943983e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0, 'grad_norm': 6.161071208765634e-09, 'learning_rate': 2.2546590541850694e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0, 'grad_norm': 1.1455498679424636e-05, 'learning_rate': 2.249272864375741e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0004, 'grad_norm': 1.3216416405725795e-08, 'learning_rate': 2.243886674566412e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0, 'grad_norm': 2.7829230031528596e-09, 'learning_rate': 2.2385004847570828e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0, 'grad_norm': 8.371156212660935e-08, 'learning_rate': 2.233114294947754e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0, 'grad_norm': 1.4615883969781862e-07, 'learning_rate': 2.2277281051384253e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0, 'grad_norm': 9.12463349322934e-08, 'learning_rate': 2.2223419153290964e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 4.464685616767383e-07, 'learning_rate': 2.2169557255197672e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 3.8699477045156527e-07, 'learning_rate': 2.2115695357104387e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 1.1808638511467962e-08, 'learning_rate': 2.2061833459011098e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 2.657804998307256e-08, 'learning_rate': 2.200797156091781e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 3.3648106523287424e-07, 'learning_rate': 2.1954109662824516e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 8.979837673450675e-08, 'learning_rate': 2.190024776473123e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 2.019540033870726e-06, 'learning_rate': 2.1846385866637942e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002967613108921796, 'learning_rate': 2.1792523968544653e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 0.01386455912142992, 'learning_rate': 2.1738662070451364e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0001, 'grad_norm': 1.3769490578852128e-06, 'learning_rate': 2.1684800172358075e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003052598622161895, 'learning_rate': 2.1630938274264786e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0006, 'grad_norm': 0.04726811870932579, 'learning_rate': 2.1577076376171497e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0, 'grad_norm': 4.827558655051689e-07, 'learning_rate': 2.152321447807821e-05, 'epoch': 1.57}\n",
      "{'loss': 0.5939, 'grad_norm': 0.0003067916550207883, 'learning_rate': 2.146935257998492e-05, 'epoch': 1.57}\n",
      "{'loss': 0.4562, 'grad_norm': 1.4742902294528903e-06, 'learning_rate': 2.141549068189163e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0, 'grad_norm': 1.7177819700009422e-07, 'learning_rate': 2.1361628783798342e-05, 'epoch': 1.57}\n",
      "{'loss': 0.1409, 'grad_norm': 2.1688087770144193e-08, 'learning_rate': 2.1307766885705053e-05, 'epoch': 1.57}\n",
      "{'loss': 0.001, 'grad_norm': 0.0012269694125279784, 'learning_rate': 2.1253904987611764e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0001, 'grad_norm': 1.1008565792280933e-07, 'learning_rate': 2.1200043089518475e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0001, 'grad_norm': 3.8754626530135283e-07, 'learning_rate': 2.114618119142519e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0, 'grad_norm': 1.372796987197944e-06, 'learning_rate': 2.1092319293331897e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0, 'grad_norm': 0.0010077268816530704, 'learning_rate': 2.1038457395238608e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0, 'grad_norm': 7.058175128804578e-07, 'learning_rate': 2.098459549714532e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003328062011860311, 'learning_rate': 2.0930733599052034e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0046, 'grad_norm': 0.0003263618564233184, 'learning_rate': 2.087687170095874e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0, 'grad_norm': 2.7518146907823393e-06, 'learning_rate': 2.0823009802865453e-05, 'epoch': 1.58}\n",
      "{'loss': 0.2699, 'grad_norm': 0.00015192103455774486, 'learning_rate': 2.0769147904772164e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0, 'grad_norm': 0.005678105633705854, 'learning_rate': 2.0715286006678878e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0, 'grad_norm': 2.702146275623818e-06, 'learning_rate': 2.0661424108585586e-05, 'epoch': 1.59}\n",
      "{'loss': 0.385, 'grad_norm': 0.9127275347709656, 'learning_rate': 2.0607562210492297e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0, 'grad_norm': 8.206328629967174e-07, 'learning_rate': 2.055370031239901e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0, 'grad_norm': 1.8631661191648163e-07, 'learning_rate': 2.0499838414305723e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0011491569457575679, 'learning_rate': 2.044597651621243e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0, 'grad_norm': 4.245993068252574e-08, 'learning_rate': 2.039211461811914e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0, 'grad_norm': 9.865211758608439e-09, 'learning_rate': 2.0338252720025856e-05, 'epoch': 1.59}\n",
      "{'loss': 0.2486, 'grad_norm': 64.65361022949219, 'learning_rate': 2.0284390821932567e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0, 'grad_norm': 1.1123772765131434e-06, 'learning_rate': 2.0230528923839275e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'grad_norm': 7.501556865463499e-06, 'learning_rate': 2.017666702574599e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'grad_norm': 9.556949676081672e-10, 'learning_rate': 2.01228051276527e-05, 'epoch': 1.6}\n",
      "{'loss': 0.1461, 'grad_norm': 1.4969542689868831e-06, 'learning_rate': 2.006894322955941e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0009, 'grad_norm': 1.898804293887224e-05, 'learning_rate': 2.001508133146612e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002195057604694739, 'learning_rate': 1.9961219433372833e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'grad_norm': 4.5433230866365193e-07, 'learning_rate': 1.9907357535279545e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0948, 'grad_norm': 2.914569904532982e-06, 'learning_rate': 1.9853495637186256e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'grad_norm': 4.893512959824875e-05, 'learning_rate': 1.9799633739092967e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'grad_norm': 3.4646747693045654e-09, 'learning_rate': 1.9745771840999678e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0, 'grad_norm': 1.3281588451263815e-07, 'learning_rate': 1.969190994290639e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0, 'grad_norm': 7.402584742521867e-06, 'learning_rate': 1.96380480448131e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0044, 'grad_norm': 2.9445233806768556e-08, 'learning_rate': 1.958418614671981e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0, 'grad_norm': 0.0015923468163236976, 'learning_rate': 1.9530324248626522e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0028, 'grad_norm': 1.3061938943792484e-06, 'learning_rate': 1.9476462350533233e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0, 'grad_norm': 6.426877371268347e-05, 'learning_rate': 1.9422600452439944e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0, 'grad_norm': 1.6560370568186045e-06, 'learning_rate': 1.9368738554346655e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0001, 'grad_norm': 1.127394853028818e-06, 'learning_rate': 1.9314876656253367e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0, 'grad_norm': 8.946335228188218e-09, 'learning_rate': 1.9261014758160078e-05, 'epoch': 1.61}\n",
      "{'loss': 0.009, 'grad_norm': 5.02049829265161e-07, 'learning_rate': 1.9207152860066792e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0, 'grad_norm': 9.454432984057348e-06, 'learning_rate': 1.91532909619735e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0032, 'grad_norm': 2.7576372758630896e-06, 'learning_rate': 1.909942906388021e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0094, 'grad_norm': 1.2229415006004274e-05, 'learning_rate': 1.9045567165786922e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0001, 'grad_norm': 8.246395009336993e-05, 'learning_rate': 1.8991705267693636e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0004, 'grad_norm': 7.879883924033493e-06, 'learning_rate': 1.8937843369600344e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0017, 'grad_norm': 2.381058976652639e-07, 'learning_rate': 1.8883981471507055e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0, 'grad_norm': 1.1853167052322533e-06, 'learning_rate': 1.883011957341377e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0047655352391302586, 'learning_rate': 1.877625767532048e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0, 'grad_norm': 1.497927087257267e-06, 'learning_rate': 1.872239577722719e-05, 'epoch': 1.63}\n",
      "{'loss': 0.3902, 'grad_norm': 8.805444622339564e-07, 'learning_rate': 1.86685338791339e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0, 'grad_norm': 5.179775689612143e-05, 'learning_rate': 1.8614671981040614e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0, 'grad_norm': 7.035188875903486e-09, 'learning_rate': 1.8560810082947325e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0, 'grad_norm': 1.5525544738714814e-10, 'learning_rate': 1.8506948184854033e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012274547771085054, 'learning_rate': 1.8453086286760744e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0, 'grad_norm': 0.001721791224554181, 'learning_rate': 1.839922438866746e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0457, 'grad_norm': 6.443514610765533e-09, 'learning_rate': 1.834536249057417e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0001, 'grad_norm': 3.2436631158816454e-07, 'learning_rate': 1.8291500592480877e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0, 'grad_norm': 1.1038015145459212e-05, 'learning_rate': 1.8237638694387592e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0, 'grad_norm': 2.1105354335304582e-06, 'learning_rate': 1.8183776796294303e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0017, 'grad_norm': 2.2725412662794042e-08, 'learning_rate': 1.8129914898201014e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0, 'grad_norm': 3.150583722799638e-07, 'learning_rate': 1.807605300010772e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0042, 'grad_norm': 3.8621595876975334e-07, 'learning_rate': 1.8022191102014436e-05, 'epoch': 1.64}\n",
      "{'loss': 0.189, 'grad_norm': 2.6701611659518676e-06, 'learning_rate': 1.7968329203921147e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0031, 'grad_norm': 3.0609164696215885e-06, 'learning_rate': 1.7914467305827858e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0002, 'grad_norm': 1.1727547644113656e-05, 'learning_rate': 1.786060540773457e-05, 'epoch': 1.64}\n",
      "{'loss': 0.5907, 'grad_norm': 134.67051696777344, 'learning_rate': 1.780674350964128e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0001, 'grad_norm': 3.4625301736923575e-07, 'learning_rate': 1.775288161154799e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0, 'grad_norm': 2.457392611177056e-06, 'learning_rate': 1.7699019713454703e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0, 'grad_norm': 6.949016473356551e-09, 'learning_rate': 1.7645157815361417e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001508200220996514, 'learning_rate': 1.7591295917268125e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0, 'grad_norm': 1.9076596800005063e-06, 'learning_rate': 1.7537434019174836e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0, 'grad_norm': 0.002414242597296834, 'learning_rate': 1.7483572121081547e-05, 'epoch': 1.65}\n",
      "{'loss': 0.9846, 'grad_norm': 2.2139215616334695e-07, 'learning_rate': 1.742971022298826e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0, 'grad_norm': 2.1171463231439702e-06, 'learning_rate': 1.737584832489497e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0, 'grad_norm': 1.5850771717396128e-07, 'learning_rate': 1.732198642680168e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0, 'grad_norm': 2.2019984058374575e-08, 'learning_rate': 1.7268124528708395e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0, 'grad_norm': 0.00028544783708639443, 'learning_rate': 1.7214262630615106e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0, 'grad_norm': 2.1168290231798892e-07, 'learning_rate': 1.7160400732521814e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0203, 'grad_norm': 1.7637778455537045e-06, 'learning_rate': 1.7106538834428525e-05, 'epoch': 1.66}\n",
      "{'loss': 0.018, 'grad_norm': 2.8225766072864644e-05, 'learning_rate': 1.705267693633524e-05, 'epoch': 1.66}\n",
      "{'loss': 0.1423, 'grad_norm': 1.8514449493522989e-06, 'learning_rate': 1.699881503824195e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0, 'grad_norm': 1.211776634590933e-06, 'learning_rate': 1.6944953140148658e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0, 'grad_norm': 5.5211843573488295e-05, 'learning_rate': 1.6891091242055372e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0, 'grad_norm': 4.7566626903972065e-07, 'learning_rate': 1.6837229343962083e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0, 'grad_norm': 0.00959212426096201, 'learning_rate': 1.6783367445868795e-05, 'epoch': 1.66}\n",
      "{'loss': 0.2982, 'grad_norm': 1.4976107195252553e-05, 'learning_rate': 1.6729505547775502e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0085, 'grad_norm': 1.0684637175017997e-08, 'learning_rate': 1.6675643649682217e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0002, 'grad_norm': 9.82712045072276e-09, 'learning_rate': 1.6621781751588928e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0, 'grad_norm': 7.807102880841299e-11, 'learning_rate': 1.656791985349564e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0, 'grad_norm': 7.896045644883998e-06, 'learning_rate': 1.651405795540235e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0011244724737480283, 'learning_rate': 1.646019605730906e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0001, 'grad_norm': 8.811616325488103e-09, 'learning_rate': 1.6406334159215772e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0, 'grad_norm': 1.2475632047426188e-06, 'learning_rate': 1.6352472261122483e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0, 'grad_norm': 6.360718884934613e-07, 'learning_rate': 1.6298610363029194e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0003, 'grad_norm': 7.426728121373571e-10, 'learning_rate': 1.6244748464935905e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0, 'grad_norm': 2.2396525309886783e-06, 'learning_rate': 1.6190886566842617e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0102, 'grad_norm': 2.2688936951453798e-07, 'learning_rate': 1.6137024668749328e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0, 'grad_norm': 4.8154129217437e-07, 'learning_rate': 1.608316277065604e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0, 'grad_norm': 3.2997888865793357e-06, 'learning_rate': 1.602930087256275e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0, 'grad_norm': 1.375375859424821e-06, 'learning_rate': 1.597543897446946e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0001, 'grad_norm': 5.5301534302998334e-05, 'learning_rate': 1.5921577076376172e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0, 'grad_norm': 2.0211726337038272e-07, 'learning_rate': 1.5867715178282883e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0188, 'grad_norm': 9.645518161960354e-08, 'learning_rate': 1.5813853280189594e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0, 'grad_norm': 6.046595899533713e-07, 'learning_rate': 1.5759991382096305e-05, 'epoch': 1.68}\n",
      "{'loss': 0.2731, 'grad_norm': 1.408594907381655e-09, 'learning_rate': 1.570612948400302e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0, 'grad_norm': 3.2311172049048764e-07, 'learning_rate': 1.5652267585909727e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0, 'grad_norm': 1.2314427294768393e-05, 'learning_rate': 1.559840568781644e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0645, 'grad_norm': 6.231823022062599e-07, 'learning_rate': 1.554454378972315e-05, 'epoch': 1.69}\n",
      "{'loss': 0.2192, 'grad_norm': 0.005341168027371168, 'learning_rate': 1.5490681891629864e-05, 'epoch': 1.69}\n",
      "{'loss': 0.1535, 'grad_norm': 0.0034509124234318733, 'learning_rate': 1.5436819993536572e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0, 'grad_norm': 1.8856542993717085e-08, 'learning_rate': 1.5382958095443283e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0236, 'grad_norm': 0.0010149604640901089, 'learning_rate': 1.5329096197349997e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0001, 'grad_norm': 0.18409563601016998, 'learning_rate': 1.527523429925671e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0007, 'grad_norm': 3.222015220671892e-05, 'learning_rate': 1.5221372401163416e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 9.509774827165529e-06, 'learning_rate': 1.5167510503070129e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 8.650233212392777e-06, 'learning_rate': 1.511364860497684e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 2.172333007921523e-10, 'learning_rate': 1.5059786706883553e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 2.2637364054389764e-06, 'learning_rate': 1.5005924808790262e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 7.990261110535357e-06, 'learning_rate': 1.4952062910696973e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 0.052145253866910934, 'learning_rate': 1.4898201012603686e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 8.338761290360708e-07, 'learning_rate': 1.4844339114510397e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 1.659583176660817e-05, 'learning_rate': 1.4790477216417107e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 1.0127141649718396e-06, 'learning_rate': 1.4736615318323818e-05, 'epoch': 1.71}\n",
      "{'loss': 0.2783, 'grad_norm': 0.000541421992238611, 'learning_rate': 1.468275342023053e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0001, 'grad_norm': 0.18567992746829987, 'learning_rate': 1.4628891522137242e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0, 'grad_norm': 9.01037401490612e-06, 'learning_rate': 1.4575029624043951e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005341644864529371, 'learning_rate': 1.4521167725950662e-05, 'epoch': 1.71}\n",
      "{'loss': 0.5077, 'grad_norm': 4.2892338569799904e-07, 'learning_rate': 1.4467305827857375e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0, 'grad_norm': 2.1782887415611185e-05, 'learning_rate': 1.4413443929764086e-05, 'epoch': 1.71}\n",
      "{'loss': 0.3936, 'grad_norm': 7.99598274170421e-06, 'learning_rate': 1.4359582031670795e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0003, 'grad_norm': 9.655209396441933e-07, 'learning_rate': 1.4305720133577508e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0, 'grad_norm': 2.855572889259861e-09, 'learning_rate': 1.425185823548422e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0842, 'grad_norm': 7.382628837149241e-07, 'learning_rate': 1.4197996337390932e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 6.601810582651524e-07, 'learning_rate': 1.414413443929764e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 1.0968094488195135e-10, 'learning_rate': 1.4090272541204352e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 1.2022620694551733e-06, 'learning_rate': 1.4036410643111064e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 4.6908817807889136e-07, 'learning_rate': 1.3982548745017776e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 2.2958907663905848e-07, 'learning_rate': 1.3928686846924486e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 2.9853816158720292e-05, 'learning_rate': 1.3874824948831197e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 5.55021979380399e-05, 'learning_rate': 1.382096305073791e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 6.252997764022439e-07, 'learning_rate': 1.376710115264462e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 7.523254907937371e-07, 'learning_rate': 1.371323925455133e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0, 'grad_norm': 2.1333016775315627e-07, 'learning_rate': 1.3659377356458041e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0, 'grad_norm': 6.974720356822672e-09, 'learning_rate': 1.3605515458364754e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0, 'grad_norm': 3.6438312235986814e-05, 'learning_rate': 1.3551653560271465e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0, 'grad_norm': 6.221916351023538e-07, 'learning_rate': 1.3497791662178174e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0, 'grad_norm': 1.4823942073860508e-09, 'learning_rate': 1.3443929764084887e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0009, 'grad_norm': 8.043879518027097e-08, 'learning_rate': 1.3390067865991598e-05, 'epoch': 1.73}\n",
      "{'loss': 0.2451, 'grad_norm': 0.0006882407469674945, 'learning_rate': 1.3336205967898311e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0, 'grad_norm': 5.446129947017653e-08, 'learning_rate': 1.3282344069805019e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0025, 'grad_norm': 3.988060235977173, 'learning_rate': 1.3228482171711732e-05, 'epoch': 1.74}\n",
      "{'loss': 0.1684, 'grad_norm': 1.823716957005672e-05, 'learning_rate': 1.3174620273618443e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0, 'grad_norm': 7.719002496742178e-06, 'learning_rate': 1.3120758375525155e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0, 'grad_norm': 1.7014670447679237e-05, 'learning_rate': 1.3066896477431865e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0002, 'grad_norm': 0.00020278671581763774, 'learning_rate': 1.3013034579338576e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0, 'grad_norm': 1.0887924162261697e-07, 'learning_rate': 1.2959172681245289e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0, 'grad_norm': 7.207187309177243e-07, 'learning_rate': 1.2905310783152e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0, 'grad_norm': 2.3301194573832618e-07, 'learning_rate': 1.285144888505871e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0007, 'grad_norm': 7.801525498507544e-06, 'learning_rate': 1.279758698696542e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0, 'grad_norm': 2.7159367164131254e-05, 'learning_rate': 1.2743725088872133e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0001, 'grad_norm': 3.835231576765352e-10, 'learning_rate': 1.2689863190778844e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0, 'grad_norm': 3.613907608723821e-07, 'learning_rate': 1.2636001292685554e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0, 'grad_norm': 1.4300203474704176e-06, 'learning_rate': 1.2582139394592266e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0001, 'grad_norm': 1.4564553729723428e-10, 'learning_rate': 1.2528277496498977e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0, 'grad_norm': 1.936866055984865e-06, 'learning_rate': 1.2474415598405689e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0, 'grad_norm': 1.1480035482236417e-06, 'learning_rate': 1.24205537003124e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0, 'grad_norm': 2.083939847352667e-07, 'learning_rate': 1.236669180221911e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0001, 'grad_norm': 4.012395606878272e-07, 'learning_rate': 1.2312829904125822e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0803, 'grad_norm': 0.00025268326862715185, 'learning_rate': 1.2258968006032533e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0004, 'grad_norm': 1.734670513542369e-05, 'learning_rate': 1.2205106107939244e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0, 'grad_norm': 1.706732518869103e-06, 'learning_rate': 1.2151244209845955e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0, 'grad_norm': 5.418851287686266e-07, 'learning_rate': 1.2097382311752666e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0025, 'grad_norm': 0.0017818909836933017, 'learning_rate': 1.2043520413659377e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0581, 'grad_norm': 9.948812884630343e-10, 'learning_rate': 1.198965851556609e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0093, 'grad_norm': 0.0017470777966082096, 'learning_rate': 1.19357966174728e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0023673975374549627, 'learning_rate': 1.1881934719379512e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0, 'grad_norm': 2.46154144178945e-07, 'learning_rate': 1.1828072821286222e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0, 'grad_norm': 7.474128871365338e-09, 'learning_rate': 1.1774210923192934e-05, 'epoch': 1.76}\n",
      "{'loss': 0.1091, 'grad_norm': 2.685781328182202e-05, 'learning_rate': 1.1720349025099644e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0002, 'grad_norm': 1.8660590512808994e-06, 'learning_rate': 1.1666487127006357e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0, 'grad_norm': 1.534819830339984e-06, 'learning_rate': 1.1612625228913068e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0, 'grad_norm': 7.345120684476569e-05, 'learning_rate': 1.1558763330819779e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0, 'grad_norm': 0.00014428938447963446, 'learning_rate': 1.150490143272649e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0, 'grad_norm': 1.1601517826420604e-06, 'learning_rate': 1.1451039534633201e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0012, 'grad_norm': 0.0029419439379125834, 'learning_rate': 1.1397177636539912e-05, 'epoch': 1.77}\n",
      "{'loss': 0.031, 'grad_norm': 3.9702459275758883e-07, 'learning_rate': 1.1343315738446623e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0, 'grad_norm': 0.01981073059141636, 'learning_rate': 1.1289453840353334e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0, 'grad_norm': 0.00015010220522526652, 'learning_rate': 1.1235591942260045e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0401, 'grad_norm': 0.0006075574783608317, 'learning_rate': 1.1181730044166756e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0002, 'grad_norm': 3.2223164453171194e-05, 'learning_rate': 1.1127868146073467e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0, 'grad_norm': 5.993097147438675e-07, 'learning_rate': 1.1074006247980179e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0, 'grad_norm': 0.00021016725804656744, 'learning_rate': 1.1020144349886891e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0054, 'grad_norm': 5.870730400085449, 'learning_rate': 1.09662824517936e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0, 'grad_norm': 0.00565593084320426, 'learning_rate': 1.0912420553700314e-05, 'epoch': 1.78}\n",
      "{'loss': 0.1248, 'grad_norm': 0.0007777253049425781, 'learning_rate': 1.0858558655607023e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0001, 'grad_norm': 3.3483935624190053e-09, 'learning_rate': 1.0804696757513736e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0, 'grad_norm': 1.3089309049973963e-06, 'learning_rate': 1.0750834859420445e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008878700318746269, 'learning_rate': 1.0696972961327158e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0279, 'grad_norm': 41.59709930419922, 'learning_rate': 1.0643111063233869e-05, 'epoch': 1.79}\n",
      "{'loss': 0.1112, 'grad_norm': 74.82685089111328, 'learning_rate': 1.058924916514058e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0001, 'grad_norm': 3.9414389902958646e-05, 'learning_rate': 1.0535387267047291e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0, 'grad_norm': 4.97705059387954e-07, 'learning_rate': 1.0481525368954002e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0, 'grad_norm': 4.1688110741233686e-08, 'learning_rate': 1.0427663470860715e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0, 'grad_norm': 4.140489284765181e-09, 'learning_rate': 1.0373801572767424e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0002, 'grad_norm': 1.2203492588014342e-05, 'learning_rate': 1.0319939674674137e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0, 'grad_norm': 0.0010617133229970932, 'learning_rate': 1.0266077776580847e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0, 'grad_norm': 8.225058678590358e-08, 'learning_rate': 1.021221587848756e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0, 'grad_norm': 0.015833577141165733, 'learning_rate': 1.0158353980394269e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0001, 'grad_norm': 7.804081292306364e-07, 'learning_rate': 1.0104492082300982e-05, 'epoch': 1.8}\n",
      "{'loss': 0.1991, 'grad_norm': 2.7677668867909233e-07, 'learning_rate': 1.0050630184207693e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0, 'grad_norm': 1.4434451713896124e-06, 'learning_rate': 9.996768286114404e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0009, 'grad_norm': 6.944382039364427e-05, 'learning_rate': 9.942906388021115e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0006, 'grad_norm': 6.785894157701478e-08, 'learning_rate': 9.889044489927826e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0, 'grad_norm': 1.958804451973606e-09, 'learning_rate': 9.835182591834537e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0004, 'grad_norm': 3.2423170068796026e-06, 'learning_rate': 9.781320693741248e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0, 'grad_norm': 4.669028541570697e-08, 'learning_rate': 9.72745879564796e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0191, 'grad_norm': 7.779744009894785e-06, 'learning_rate': 9.67359689755467e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0, 'grad_norm': 1.9168044673278928e-05, 'learning_rate': 9.619734999461381e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0, 'grad_norm': 0.00025187357096001506, 'learning_rate': 9.565873101368092e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0001, 'grad_norm': 7.258494747475197e-07, 'learning_rate': 9.512011203274804e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0001, 'grad_norm': 5.63392968615517e-05, 'learning_rate': 9.458149305181516e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0, 'grad_norm': 1.922580494806425e-08, 'learning_rate': 9.404287407088226e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0, 'grad_norm': 7.435744464601157e-06, 'learning_rate': 9.350425508994939e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0, 'grad_norm': 3.5690834465640364e-07, 'learning_rate': 9.296563610901648e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0, 'grad_norm': 2.7904025046154857e-07, 'learning_rate': 9.24270171280836e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0, 'grad_norm': 1.3980143194203265e-06, 'learning_rate': 9.18883981471507e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0, 'grad_norm': 1.3685579336453202e-09, 'learning_rate': 9.134977916621783e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0, 'grad_norm': 3.3362186968588503e-07, 'learning_rate': 9.081116018528494e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0, 'grad_norm': 1.6153237083926797e-05, 'learning_rate': 9.027254120435205e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0037, 'grad_norm': 3.724370981217362e-05, 'learning_rate': 8.973392222341916e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0, 'grad_norm': 3.2062091870344034e-10, 'learning_rate': 8.919530324248627e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0002, 'grad_norm': 1.6421628970419988e-05, 'learning_rate': 8.865668426155338e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0, 'grad_norm': 0.0019581876695156097, 'learning_rate': 8.81180652806205e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0, 'grad_norm': 9.399039768709372e-09, 'learning_rate': 8.75794462996876e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0, 'grad_norm': 7.23973343497164e-08, 'learning_rate': 8.704082731875472e-06, 'epoch': 1.83}\n",
      "{'loss': 0.3041, 'grad_norm': 7.5068819569423795e-06, 'learning_rate': 8.650220833782183e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0, 'grad_norm': 1.324702338934003e-06, 'learning_rate': 8.596358935688894e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0003, 'grad_norm': 0.6471361517906189, 'learning_rate': 8.542497037595605e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0, 'grad_norm': 1.015297766571166e-05, 'learning_rate': 8.488635139502318e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0, 'grad_norm': 1.440553546672163e-06, 'learning_rate': 8.434773241409027e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0, 'grad_norm': 6.371467242693996e-11, 'learning_rate': 8.38091134331574e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0, 'grad_norm': 1.3972384138583038e-08, 'learning_rate': 8.32704944522245e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0307, 'grad_norm': 2.5122324132098584e-07, 'learning_rate': 8.273187547129162e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0, 'grad_norm': 8.291278774663624e-09, 'learning_rate': 8.219325649035871e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 2.191357850733766e-07, 'learning_rate': 8.165463750942584e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013746051990892738, 'learning_rate': 8.111601852849295e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 5.318291368894279e-05, 'learning_rate': 8.057739954756006e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005287828971631825, 'learning_rate': 8.003878056662717e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 0.006143655627965927, 'learning_rate': 7.950016158569429e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 2.153249397451873e-06, 'learning_rate': 7.89615426047614e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 1.5349902469097287e-06, 'learning_rate': 7.84229236238285e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 4.7441420747418306e-07, 'learning_rate': 7.788430464289562e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 1.2760924761323622e-08, 'learning_rate': 7.734568566196273e-06, 'epoch': 1.85}\n",
      "{'loss': 0.173, 'grad_norm': 1.04516033161417e-07, 'learning_rate': 7.680706668102984e-06, 'epoch': 1.85}\n",
      "{'loss': 0.0033, 'grad_norm': 5.0398647033489397e-08, 'learning_rate': 7.626844770009696e-06, 'epoch': 1.85}\n",
      "{'loss': 0.0001, 'grad_norm': 2.0406282601470593e-06, 'learning_rate': 7.572982871916406e-06, 'epoch': 1.85}\n",
      "{'loss': 0.0, 'grad_norm': 8.742012869333848e-05, 'learning_rate': 7.519120973823118e-06, 'epoch': 1.85}\n",
      "{'loss': 0.0001, 'grad_norm': 6.7644112277776e-05, 'learning_rate': 7.465259075729829e-06, 'epoch': 1.85}\n",
      "{'loss': 0.5097, 'grad_norm': 83.36500549316406, 'learning_rate': 7.41139717763654e-06, 'epoch': 1.85}\n",
      "{'loss': 0.3696, 'grad_norm': 1.029000213748077e-05, 'learning_rate': 7.357535279543251e-06, 'epoch': 1.85}\n",
      "{'loss': 0.0, 'grad_norm': 7.027144602034241e-05, 'learning_rate': 7.303673381449963e-06, 'epoch': 1.85}\n",
      "{'loss': 0.0847, 'grad_norm': 1.623610401679798e-08, 'learning_rate': 7.2498114833566736e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0, 'grad_norm': 1.9343616985967316e-10, 'learning_rate': 7.1959495852633855e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0, 'grad_norm': 1.4732313502463512e-06, 'learning_rate': 7.142087687170096e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0, 'grad_norm': 0.002181022195145488, 'learning_rate': 7.088225789076808e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0001, 'grad_norm': 1.1996098692179658e-07, 'learning_rate': 7.034363890983518e-06, 'epoch': 1.86}\n",
      "{'loss': 0.65, 'grad_norm': 2.25719759328058e-06, 'learning_rate': 6.98050199289023e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0009, 'grad_norm': 0.00011803638335550204, 'learning_rate': 6.926640094796941e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0, 'grad_norm': 3.8442263416982314e-07, 'learning_rate': 6.872778196703653e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0, 'grad_norm': 0.0026569680776447058, 'learning_rate': 6.818916298610363e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0062, 'grad_norm': 0.0001351531536784023, 'learning_rate': 6.765054400517075e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0, 'grad_norm': 2.0236069758539088e-05, 'learning_rate': 6.711192502423785e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003035799309145659, 'learning_rate': 6.657330604330497e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0, 'grad_norm': 5.171457087271847e-05, 'learning_rate': 6.6034687062372075e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0001, 'grad_norm': 0.004834212828427553, 'learning_rate': 6.5496068081439194e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0, 'grad_norm': 3.560817063430477e-08, 'learning_rate': 6.4957449100506305e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0522, 'grad_norm': 0.0066202664747834206, 'learning_rate': 6.441883011957342e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0026, 'grad_norm': 1.2220957614772487e-05, 'learning_rate': 6.388021113864053e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0237, 'grad_norm': 9.792426135390997e-05, 'learning_rate': 6.334159215770765e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0001, 'grad_norm': 9.661064268584596e-07, 'learning_rate': 6.280297317677475e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0, 'grad_norm': 2.5581769591553893e-08, 'learning_rate': 6.226435419584187e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0, 'grad_norm': 1.5896912941570918e-07, 'learning_rate': 6.172573521490898e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0, 'grad_norm': 5.027288239034533e-07, 'learning_rate': 6.118711623397609e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0002, 'grad_norm': 0.3569098711013794, 'learning_rate': 6.06484972530432e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0, 'grad_norm': 2.563802503630086e-10, 'learning_rate': 6.010987827211031e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0, 'grad_norm': 1.8852544599212706e-05, 'learning_rate': 5.957125929117742e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0, 'grad_norm': 7.157676918723155e-06, 'learning_rate': 5.903264031024454e-06, 'epoch': 1.88}\n",
      "{'loss': 0.009, 'grad_norm': 43.84579086303711, 'learning_rate': 5.849402132931165e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011208850628463551, 'learning_rate': 5.795540234837876e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0, 'grad_norm': 6.889866926940158e-05, 'learning_rate': 5.7416783367445875e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0001, 'grad_norm': 2.2117552362033166e-05, 'learning_rate': 5.6878164386512986e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013400478928815573, 'learning_rate': 5.63395454055801e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0002, 'grad_norm': 1.4977545870209497e-09, 'learning_rate': 5.580092642464721e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0, 'grad_norm': 0.002968000713735819, 'learning_rate': 5.526230744371432e-06, 'epoch': 1.89}\n",
      "{'loss': 0.1067, 'grad_norm': 0.0005526013555936515, 'learning_rate': 5.472368846278143e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0184, 'grad_norm': 3.6604249089577934e-06, 'learning_rate': 5.418506948184855e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0, 'grad_norm': 7.827618761502286e-12, 'learning_rate': 5.364645050091566e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008545528980903327, 'learning_rate': 5.310783151998277e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0, 'grad_norm': 7.314247341128066e-07, 'learning_rate': 5.256921253904988e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0, 'grad_norm': 1.2068784371876973e-06, 'learning_rate': 5.203059355811699e-06, 'epoch': 1.9}\n",
      "{'loss': 0.2921, 'grad_norm': 3.438127862409601e-07, 'learning_rate': 5.14919745771841e-06, 'epoch': 1.9}\n",
      "{'loss': 0.3279, 'grad_norm': 0.05497013032436371, 'learning_rate': 5.095335559625121e-06, 'epoch': 1.9}\n",
      "{'loss': 0.0925, 'grad_norm': 77.521484375, 'learning_rate': 5.0414736615318325e-06, 'epoch': 1.9}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001174825374619104, 'learning_rate': 4.987611763438544e-06, 'epoch': 1.9}\n",
      "{'loss': 0.1728, 'grad_norm': 5.056168674855144e-07, 'learning_rate': 4.9337498653452555e-06, 'epoch': 1.9}\n",
      "{'loss': 0.0, 'grad_norm': 8.077035568154756e-11, 'learning_rate': 4.879887967251967e-06, 'epoch': 1.9}\n",
      "{'loss': 0.0001, 'grad_norm': 9.979405604099156e-07, 'learning_rate': 4.826026069158678e-06, 'epoch': 1.9}\n",
      "{'loss': 0.0, 'grad_norm': 4.776165587827563e-05, 'learning_rate': 4.772164171065389e-06, 'epoch': 1.9}\n",
      "{'loss': 0.0025, 'grad_norm': 1.3316119318318442e-09, 'learning_rate': 4.7183022729721e-06, 'epoch': 1.91}\n",
      "{'loss': 0.026, 'grad_norm': 1.9840434106299654e-05, 'learning_rate': 4.664440374878811e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0, 'grad_norm': 3.088022140218527e-06, 'learning_rate': 4.610578476785522e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0, 'grad_norm': 3.165574526065029e-05, 'learning_rate': 4.556716578692233e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0, 'grad_norm': 7.067149567774322e-07, 'learning_rate': 4.502854680598944e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0, 'grad_norm': 2.9360628559516044e-06, 'learning_rate': 4.448992782505656e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0012330494355410337, 'learning_rate': 4.395130884412367e-06, 'epoch': 1.91}\n",
      "{'loss': 0.2228, 'grad_norm': 6.94155687597231e-07, 'learning_rate': 4.341268986319078e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0001, 'grad_norm': 8.605072252976242e-06, 'learning_rate': 4.2874070882257894e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0, 'grad_norm': 9.051702363649383e-06, 'learning_rate': 4.2335451901325005e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'grad_norm': 1.820301690713677e-06, 'learning_rate': 4.179683292039212e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'grad_norm': 3.744721470866352e-05, 'learning_rate': 4.125821393945923e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0018, 'grad_norm': 0.0025231901090592146, 'learning_rate': 4.071959495852634e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'grad_norm': 0.00566368643194437, 'learning_rate': 4.018097597759346e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'grad_norm': 2.5879989706822926e-08, 'learning_rate': 3.964235699666057e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'grad_norm': 9.984181815525517e-06, 'learning_rate': 3.910373801572768e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'grad_norm': 1.8660311980056576e-05, 'learning_rate': 3.856511903479479e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'grad_norm': 1.2510966485024255e-07, 'learning_rate': 3.80265000538619e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'grad_norm': 7.557434400951024e-06, 'learning_rate': 3.748788107292901e-06, 'epoch': 1.93}\n",
      "{'loss': 0.0015, 'grad_norm': 1.2600384277638987e-11, 'learning_rate': 3.6949262091996123e-06, 'epoch': 1.93}\n",
      "{'loss': 0.0001, 'grad_norm': 2.2088859907842817e-10, 'learning_rate': 3.641064311106324e-06, 'epoch': 1.93}\n",
      "{'loss': 0.0, 'grad_norm': 0.0012830125633627176, 'learning_rate': 3.587202413013035e-06, 'epoch': 1.93}\n",
      "{'loss': 0.0, 'grad_norm': 2.471400080139574e-07, 'learning_rate': 3.533340514919746e-06, 'epoch': 1.93}\n",
      "{'loss': 0.0, 'grad_norm': 3.142390170296494e-08, 'learning_rate': 3.479478616826457e-06, 'epoch': 1.93}\n",
      "{'loss': 0.0, 'grad_norm': 1.0828066221435506e-09, 'learning_rate': 3.425616718733168e-06, 'epoch': 1.93}\n",
      "{'loss': 0.0015, 'grad_norm': 7.598528384278325e-08, 'learning_rate': 3.3717548206398797e-06, 'epoch': 1.93}\n",
      "{'loss': 0.0, 'grad_norm': 3.8756283515795076e-07, 'learning_rate': 3.3178929225465908e-06, 'epoch': 1.93}\n",
      "{'loss': 0.0, 'grad_norm': 2.8073986868548673e-06, 'learning_rate': 3.264031024453302e-06, 'epoch': 1.93}\n",
      "{'loss': 0.0232, 'grad_norm': 0.0002851052850019187, 'learning_rate': 3.210169126360013e-06, 'epoch': 1.94}\n",
      "{'loss': 0.0035, 'grad_norm': 9.094312645174796e-07, 'learning_rate': 3.1563072282667245e-06, 'epoch': 1.94}\n",
      "{'loss': 0.0, 'grad_norm': 1.9478879039525054e-05, 'learning_rate': 3.1024453301734355e-06, 'epoch': 1.94}\n",
      "{'loss': 0.0, 'grad_norm': 4.4346531780092846e-08, 'learning_rate': 3.0485834320801466e-06, 'epoch': 1.94}\n",
      "{'loss': 0.0847, 'grad_norm': 88.01536560058594, 'learning_rate': 2.9947215339868577e-06, 'epoch': 1.94}\n",
      "{'loss': 0.158, 'grad_norm': 6.238366268007667e-07, 'learning_rate': 2.940859635893569e-06, 'epoch': 1.94}\n",
      "{'loss': 0.0008, 'grad_norm': 7.917122246681174e-08, 'learning_rate': 2.8869977378002803e-06, 'epoch': 1.94}\n",
      "{'loss': 0.0, 'grad_norm': 9.658835864456705e-08, 'learning_rate': 2.8331358397069914e-06, 'epoch': 1.94}\n",
      "{'loss': 0.0, 'grad_norm': 0.0014761049533262849, 'learning_rate': 2.7792739416137025e-06, 'epoch': 1.94}\n",
      "{'loss': 0.3641, 'grad_norm': 1.8515054307499668e-06, 'learning_rate': 2.7254120435204136e-06, 'epoch': 1.95}\n",
      "{'loss': 0.0, 'grad_norm': 3.402706383326404e-08, 'learning_rate': 2.671550145427125e-06, 'epoch': 1.95}\n",
      "{'loss': 0.0, 'grad_norm': 3.387739511140353e-08, 'learning_rate': 2.617688247333836e-06, 'epoch': 1.95}\n",
      "{'loss': 0.4445, 'grad_norm': 1.1832904789343957e-07, 'learning_rate': 2.5638263492405473e-06, 'epoch': 1.95}\n",
      "{'loss': 0.0, 'grad_norm': 7.95571759226732e-06, 'learning_rate': 2.5099644511472584e-06, 'epoch': 1.95}\n",
      "{'loss': 0.2021, 'grad_norm': 1.3514164720618282e-06, 'learning_rate': 2.45610255305397e-06, 'epoch': 1.95}\n",
      "{'loss': 0.0, 'grad_norm': 6.973042587787859e-08, 'learning_rate': 2.402240654960681e-06, 'epoch': 1.95}\n",
      "{'loss': 0.0, 'grad_norm': 0.0007507273112423718, 'learning_rate': 2.348378756867392e-06, 'epoch': 1.95}\n",
      "{'loss': 0.0, 'grad_norm': 1.9338523671308394e-08, 'learning_rate': 2.294516858774103e-06, 'epoch': 1.95}\n",
      "{'loss': 0.1903, 'grad_norm': 8.233409687363746e-08, 'learning_rate': 2.2406549606808143e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0, 'grad_norm': 1.2017152357657324e-06, 'learning_rate': 2.1867930625875258e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0007, 'grad_norm': 2.840299748640973e-05, 'learning_rate': 2.132931164494237e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0003, 'grad_norm': 4.567090172713506e-07, 'learning_rate': 2.079069266400948e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0, 'grad_norm': 1.8574692717265862e-07, 'learning_rate': 2.025207368307659e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0, 'grad_norm': 8.124133273668122e-06, 'learning_rate': 1.9713454702143706e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0, 'grad_norm': 0.01588362455368042, 'learning_rate': 1.9174835721210816e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0, 'grad_norm': 2.2514962338959776e-08, 'learning_rate': 1.8636216740277927e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0, 'grad_norm': 0.002394856186583638, 'learning_rate': 1.8097597759345038e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0, 'grad_norm': 8.240284614657867e-07, 'learning_rate': 1.7558978778412151e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0, 'grad_norm': 5.434337072074413e-05, 'learning_rate': 1.7020359797479262e-06, 'epoch': 1.97}\n",
      "{'loss': 0.0, 'grad_norm': 1.8309852976017282e-06, 'learning_rate': 1.6481740816546375e-06, 'epoch': 1.97}\n",
      "{'loss': 0.0346, 'grad_norm': 0.007418758701533079, 'learning_rate': 1.5943121835613486e-06, 'epoch': 1.97}\n",
      "{'loss': 0.0002, 'grad_norm': 4.822017581318505e-05, 'learning_rate': 1.54045028546806e-06, 'epoch': 1.97}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010659276449587196, 'learning_rate': 1.4865883873747712e-06, 'epoch': 1.97}\n",
      "{'loss': 0.0, 'grad_norm': 0.00021942979947198182, 'learning_rate': 1.4327264892814823e-06, 'epoch': 1.97}\n",
      "{'loss': 0.0002, 'grad_norm': 7.455955710611306e-06, 'learning_rate': 1.3788645911881936e-06, 'epoch': 1.97}\n",
      "{'loss': 0.0, 'grad_norm': 7.561835900560254e-06, 'learning_rate': 1.3250026930949047e-06, 'epoch': 1.97}\n",
      "{'loss': 0.0, 'grad_norm': 0.0010061097564175725, 'learning_rate': 1.271140795001616e-06, 'epoch': 1.97}\n",
      "{'loss': 0.0, 'grad_norm': 1.7012880562106147e-05, 'learning_rate': 1.217278896908327e-06, 'epoch': 1.98}\n",
      "{'loss': 0.0003, 'grad_norm': 8.779397120406429e-08, 'learning_rate': 1.1634169988150384e-06, 'epoch': 1.98}\n",
      "{'loss': 0.5654, 'grad_norm': 3.6910268619294584e-08, 'learning_rate': 1.1095551007217495e-06, 'epoch': 1.98}\n",
      "{'loss': 0.0029, 'grad_norm': 3.0289156711660326e-06, 'learning_rate': 1.0556932026284608e-06, 'epoch': 1.98}\n",
      "{'loss': 0.0, 'grad_norm': 1.3436773471653396e-08, 'learning_rate': 1.0018313045351719e-06, 'epoch': 1.98}\n",
      "{'loss': 0.0001, 'grad_norm': 0.005258519668132067, 'learning_rate': 9.479694064418831e-07, 'epoch': 1.98}\n",
      "{'loss': 0.0001, 'grad_norm': 5.293061917654995e-07, 'learning_rate': 8.941075083485943e-07, 'epoch': 1.98}\n",
      "{'loss': 0.0, 'grad_norm': 1.693147879677781e-07, 'learning_rate': 8.402456102553055e-07, 'epoch': 1.98}\n",
      "{'loss': 0.0, 'grad_norm': 1.7285532294408767e-06, 'learning_rate': 7.863837121620166e-07, 'epoch': 1.98}\n",
      "{'loss': 0.0, 'grad_norm': 4.5564789274976647e-07, 'learning_rate': 7.325218140687277e-07, 'epoch': 1.99}\n",
      "{'loss': 0.0, 'grad_norm': 8.842202987580094e-06, 'learning_rate': 6.786599159754389e-07, 'epoch': 1.99}\n",
      "{'loss': 0.0, 'grad_norm': 3.3559214784872893e-07, 'learning_rate': 6.247980178821502e-07, 'epoch': 1.99}\n",
      "{'loss': 0.0002, 'grad_norm': 2.5337888587273483e-07, 'learning_rate': 5.709361197888614e-07, 'epoch': 1.99}\n",
      "{'loss': 0.0, 'grad_norm': 1.1217818496334075e-07, 'learning_rate': 5.170742216955726e-07, 'epoch': 1.99}\n",
      "{'loss': 0.0, 'grad_norm': 1.3541140333472867e-06, 'learning_rate': 4.6321232360228377e-07, 'epoch': 1.99}\n",
      "{'loss': 0.0, 'grad_norm': 8.80342497566744e-07, 'learning_rate': 4.0935042550899496e-07, 'epoch': 1.99}\n",
      "{'loss': 0.0008, 'grad_norm': 0.1056649386882782, 'learning_rate': 3.5548852741570616e-07, 'epoch': 1.99}\n",
      "{'loss': 0.1906, 'grad_norm': 1.9437197806837503e-07, 'learning_rate': 3.0162662932241736e-07, 'epoch': 1.99}\n",
      "{'loss': 0.0, 'grad_norm': 2.5339827516290825e-06, 'learning_rate': 2.4776473122912855e-07, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': 0.00021618847677018493, 'learning_rate': 1.9390283313583972e-07, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': 1.3431318848233786e-06, 'learning_rate': 1.400409350425509e-07, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': 3.404204917956122e-08, 'learning_rate': 8.61790369492621e-08, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': 3.013483080849255e-07, 'learning_rate': 3.231713885597329e-08, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c36eb6716e64d829d9e80289c63db01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04327120631933212, 'eval_accuracy': 0.9931487665625337, 'eval_runtime': 3199.9013, 'eval_samples_per_second': 14.505, 'eval_steps_per_second': 7.253, 'epoch': 2.0}\n",
      "{'train_runtime': 16410.0484, 'train_samples_per_second': 4.525, 'train_steps_per_second': 1.131, 'train_loss': 0.10475210716941649, 'epoch': 2.0}\n",
      "Done training! Model + LoRA adapter saved to: ./lora_suicide_model_seqcls\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe39d76d3b74aa5a19ebc29e9e83146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy = 0.9931\n",
      "\n",
      "Confusion Matrix:\n",
      "[[23134   132]\n",
      " [  186 22963]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " non-suicide       0.99      0.99      0.99     23266\n",
      "     suicide       0.99      0.99      0.99     23149\n",
      "\n",
      "    accuracy                           0.99     46415\n",
      "   macro avg       0.99      0.99      0.99     46415\n",
      "weighted avg       0.99      0.99      0.99     46415\n",
      "\n",
      "\n",
      "Number of misclassifications: 318\n",
      "\n",
      "--- Misclassified Sample 0 ---\n",
      "Row ID: 346690\n",
      "Predicted: suicide, Actual: non-suicide\n",
      "Snippet of text: So Iâ€™m really upset at my friend right now (I hardly post here, just didnâ€™t know where to put it. I donâ€™t necessarily need advice, I just need to spell this out. Thanks to anyone that reads to the end) So Iâ€™m (16 M) kind of pissed at my friend (16 F) right now. Light backstory, I met her online, weâ€™...\n",
      "\n",
      "--- Misclassified Sample 1 ---\n",
      "Row ID: 245540\n",
      "Predicted: non-suicide, Actual: suicide\n",
      "Snippet of text: i have to wait another million years to diewow what a wonderful world\n",
      "\n",
      "--- Misclassified Sample 2 ---\n",
      "Row ID: 121764\n",
      "Predicted: non-suicide, Actual: suicide\n",
      "Snippet of text: Bojack Horseman season 4 episode 6\\[removed\\]\n",
      "\n",
      "--- Misclassified Sample 3 ---\n",
      "Row ID: 203842\n",
      "Predicted: suicide, Actual: non-suicide\n",
      "Snippet of text: did you ever feel empty?(repost cuz original got lost in new) because i do. i don't have a lot of friends and the only friends i have are primary school friends who i barely see. my last 2 years i got bullied really badly and that made my feel like nothing is inside me. i changed classes now but it ...\n",
      "\n",
      "--- Misclassified Sample 4 ---\n",
      "Row ID: 52713\n",
      "Predicted: non-suicide, Actual: suicide\n",
      "Snippet of text: I messed up and my fiance is only giving me a few hours. She's also in another country. So I messed up. I'll be honest. I have a problem with my sex drive. I'm not a cheater in that I don't flirt with other women, knowingly anyway, and in the literal meaning too. But, I get aroused pretty easily, an...\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Unified Single-Cell Code: Train & Evaluate on the Full Dataset\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Optional helper to display Markdown in notebooks\n",
    "def md(text):\n",
    "    display(Markdown(text))\n",
    "\n",
    "# --------------------------\n",
    "# 1) Configurable Parameters\n",
    "# --------------------------\n",
    "DATA_FILE = \"Suicide_Detection.csv\"\n",
    "TRAIN_FRACTION = 0.8         # 80% for train, 20% for test\n",
    "TRAIN_PERCENTAGE_INNER = 0.2 # Use 20% of that training portion for demonstration\n",
    "num_train_epochs = 2\n",
    "max_length = 256\n",
    "batch_size = 2\n",
    "grad_accum_steps = 2\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# LoRA config\n",
    "lora_rank = 16\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "\n",
    "base_model_path = \"/Users/ehsan/.llama/checkpoints/Llama3.2-1B-hf\"\n",
    "output_dir = \"./checkpoints/llama3_seqcls_lora\"\n",
    "\n",
    "# --------------------------\n",
    "# 2) Load the CSV\n",
    "# --------------------------\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "print(f\"Loaded CSV with shape: {df.shape}\")\n",
    "\n",
    "# Create a map from \"row_id\" -> original text, so we can later retrieve text for misclassifications\n",
    "original_text_map = {}\n",
    "for i, row in df.iterrows():\n",
    "    # We'll assume \"Unnamed: 0\" is a unique ID in the CSV\n",
    "    row_id = row[\"Unnamed: 0\"]\n",
    "    original_text_map[row_id] = row[\"text\"]\n",
    "\n",
    "# --------------------------\n",
    "# 3) Create a Hugging Face Dataset\n",
    "# --------------------------\n",
    "dataset_dict = {\n",
    "    \"row_id\": df[\"Unnamed: 0\"].tolist(),\n",
    "    \"class\": df[\"class\"].tolist(),\n",
    "    \"text\": df[\"text\"].tolist()\n",
    "}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "label2id = {\"non-suicide\": 0, \"suicide\": 1}\n",
    "def keep_minimal_columns(example):\n",
    "    return {\n",
    "        \"row_id\": example[\"row_id\"],\n",
    "        \"labels\": label2id[example[\"class\"]],\n",
    "        \"text\": example[\"text\"]\n",
    "    }\n",
    "\n",
    "dataset_min = dataset.map(keep_minimal_columns)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Train/Test split\n",
    "# --------------------------\n",
    "# We do an 80%/20% split for training/evaluation\n",
    "split_dset = dataset_min.train_test_split(test_size=1 - TRAIN_FRACTION, seed=42)\n",
    "train_dataset_full = split_dset[\"train\"]  # 80% portion\n",
    "eval_dataset_full  = split_dset[\"test\"]   # 20% portion\n",
    "\n",
    "print(f\"Train set size: {len(train_dataset_full)}, Test set size: {len(eval_dataset_full)}\")\n",
    "\n",
    "# Then subselect 20% of the training set if you only want a quick demonstration:\n",
    "train_size = int(len(train_dataset_full) * TRAIN_PERCENTAGE_INNER)\n",
    "train_dataset = train_dataset_full.shuffle(seed=42).select(range(train_size))\n",
    "eval_dataset  = eval_dataset_full\n",
    "\n",
    "print(\"Final TRAIN size:\", len(train_dataset))\n",
    "print(\"Final EVAL size:\", len(eval_dataset))\n",
    "\n",
    "# --------------------------\n",
    "# 5) Tokenizer & LoRA Model Setup\n",
    "# --------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_path,\n",
    "    num_labels=2,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model_base.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Setup LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "model = get_peft_model(model_base, lora_config)\n",
    "\n",
    "# --------------------------\n",
    "# 6) Tokenize the Train/Eval\n",
    "# --------------------------\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_dataset  = eval_dataset.map(tokenize_function,  batched=True)\n",
    "\n",
    "# Remove leftover \"text\" field so the collator sees only input_ids, attention_mask, labels, row_id\n",
    "train_dataset = train_dataset.remove_columns([\"text\"])\n",
    "eval_dataset  = eval_dataset.remove_columns([\"text\"])\n",
    "\n",
    "# Convert to PyTorch format\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"row_id\"])\n",
    "eval_dataset.set_format(\"torch\",  columns=[\"input_ids\", \"attention_mask\", \"labels\", \"row_id\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n",
    "\n",
    "# --------------------------\n",
    "# 7) Trainer Setup & Training\n",
    "# --------------------------\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum_steps,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    fp16=False,   # keep False if using Apple MPS\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./lora_suicide_model_seqcls\")\n",
    "print(\"Done training! Model + LoRA adapter saved to: ./lora_suicide_model_seqcls\")\n",
    "\n",
    "# --------------------------\n",
    "# 8) Evaluate & Print Misclassifications\n",
    "# --------------------------\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "pred_probs  = predictions.predictions\n",
    "pred_labels = pred_probs.argmax(-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "acc = (pred_labels == true_labels).mean()\n",
    "print(f\"Evaluation Accuracy = {acc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(true_labels, pred_labels))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, pred_labels, target_names=[\"non-suicide\",\"suicide\"]))\n",
    "\n",
    "# Identify misclassifications\n",
    "wrong_mask   = (pred_labels != true_labels)\n",
    "wrong_indices = np.where(wrong_mask)[0]  # dataset indices in eval_dataset\n",
    "\n",
    "print(\"\\nNumber of misclassifications:\", len(wrong_indices))\n",
    "\n",
    "max_show = 5\n",
    "label_name_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "\n",
    "for i, ds_idx in enumerate(wrong_indices[:max_show]):\n",
    "    ds_idx_py = int(ds_idx)  # Convert np.int64 -> Python int for indexing\n",
    "    sample_row = eval_dataset[ds_idx_py]\n",
    "\n",
    "    row_id = int(sample_row[\"row_id\"])\n",
    "    predicted = label_name_map[pred_labels[ds_idx_py]]\n",
    "    actual = label_name_map[true_labels[ds_idx_py]]\n",
    "\n",
    "    # Retrieve original text from your map\n",
    "    original_text = original_text_map[row_id]\n",
    "    snippet = original_text[:300] + (\"...\" if len(original_text) > 300 else \"\")\n",
    "\n",
    "    print(f\"\\n--- Misclassified Sample {i} ---\")\n",
    "    print(f\"Row ID: {row_id}\")\n",
    "    print(f\"Predicted: {predicted}, Actual: {actual}\")\n",
    "    print(\"Snippet of text:\", snippet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Misclassified Sample 0 ---\n",
      "Row ID: 1396\n",
      "Predicted: non-suicide, Actual: suicide\n",
      "Snippet of text: I have a good lifeseeing some of the posts here, there are so many people with actual problems in life and reasons to wanna die. I feel guilty since my life is actually pretty good yet I have suicidal thoughts daily for no reason\n",
      "\n",
      "--- Misclassified Sample 1 ---\n",
      "Row ID: 384\n",
      "Predicted: non-suicide, Actual: suicide\n",
      "Snippet of text: I'm ending it on new year's day.I've posted on here before and the only response was a troll comment so idk why I'm posting again. I guess I just need to organize my thoughts somehow. I've been looking for a better job for well over a year now and I can't even get anyone to look at my resume. Ive be...\n",
      "\n",
      "--- Misclassified Sample 2 ---\n",
      "Row ID: 953\n",
      "Predicted: suicide, Actual: non-suicide\n",
      "Snippet of text: Love you so much I wish I could W ha\n",
      "\n",
      "--- Misclassified Sample 3 ---\n",
      "Row ID: 1447\n",
      "Predicted: suicide, Actual: non-suicide\n",
      "Snippet of text: Today me and my girlfriend broke up. It was a mutual decision and weâ€™re still friends. We just didnâ€™t find that our relationship could fit into a bf gf one.\n",
      "\n",
      "--- Misclassified Sample 4 ---\n",
      "Row ID: 703\n",
      "Predicted: non-suicide, Actual: suicide\n",
      "Snippet of text: No sleep in 4 days. Can't win. Can't not lose.I started coughing 4 days ago and haven't stopped.  Can't sleep, can't lie down, can't walk any distance at any pace, can only sit if I'm hunched over like a cocktail shrimp, and that only delays the coughing for about half an hour.  \n",
      "\n",
      "Full throated coug...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /Users/ehsan/.llama/checkpoints/Llama3.2-1B-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/581 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:777\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 777\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:739\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    115\u001b[0m     offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Map batch items to dataset indices\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3380\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3377\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3378\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:241\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    237\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:793\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    790\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    791\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    792\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column test not in the dataset. Current columns in the dataset: ['row_id', 'labels', 'input_ids', 'attention_mask']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mtokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mlabel_ids\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/datasets/arrow_dataset.py:2780\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/datasets/arrow_dataset.py:2764\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2762\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2763\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2764\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2765\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2766\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2767\u001b[0m )\n\u001b[1;32m   2768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/datasets/formatting/formatting.py:590\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    588\u001b[0m         _raise_bad_key_type(key)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 590\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/datasets/formatting/formatting.py:527\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 527\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column test not in the dataset. Current columns in the dataset: ['row_id', 'labels', 'input_ids', 'attention_mask']\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/581 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in collate_fn. Printing batch features:\n",
      "\n",
      "--- Feature 0 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,     40,  47177,     11,    279,   7757,    706,   1903,   1274,\n",
      "          2288,  17668,  10882,    449,   1694,  28201,   1274,     13,    358,\n",
      "          1766,    279,   1207,    436,     14,   1736,    267,     13,   2876,\n",
      "         82495,     11,   7000,    315,    430,     13,   4702,    653,   2534,\n",
      "           292,    382,   3957,   1070,    264,   1486,   1405,   1274,   5616,\n",
      "          1427,    520,   5694,    323,   2019,    330,  36661,    499,   1440,\n",
      "            11,   7344,    358,  13434,    956,    656,    420,  17619,  12241,\n",
      "            40,   3974,    304,    264,  28859,  69110,  30651,     13,    578,\n",
      "          3823,   7102,    706,    912,  21648,  14926,     13, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 118299 (type: <class 'int'>)\n",
      "text => I swear, the internet has made people too fucking comfortable with being horrible people. I found the sub r/incest. Not satire, none of that. Just unironic.\n",
      "\n",
      "Is there a point where people finally look at themselves and say \"hey you know, maybe I shouldn't do this shit?\"\n",
      "\n",
      "I live in a boring dystopia. The human race has no shame anymore. (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 1 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,  29089,    499,   1520,    704,    264,   9562,    889,  38643,\n",
      "          3966,   5133,   9650,     30,   2100,    856,  23601,    320,    845,\n",
      "            37,      8,    323,    358,    320,   1114,     44,      8,    527,\n",
      "           304,    264,   1317,   6138,   5133,    323,   1606,    315,   1057,\n",
      "         15082,    584,    649,    956,   3449,   3156,    220,     17,   1667,\n",
      "           505,   1457,     13,   4800,     11,    856,  14048,    374,    264,\n",
      "          7452,  14048,    323,    856,   6699,   1541,    956,   1440,    922,\n",
      "          1077,     13,  49203,    358,    649,    956,   1650,   1077,    477,\n",
      "          3547,   4199,   1077,     13,   1628,   1364,    596,   2216,  23268,\n",
      "           922,    430,    627,   7184,     11,    358,   3021,   1077,     13,\n",
      "          1628,   1364,   2795,   1364,  16180,    757,   2288,     13,  42096,\n",
      "            11,   2574,   2751,  17395,    323,   1457,   1364,    596,  13126,\n",
      "           422,   1364,    596,   6160,     13,  16299,    374,   7060,    315,\n",
      "          3388,     13,   3011,    596,    539,    279,   3575,    627,  21364,\n",
      "           323,    358,    527,   2225,  10213,  79966,    323,  14188,    315,\n",
      "           603,    706,   1047,    872,   1176,  21735,    323,   1364,    596,\n",
      "          1027,  11890,    757,    922,   1268,   1364,  15849,   1077,   9562,\n",
      "          1667,    527,   3794,  39928,    627,   7184,   1618,    596,    279,\n",
      "          3575,     13,    362,   2478,  22178,   4227,   1364,   3309,    757,\n",
      "          1364,  11321,   1093,    358,   2846,    539,   3403,    369,   1077,\n",
      "            13,   1628,   1524,   3582,   1364,  16180,    757,     11,   1070,\n",
      "           527,   2574,   1364,   6944,    311,    656,    430,   1364,    649,\n",
      "           956,    656,    449,    757,     13,   1628,   1243,   1364,   1071,\n",
      "           330,     40,   1390,   2555,   7106,    449,   4423,  10246,     40,\n",
      "          2846,    539,   1694,  49373,     13,   3005,   6944,    264,  62414,\n",
      "           449,   4423,     13,   4702,   3131,     11,  14132,     13,   2582,\n",
      "           520,   3325,    430,    596,   1148,   1364,   6944,  56939,     11,\n",
      "           719,   1364,   1071,   1364,   6944,    757,    304,   1077,   2324,\n",
      "           627,     40,   4691,    422]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 54709 (type: <class 'int'>)\n",
      "text => Would you help out a teen who desperately needs relationship advice? So my girlfriend (16F) and I (17M) are in a long distance relationship and because of our situations we can't meet until 2 years from now. Now, my household is a strict household and my parents don't know about her. Meaning I can't call her or facetime her. And she's really upset about that.\n",
      "Now, I love her. And she says she loves me too. Recently, things got complicated and now she's considering if she's bi. Which is fine of course. That's not the problem.\n",
      "Her and I are both mostly inexperienced and neither of us has had their first kiss and she's been telling me about how she thinks her teen years are getting wasted.\n",
      "Now here's the problem. A few nights ago she told me she feels like I'm not enough for her. And even though she loves me, there are things she wants to do that she can't do with me. And then she said \"I want something physical with someone.\"\n",
      "I'm not being dumped. She wants a hookup with someone. Just once, apparently. Or at least that's what she wants rn, but she said she wants me in her life.\n",
      "I asked if I might get replaced. She said we might not be a couple anymore, but she still wants us to be close.\n",
      "Now all of this hurts. And she said I can say no. But based on how much I know her, things won't end if I say no. The argument woll come up again and again till I'm ready to say yes to her and let her do what she wants to.\n",
      "I don't want to lose her. And this hurts so much. What should I do? (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 2 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,    791,   7757,    387,   1093,  98645,  15334,  76121,   1753,\n",
      "         13692,   6005,   7520,    362,    735,    926,   7520,  71501,    271,\n",
      "          2675,    527,    264,  18118,    271,   3112,    220,   2881,    387,\n",
      "         15718,   1093,    459,  10973,  27075,   2025,   1093,   3009,   1694,\n",
      "           264,  67586,   1269,  27136,   4815,   2746,    279,   7757,   1436,\n",
      "           387,   3463,    315,    439,    264,   8162,   9594,     76,    369,\n",
      "          8396,   1243,   8396,   1053,    387,  10409,    449,  32738,    315,\n",
      "          3674,  12437,  43600,   1370,   2277,    279,  14708,     13,  26146,\n",
      "           430,    753,  16280,    279,  67422,    638,    315,    436,  10576,\n",
      "         64286, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 190669 (type: <class 'int'>)\n",
      "text => The internet be like OMG YOU FUCKING SWORE AT A KID AT SCHOOL\n",
      "\n",
      "You are a monster\n",
      "\n",
      "And irl be acting like an absolute dickhead like stop being a hypocrite honestly \n",
      "\n",
      "If the internet could be thought of as a microcosm for society then society would be filled with masses of social justice warriors parading the streets. Fuck thatâ€™s literally the epitome of r/AITA (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 3 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,   3055,    499,  38307,   1772,    323,   3249,    602,    656,\n",
      "          7170,   4376,    315,   1202,  58134,   1023,   4376,  70058,  58134,\n",
      "            11,    602,  15890,   2216,   4059,   1274,    889,    656,    433,\n",
      "           439,   1317,    439,   1202,    539,    682,    577,   1772,    602,\n",
      "          1781,   4433,   1695, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 67260 (type: <class 'int'>)\n",
      "text => do you horny post and why i do sometimes half of its joking other half isnt joking, i dont really mind people who do it as long as its not all u post i think ur good (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 4 ---\n",
      "labels => 1 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,   2127,  16708,  54950,   5159,  18547,    374,    779,   3958,\n",
      "           358,  15320,   2571,    709,    323,  49394,    788,     13,    358,\n",
      "          3077,   6818,   1690,  39653,    323,    433,    596,   1120,   3794,\n",
      "           311,    387,   2288,   1790,     13,    358,   1541,    956,   1440,\n",
      "          1268,   1690,    810,   2919,    358,    649,    656,    420,     13,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 78133 (type: <class 'int'>)\n",
      "text => Anxiety sicknessMy anxiety is so bad I constantly throw up and nauseous. I've tried many medicines and it's just getting to be too much. I don't know how many more days I can do this. (type: <class 'str'>)\n",
      "class => suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 5 ---\n",
      "labels => 1 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,  96056,   1171,     40,   2846,  27207,  18575,     40,   2846,\n",
      "         23087,   1171,    358,   2846,  11961,   8931,    505,    499,    439,\n",
      "           856,   4851,  23874,  10284,    264,  13636,    277,     11,    719,\n",
      "           856,   8271,    374,    264,   9622,     11,    323,    856,  13836,\n",
      "           374,   9622,    358,   2846,  27207,    315,  48389,   5606,    477,\n",
      "          1694,    264,  41698,     26,  63120,      0,  63120,      0,    358,\n",
      "          3077,   1903,    459,  27204,      0,    578,   2132,    358,    617,\n",
      "          2753,   1243,    220,     16,   2944,    311,   2559,   1243,    358,\n",
      "          3358,   1212,    311,    387,    264,   2731,    893,      0,    720,\n",
      "            40,   2846,  27207,    430,    358,   2846,  11509,    304,    279,\n",
      "          9462,   1131,   4291,    279,   8798,   2403,    856,   1203,    323,\n",
      "           279,   1193,   2944,    358,   2846,    539,   7366,    374,   1606,\n",
      "           358,   2846,  11039,    369,    279,   5076,   2944,    323,    433,\n",
      "           596,   3987,    539,   8333,    358,   6996,     13,   1789,    264,\n",
      "          7191,    893,   1053,    617,  54244,    813,  41040,    311,   3974,\n",
      "           323,    279,  43383,    893,   1053,  14284,    709,    323,   2815,\n",
      "        100004,   4619,    358,   9396,    856,  45713,    304,    279,   9462,\n",
      "            11,   2559,   7833,    323,  16106,    449,    856,  23726,  43939,\n",
      "           323,    856,  11013,   9235,  26139,   3235,    311,  78558,  11936,\n",
      "           627,     40,   2846,  27207,    430,   1306,   4395,    374,   8208,\n",
      "           358,   3358,    617,    264,   3838,    323,   3974,   7636,    323,\n",
      "           279,   2800,    315,    856,   2324,    690,    387,    264,  22520,\n",
      "         45796,     13, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 225725 (type: <class 'int'>)\n",
      "text => ScarredI'm scared.....I'm scarred I'm sitting miles from you as my heart sits slightly a-jar, but my brain is a mess, and my soul is mess I'm scared of hurting anyone or being a disappointment; Attention! Attention! I've made an arrangement! The second I have less then 1 reason to stand then I'll start to be a better man! \n",
      "I'm scared that I'm standing in the sand...with the heat against my back and the only reason I'm not moving is because I'm fighting for the wrong reason and it's hope not strength I lack. For a greater man would have drank his piss to live and the weaker man would curl up and die,but instead I stick my toes in the sand, stand straight and cry with my lips cracked and my mouth dry singing along to shitty songs.\n",
      "I'm scared that after everything is gone I'll have a house and live alone and the rest of my life will be a dramatic marathon. (type: <class 'str'>)\n",
      "class => suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 6 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,     40,   2846,  18069,    439,    282,    334,     74,    358,\n",
      "           574,  15389,    369,    264,  23601,    369,   4661,    220,     20,\n",
      "          1667,     13,   1628,   1457,    358,   2846,    304,   3907,    323,\n",
      "          1131,  14410,   1174,    358,   1766,    279,   3828,    889,  15262,\n",
      "           757,     13,   3005,    374,   1093,    757,   1174,    584,    617,\n",
      "          1890,  12034,   1174,   1890,   5435,   5099,     13,   1628,  10035,\n",
      "          1174,    433,   5084,    430,    584,    527,   3549,    369,   1855,\n",
      "          1023,  12340,   2030,   1131,   1102,    596,    832,    908,   3893,\n",
      "          1017,   3575,   1131,   3005,    617,    264,  26923,    551,   6018,\n",
      "            41,   2176,     89,   1174,    994,    358,   1766,    459,  10728,\n",
      "          3828,   1174,    279,   1828,   1938,   1364,   1071,    430,   1364,\n",
      "           617,    264,  27860,  40193,   9522,     40,   2846,   2216,  18069,\n",
      "          1732,     13,    720,     47,    815,     13,  26146,      0, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 319566 (type: <class 'int'>)\n",
      "text => I'm lucky as f**k I was searching for a girlfriend for almost 5 years. And now I'm in University and... YES , I found the girl who liked me. She is like me , we have same interests , same problems etc. And yes , it seems that we are created for each other!!! But... It's one liiiitle problem... She have a boyfriend :/\n",
      "Jeez , when I found an ideal girl , the next day she said that she have a damn bf...\n",
      "I'm really lucky person. \n",
      "P.S. Fuck! (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 7 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,  36661,   3828,    527,    499,    264,  33297,   1606,    358,\n",
      "         33833,   2571,    499, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 319341 (type: <class 'int'>)\n",
      "text => hey girl are you a magnet because I wanna throw you (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 8 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,   8607,  26159,    889,   2351,   1990,    220,    605,     12,\n",
      "           508,   2103,   6227,  11011,    279,  11276,  38345,     11,  23643,\n",
      "         40292,    612,   1141,     26,  21223,     11,   1630,  23892,    220,\n",
      "          6843,     11,  11659,     17,    612,   1141,     26,    220,     18,\n",
      "         17262,     64,   3872,    757,   1203,   1606,   1203,   1606,    856,\n",
      "         26159,    617,  15042,    709,   9298,   1057,  15740,    304,   1120,\n",
      "           220,    868,    323,    220,   1114,   1667,     13, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 85506 (type: <class 'int'>)\n",
      "text => Many pets who're between 10-20 still remember hearing the windows xp, Nintendo Wii &amp; DS, XBOX 360, PS2 &amp; 3 Kinda throws me back because back because my pets have grown up seeing our evolution in just 15 and 17 years. (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 9 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,  10854,     64,  23268,   3092,   1579,   2978,   3250,   1431,\n",
      "          3085,   1690,   6989,    358,   1505,   7185,     13,   8442,    538,\n",
      "           358,   3604,   1390,    311,   1935,    374,  49593,  34458,     13,\n",
      "          1442,   1193,   1070,    574,  11164,   3852,  27623,    242,     13,\n",
      "          8840,   1664,  28509, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 141490 (type: <class 'int'>)\n",
      "text => Kinda upset My high school doesnâ€™t offer many classes I find interesting. Only class I actually want to take is honors biology. If only there was botany ðŸ˜”. Oh well lol (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 10 ---\n",
      "labels => 1 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,     40,  17668,  12491,   1884,   5524,  38040,  10371,    369,\n",
      "          1520,  20670,    617,  18570,    311,   2610, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 302153 (type: <class 'int'>)\n",
      "text => I fucking hate those Ñunts asking for helpthey have somebody to ask (type: <class 'str'>)\n",
      "class => suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 11 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,     40,   2846,    539,   1093,   1023,  13305,    358,   2846,\n",
      "           264,  35295,  17627,  38443,     84, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 298186 (type: <class 'int'>)\n",
      "text => I'm not like other boys I'm a Femboy uwu (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 12 ---\n",
      "labels => 1 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,   5159,  18547,    374,   3794,    311,    757,   2506,    574,\n",
      "          9405,    420,   1648,     11,    422,    499,   3974,    449,  18547,\n",
      "           477,   1440,   4423,    449,  15748,  18547,    358,   1781,    499,\n",
      "          1440,     13,    358,  89986,    449,    279,   1054,    333,    264,\n",
      "          1841,    574,    922,    311,   4295,    757,    358,   8434,   1431,\n",
      "          7940,    704,    315,    279,   1648,  11453,   2057,   1884,   5496,\n",
      "           304,   6926,  10723,   6784,    499,   3207,    539,   7636,     13,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 203524 (type: <class 'int'>)\n",
      "text => My anxiety is getting to me.I was born this way, if you live with anxiety or know someone with severe anxiety I think you know. I resonate with the â€œif a car was about to hit me I wouldnâ€™t jump out of the wayâ€. To those living in constant mental pain youâ€™re not alone. (type: <class 'str'>)\n",
      "class => suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 13 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,    791,  18165,   7498,   1392,    315,    436,     14,  15247,\n",
      "         11467,   4314,    527,    279,  18165,   7498,     76,   1821,    430,\n",
      "           499,   2011,   1833,   1418,    389,    420,  63192,     13,   1442,\n",
      "           499,   1464,   1521,   7498,   1392,     11,    323,    656,    539,\n",
      "         63278,     11,    499,   4985,    387,   9120,   3384,    311,    279,\n",
      "           452,   2791,    369,    682,  64482,     13,  17674,   6807,   5900,\n",
      "           311,    856,    893,  10811,    889,   3782,    709,    449,    420,\n",
      "          4623,   1176,    271,     16,     13,  86471,  89635,    539,    387,\n",
      "           264,  12714,    198,     17,     13,  86471,  89635,    539,   2610,\n",
      "           369,    904,    892,    315,   6299,    477,  21845,    311,    387,\n",
      "          2728,    311,    701,   1772,    198,     18,     13,  86471,  89635,\n",
      "           539,   1005,    279,    836,    315,   2876,    331,     11,    279,\n",
      "         36778,    315,  41330,     11,    304,  46604,    627,     19,     13,\n",
      "         20474,    279,    659,   1772,   9178,     11,    311,   2567,    433,\n",
      "          2217,   1949,    198,     20,     13,  86471,  89635,    539,   5622,\n",
      "          8945,  10099,    477,   6896,  12875,   2971,   1900,    304,  41330,\n",
      "           198,     21,     13,  86471,  89635,    539,   2231,  87520,   1603,\n",
      "         41330,    198,     22,     13,  86471,  89635,    539,   2231,  14318,\n",
      "           477,  73842,  53954,   1603,  27728,    198,     23,     13,  86471,\n",
      "         89635,    539,  47177,   1418,  25394,  41330,    198,     24,     13,\n",
      "         86471,  89635,   1833,    279,  63192,   5718,    198,    605,     13,\n",
      "         86471,  89635,    539,   2019,    430,  10696,  93655,    374,    264,\n",
      "         39275,     11,   1606,    433,    753,    539,    382,   2746,    499,\n",
      "           617,  11102,    904,    315,   1521,   3290,   1392,     11,    323,\n",
      "          2733,    430,    499,   1205,    311,  63278,     11,   1618,    753,\n",
      "          1148,    311,    656,     13,   6122,   1139,    264,  41330,   3254,\n",
      "          2851,   1917,     11,   5918,    264,  22996,  44721,     11,    323,\n",
      "          2019,   1054,  31765,   2876,    331,   2476,    304,   6369,     11,\n",
      "          8272,    555,    701,  63278]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 201946 (type: <class 'int'>)\n",
      "text => The Ten Commandments of r/teenagers These are the Ten Commandmants that you must follow while on this subreddit. If you break these Commandments, and do not repent, you shall be banished to the Nether for all eternity. Original credit goes to my man Jesus who came up with this idea first\n",
      "\n",
      "1. Thou shalt not be a simp\n",
      "2. Thou shalt not ask for any time of actions or rewards to be given to your post\n",
      "3. Thou shalt not use the name of Notch, the Creator of Minecraft, in vain.\n",
      "4. Remember the self post weekend, to keep it image free\n",
      "5. Thou shalt not kill baby animals or pet dogs/cats in Minecraft\n",
      "6. Thou shalt not put Fortnite before Minecraft\n",
      "7. Thou shalt not put Instagram or TikTok before Reddit\n",
      "8. Thou shalt not swear while discussing Minecraft\n",
      "9. Thou shalt follow the subreddit rules\n",
      "10. Thou shalt not say that pedophilia is a sexuality, because itâ€™s not.\n",
      "\n",
      "If you have broken any of these commandments, and feel that you need to repent, hereâ€™s what to do. Go into a Minecraft single player world, built a diamond throne, and say â€œDear Notch,â€ in chat, followed by your repentance. Also, if you think there should be more commandments, feel free to leave them in the comments. Have a nice day! (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 14 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,  82752,   1618,   7446,   3156,    358,    636,    264,  42988,\n",
      "            25,   6187,    220,   3534,   1226,    527,   3794,   3345,    311,\n",
      "         24657,  19016,      0,   5884,   6848,    369,    264,  25960,   1772,\n",
      "            30, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 171047 (type: <class 'int'>)\n",
      "text => Posting here daily until I get a gf: Day 97 We are getting close to triple digits! Any ideas for a celebration post? (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "\n",
      "--- Feature 15 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,  33874,  12170,    436,    779,  30931,    912,    358,   1541,\n",
      "           956,  17668,    636,    279,  22380,  17339,    420,    374,   3249,\n",
      "           358,   2163,  43274,    369,    220,     23,   4038,     11,  29075,\n",
      "            42,   3249,    358,   1903,    459,   1046,    311,   2586,   1203,\n",
      "           326,   1764,     78,   1131,   7214,    994,    577,   1541,    956,\n",
      "           636,    279,  22380,    477,   3619,   2555,    499,   2351,  17668,\n",
      "         18754,   5099,     11,   1093,    358,  56319,   9109,  45373,  17773,\n",
      "          8049,   3247,  10458,   3472,  54233,  48630,    220,   4154,  91989,\n",
      "          3247,  10458,   3472,   2794,  14031,   3701,    912,   1205,    311,\n",
      "         10485,    433,   1139,    856,   3663,   1131,    577,  16280,   1304,\n",
      "           832,  16930,   1618,    323,    499,   3358,    387,  20561,   8615,\n",
      "           671,     83,    433,    369,  16058,     11,   5127,   1618,    706,\n",
      "           264,   1063,   3460,    315,  66732,   6485,   1120,   1606,    814,\n",
      "          3619,    264,   3738,  22380,    477,   1440,    279,  42285,   1131,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 127734 (type: <class 'int'>)\n",
      "text => redditors r so annoying no I don't fucking get the joke okay this is why I left reddit for 8 months, IDEK why I made an acc to come back lmao... immediately when u don't get the joke or understand something you're fucking stupid etc, like I CLEARLY DON'T GET THE JOKE PLEASE JUST  EXPLAIN THE JOKE OR SMTH no need to rub it into my face... u literally make one mistake here and you'll be ridiculed abt it for forever, everyone here has a some sort of superiority complex just because they understand a certain joke or know the meme... (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:777\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 777\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:739\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     46\u001b[0m     offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Map batch items to dataset indices\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 26\u001b[0m, in \u001b[0;36mwrapper_collate_fn\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_collate_fn\u001b[39m(features):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdebug_collate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 19\u001b[0m, in \u001b[0;36mdebug_collate_fn\u001b[0;34m(features, data_collator)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m => \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Optionally re-raise the original error to stop execution\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[47], line 11\u001b[0m, in \u001b[0;36mdebug_collate_fn\u001b[0;34m(features, data_collator)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mCustom collate function that tries to pad/convert the batch,\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mbut if an error occurs, it prints the offending samples.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in collate_fn. Printing batch features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3380\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3377\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3378\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:241\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    237\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vet/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:793\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    790\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    791\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    792\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def debug_collate_fn(features, data_collator):\n",
    "    \"\"\"\n",
    "    Custom collate function that tries to pad/convert the batch,\n",
    "    but if an error occurs, it prints the offending samples.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return data_collator(features)\n",
    "    except Exception as e:\n",
    "        print(\"Error in collate_fn. Printing batch features:\")\n",
    "        for i, feature in enumerate(features):\n",
    "            print(f\"\\n--- Feature {i} ---\")\n",
    "            for k, v in feature.items():\n",
    "                print(f\"{k} => {v} (type: {type(v)})\")\n",
    "        # Optionally re-raise the original error to stop execution\n",
    "        raise e\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=\"longest\")\n",
    "\n",
    "\n",
    "def wrapper_collate_fn(features):\n",
    "    return debug_collate_fn(features, data_collator)\n",
    "\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset_small_copy,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=wrapper_collate_fn\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_indices = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    offset = 0\n",
    "    for batch in tqdm(eval_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "        batch_size = batch[\"input_ids\"].size(0)\n",
    "\n",
    "        # Map batch items to dataset indices\n",
    "        batch_indices = list(range(offset, offset + batch_size))\n",
    "        offset += batch_size\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        # numeric (0 = non-suicide, 1 = suicide)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "        all_indices.extend(batch_indices)\n",
    "\n",
    "# --------------------------\n",
    "# 7) Compute Metrics\n",
    "# --------------------------\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_indices = np.array(all_indices)\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(\"Accuracy on eval dataset:\", acc)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds,\n",
    "      target_names=[\"non-suicide\", \"suicide\"]))\n",
    "\n",
    "wrong_indices = all_indices[all_preds != all_labels]\n",
    "print(\"Number of misclassifications:\", len(wrong_indices))\n",
    "\n",
    "# --------------------------\n",
    "# 8) Inspect Misclassifications\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "def print_misclassified_samples(\n",
    "    dataset,\n",
    "    indices,\n",
    "    all_preds,\n",
    "    title=\"Misclassified\",\n",
    "    max_show=5\n",
    "):\n",
    "    \"\"\"\n",
    "    dataset: the dataset with output_all_columns=True\n",
    "    indices: numeric indices into dataset\n",
    "    all_preds: array of predicted classes\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- {title} (showing up to {max_show} examples) ---\\n\")\n",
    "    label_name_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "    for i, idx in enumerate(indices[:max_show]):\n",
    "        idx = int(idx)  # Convert numpy.int64 to native int\n",
    "        # Retrieve full row (including \"text\", \"class\", etc.)\n",
    "        row = dataset[idx]\n",
    "\n",
    "        text = row[\"text\"] if \"text\" in row else \"N/A\"\n",
    "        actual_label_str = row[\"class\"] if \"class\" in row else row[\"labels\"]\n",
    "        predicted_num = all_preds[idx]\n",
    "        predicted_label_str = label_name_map[predicted_num]\n",
    "\n",
    "        print(f\"Index in subset: {idx}\")\n",
    "        print(f\"Actual Label: {actual_label_str}\")\n",
    "        print(f\"Predicted Label: {predicted_label_str}\")\n",
    "        snippet = text[:300] + (\"...\" if len(text) > 300 else \"\")\n",
    "        print(f\"Text: {snippet}\\n---\")\n",
    "\n",
    "\n",
    "# Identify false positives vs. false negatives\n",
    "all_preds = all_preds.astype(int)\n",
    "all_labels = all_labels.astype(int)\n",
    "\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "for i in range(len(all_preds)):\n",
    "    if all_preds[i] == 1 and all_labels[i] == 0:\n",
    "        false_positives.append(all_indices[i])\n",
    "    elif all_preds[i] == 0 and all_labels[i] == 1:\n",
    "        false_negatives.append(all_indices[i])\n",
    "\n",
    "print(f\"\\nFalse positives: {len(false_positives)}\")\n",
    "print(f\"False negatives: {len(false_negatives)}\")\n",
    "\n",
    "# Example usage to visualize some misclassifications\n",
    "print_misclassified_samples(eval_dataset_small_copy,\n",
    "                            false_positives, all_preds, \"False Positives\")\n",
    "print_misclassified_samples(eval_dataset_small_copy,\n",
    "                            false_negatives, all_preds, \"False Negatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(37723) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "86777.17s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from transformers) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/vet/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "!pip install --upgrade torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in collate_fn. Printing batch features:\n",
      "\n",
      "--- Feature 0 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,     40,  47177,     11,    279,   7757,    706,   1903,   1274,\n",
      "          2288,  17668,  10882,    449,   1694,  28201,   1274,     13,    358,\n",
      "          1766,    279,   1207,    436,     14,   1736,    267,     13,   2876,\n",
      "         82495,     11,   7000,    315,    430,     13,   4702,    653,   2534,\n",
      "           292,    382,   3957,   1070,    264,   1486,   1405,   1274,   5616,\n",
      "          1427,    520,   5694,    323,   2019,    330,  36661,    499,   1440,\n",
      "            11,   7344,    358,  13434,    956,    656,    420,  17619,  12241,\n",
      "            40,   3974,    304,    264,  28859,  69110,  30651,     13,    578,\n",
      "          3823,   7102,    706,    912,  21648,  14926,     13]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 118299 (type: <class 'int'>)\n",
      "text => I swear, the internet has made people too fucking comfortable with being horrible people. I found the sub r/incest. Not satire, none of that. Just unironic.\n",
      "\n",
      "Is there a point where people finally look at themselves and say \"hey you know, maybe I shouldn't do this shit?\"\n",
      "\n",
      "I live in a boring dystopia. The human race has no shame anymore. (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "\n",
      "Error occurred at dataset index 0.\n",
      "Attempting to fix text.\n",
      "\n",
      "Text before fix: 'I swear, the internet has made people too fucking comfortable with being horrible people. I found the sub r/incest. Not satire, none of that. Just unironic.\\n\\nIs there a point where people finally look at themselves and say \"hey you know, maybe I shouldn\\'t do this shit?\"\\n\\nI live in a boring dystopia. The human race has no shame anymore.'\n",
      "Error in collate_fn. Printing batch features:\n",
      "\n",
      "--- Feature 0 ---\n",
      "labels => 0 (type: <class 'torch.Tensor'>)\n",
      "input_ids => tensor([128000,     40,  47177,     11,    279,   7757,    706,   1903,   1274,\n",
      "          2288,  17668,  10882,    449,   1694,  28201,   1274,     13,    358,\n",
      "          1766,    279,   1207,    436,     14,   1736,    267,     13,   2876,\n",
      "         82495,     11,   7000,    315,    430,     13,   4702,    653,   2534,\n",
      "           292,    382,   3957,   1070,    264,   1486,   1405,   1274,   5616,\n",
      "          1427,    520,   5694,    323,   2019,    330,  36661,    499,   1440,\n",
      "            11,   7344,    358,  13434,    956,    656,    420,  17619,  12241,\n",
      "            40,   3974,    304,    264,  28859,  69110,  30651,     13,    578,\n",
      "          3823,   7102,    706,    912,  21648,  14926,     13]) (type: <class 'torch.Tensor'>)\n",
      "attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1]) (type: <class 'torch.Tensor'>)\n",
      "Unnamed: 0 => 118299 (type: <class 'int'>)\n",
      "text => I swear, the internet has made people too fucking comfortable with being horrible people. I found the sub r/incest. Not satire, none of that. Just unironic.\n",
      "\n",
      "Is there a point where people finally look at themselves and say \"hey you know, maybe I shouldn't do this shit?\"\n",
      "\n",
      "I live in a boring dystopia. The human race has no shame anymore. (type: <class 'str'>)\n",
      "class => non-suicide (type: <class 'str'>)\n",
      "Fix unsuccessful. Still erroring out:\n",
      " Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "Skipping or removing item from dataset.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_invisibles(s):\n",
    "    \"\"\"\n",
    "    Removes/strips out some common invisible characters\n",
    "    that can cause shape mismatch or dimension errors.\n",
    "    Adjust the regex as needed for your specific case.\n",
    "    \"\"\"\n",
    "    # This pattern catches zero-width spaces, left-to-right markers, etc.\n",
    "    # You can expand it if you find more codepoints cause trouble.\n",
    "    return re.sub(r\"[\\u200B-\\u200F\\u202A-\\u202E\\ufeff]+\", \"\", s)\n",
    "\n",
    "\n",
    "def debug_collate_fn(features, data_collator):\n",
    "    \"\"\"\n",
    "    Custom collate function that tries to pad/convert the batch,\n",
    "    but if an error occurs, it prints the offending samples.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return data_collator(features)\n",
    "    except Exception as e:\n",
    "        print(\"Error in collate_fn. Printing batch features:\")\n",
    "        for i, feat in enumerate(features):\n",
    "            print(f\"\\n--- Feature {i} ---\")\n",
    "            for k, v in feat.items():\n",
    "                print(f\"{k} => {v} (type: {type(v)})\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "# Single-sample approach\n",
    "for idx in range(len(eval_dataset_small_copy)):\n",
    "    try:\n",
    "        single_item = [eval_dataset_small_copy[idx]]  # one-sample 'batch'\n",
    "        _ = debug_collate_fn(single_item, data_collator)\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"\\nError occurred at dataset index {idx}.\\nAttempting to fix text.\\n\")\n",
    "\n",
    "        # 1) Inspect the text\n",
    "        row = eval_dataset_small_copy[idx]\n",
    "        text_before = row[\"text\"]\n",
    "        print(\"Text before fix:\", repr(text_before))\n",
    "\n",
    "        # 2) Remove any invisible/unicode characters\n",
    "        cleaned_text = remove_invisibles(text_before)\n",
    "\n",
    "        # 3) Replace the text in the dataset with the cleaned version\n",
    "        eval_dataset_small_copy[idx][\"text\"] = cleaned_text\n",
    "\n",
    "        # 4) Try again:\n",
    "        try:\n",
    "            single_item_fixed = [eval_dataset_small_copy[idx]]\n",
    "            _ = debug_collate_fn(single_item_fixed, data_collator)\n",
    "            print(\"Fix successful. Updated text for index\", idx)\n",
    "        except Exception as e2:\n",
    "            print(\"Fix unsuccessful. Still erroring out:\\n\", e2)\n",
    "            print(\"Skipping or removing item from dataset.\")\n",
    "            # Optionally remove the item from the dataset:\n",
    "            # eval_dataset_small_copy = eval_dataset_small_copy.select(\n",
    "            #    [i for i in range(len(eval_dataset_small_copy)) if i != idx]\n",
    "            # )\n",
    "            break  # or continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  We'll do everything by hand for that one sample Raw sample:\n",
      "Minimal sample keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# Assume you have the original dataset or eval_dataset_small\n",
    "# Let's call it sample = eval_dataset_small[0]\n",
    "# We'll do everything by hand for that one sample\n",
    "\n",
    "sample = eval_dataset_small[0]\n",
    "\n",
    "print(\"  We'll do everything by hand for that one sample Raw sample:\")\n",
    "# Copy only the relevant keys\n",
    "sample_minimal = {\n",
    "    \"input_ids\": sample[\"input_ids\"],\n",
    "    \"attention_mask\": sample[\"attention_mask\"],\n",
    "    \"labels\": sample[\"labels\"]\n",
    "}\n",
    "\n",
    "print(\"Minimal sample keys:\", sample_minimal.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collation success!\n",
      " {'input_ids': tensor([[128000,     40,  47177,     11,    279,   7757,    706,   1903,   1274,\n",
      "           2288,  17668,  10882,    449,   1694,  28201,   1274,     13,    358,\n",
      "           1766,    279,   1207,    436,     14,   1736,    267,     13,   2876,\n",
      "          82495,     11,   7000,    315,    430,     13,   4702,    653,   2534,\n",
      "            292,    382,   3957,   1070,    264,   1486,   1405,   1274,   5616,\n",
      "           1427,    520,   5694,    323,   2019,    330,  36661,    499,   1440,\n",
      "             11,   7344,    358,  13434,    956,    656,    420,  17619,  12241,\n",
      "             40,   3974,    304,    264,  28859,  69110,  30651,     13,    578,\n",
      "           3823,   7102,    706,    912,  21648,  14926,     13, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([0])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=\"longest\")\n",
    "\n",
    "single_batch = [sample_minimal]\n",
    "\n",
    "try:\n",
    "    out = data_collator(single_batch)\n",
    "    print(\"Collation success!\\n\", out)\n",
    "except Exception as e:\n",
    "    print(\"Collation error:\\n\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing check_text_issues.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile check_text_issues.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def check_csv_for_tokenization_issues(csv_file=\"Suicide_Detection.csv\", text_col=\"text\"):\n",
    "    \"\"\"\n",
    "    Checks several potential pitfalls that cause 'ValueError: too many dimensions \"str\"' \n",
    "    or other tokenization issues with text data.\n",
    "    1) Ensures text_col is always a string\n",
    "    2) Checks for extremely long text entries\n",
    "    3) Prints columns to confirm text_col exists\n",
    "    4) Shows an example row\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Load the CSV into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Loaded {len(df)} rows from '{csv_file}'\")\n",
    "\n",
    "    # 2) Identify suspicious rows where text_col is not a str\n",
    "    if text_col not in df.columns:\n",
    "        print(f\"ERROR: Column '{text_col}' not found in CSV. Columns are: {df.columns}\")\n",
    "        return\n",
    "    \n",
    "    # Check the column type\n",
    "    mask = ~df[text_col].apply(lambda x: isinstance(x, str))\n",
    "    problem_rows = df[mask]\n",
    "    print(f\"Number of suspicious rows (non-string '{text_col}'):\", len(problem_rows))\n",
    "\n",
    "    if len(problem_rows) > 0:\n",
    "        print(\"\\nSample suspicious rows:\\n\", problem_rows.head(5))\n",
    "        print(\"Consider merging lists or removing them before tokenization.\\n\")\n",
    "    else:\n",
    "        print(\"No suspicious rows found. All entries in 'text' are strings.\\n\")\n",
    "\n",
    "    # 3) Check text length\n",
    "    df[\"text_length\"] = df[text_col].apply(lambda x: len(x) if isinstance(x, str) else -1)\n",
    "    print(\"Text length stats:\\n\", df[\"text_length\"].describe(), \"\\n\")\n",
    "\n",
    "    # 4) Print columns\n",
    "    print(\"Columns in CSV:\", df.columns.tolist(), \"\\n\")\n",
    "\n",
    "    # 5) Show an example row (if any)\n",
    "    if len(df) > 0:\n",
    "        print(\"Example row:\\n\")\n",
    "        print(df.iloc[0])\n",
    "    else:\n",
    "        print(\"No rows in the dataset to preview.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_csv_for_tokenization_issues()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
